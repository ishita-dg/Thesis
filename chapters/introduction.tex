%!TEX root = ../dissertation.tex
\chapter{General Introduction}
\label{chap:intro}

Understanding the flexibility and efficiency of human intelligence, as well as how to incorporate it in artificial systems, is a long-standing open problem. A promising and successful approach has been to build structured probabilistic models of cognition. These have been very successful as a theory of human-like intelligence for two key reasons. First, by being "structured" they allow for primitives of human symbolic thought -- like rules, grammar, and logic -- that allow humans to generalize far beyond our direct experience\cite{fodor88, chomsky2002syntactic}. A classic example is of mathematical knowledge, for example, once we know the rules of addition, we can add two numbers we may never have added or even seen before. For a more everyday example, we know that if "Jane is taller than Gloria", this implies that "Gloria is shorter than Jane", since that is a logical deduction -- even if we may never have met Jane or Gloria, or know anything about their actual heights. These abilities require abstract, logical, structured representations. However, such representations cannot explain another crucial aspect of human intelligence, namely graded, uncertain inferences\cite{rogers2004semantic}. In many such cases, people do not have adequate information to make logical deductions, but can make reasonable `guesses' and are aware of their own uncertainty. Solely with access to discrete, logical rules, it would be impossible to account for human beliefs like `most birds can fly', or the human ability to make sense of and learn from ambiguous signals. This brings us to the second key aspect of structured probabilistic models -- that they operate over probabilities\footnote{In Chapter \ref{chap:psych}, I discuss in greater detail the advantages specifically of Bayesian probabilistic models.}, not discrete truths. Structured probabilistic models provide a broad, flexible framework with these key desirable properties, that abstracts away the core of the problems that intelligent systems need to solve, and provides a normative solution. This permits us to understand human cognition across a wide range of domains. ..They have also been adopted and used in artificial intelligence... 

However, these models face some crucial shortcomings as models of intelligent behavior. I first discuss the chief concerns with them as models of human cognition, followed by an outline of the approach this thesis takes -- of taking a renewed interest in the environment in which intelligent systems function -- to address these problems. I then discuss the role of structured probabilistic models in artificial intelligence, the key shortcomings, and resulting engineering solutions. I then discuss how the same approach -- of studying and understanding the role of the environment -- can better inform our search for human-like artificial intelligence.

While structured probabilistic models provide a normative answer and good theory of the abstract problem humans are trying to solve, they provide no insight into how humans might actually solve these problems (i.e. make probabilistic inferences in structured models) at the level of algorithms and psychological processes. %In other words, these models provide a good description at the `computational level' of abstraction in Marr's hierarchy\cite{marr1976understanding}, but do not provide an `algorithm / representation level' solution. 
This concern is exacerbated by two key issues. First, making exact probabilistic inferences in these structured models is largely intractable. Intuitively, the very flexibility of these models, in being able to generalize so far outside direct experience and account for uncertainty, can become a hindrance when trying to find a solution to a specific given problem. The space of solutions these models can represent is so vast, that finding the right solution for the problem at hand is like finding a needle in a haystack. I discuss this issue in greater detail in Chapter \ref{chap:approx}. Therefore, despite significant advances towards explaining behavior in humans, they largely remain "as-if" models that describe what humans might be doing, but do not provide a complete picture of how human cognition works.  Second, much empirical evidence suggests that people make several systematic inferential errors, with their probabilistic estimates deviating consistently from the predictions of a normative model. Some of these include effects like base rate neglect, where people tend to ignore certain sources of information like prior probabilities, or the anchoring effect where putatively unrelated sources of information influence future inferences, among many others. Wikipedia lists over a hundred such established cognitive biases. The fields of cognitive psychology\citep{phillips1966conservatism, gigerenzer1996reasoning}, and behavioral economics\citep{kahneman1973psychology} have responded to these findings with the proposal that perhaps humans in fact do not perform normative inference at all, and instead employ heuristic strategies. How can these we reconcile these?

%These approaches have been discussed in greater detail in Chapter \ref{chap:psych}. 

%While such models can replicate many of these biased behaviors in certain domains, without any sense of normativity, it is difficult to answer question like when and why certain heuristics are employed, and even exactly what the heuristic even is, or how it came about. Without answers to these questions, it is difficult to falsify specific claims or make new predictions, thereby losing explanatory power. 

%Therefore, while these heuristic theories make stronger and possibly more believable claims about the psychological mechanisms underlying human inference, by relinquishing the normative framework of inference in structured probabilistic models, they lose explanatory power of `why' human cognition is the way it is.
%these heuristics are brittle and any single heuristic cannot explain the wide variety of behaviors observed in human inference -- ranging from near-normativity in some case to wide range of different kinds of cognitive biases. This has lead to the development of several different heuristics, and theories where people choose a specific heuristic for each domain. However, 
%this approach leaves many questions open: What is the set of heuristic strategies humans use? In the absence of any sense of normativity, why were those specific heuristic chosen? How do humans choose when to use which heuristic? With such a loosely defined theory, it is difficult to falsify specific claims or make new predictions. Therefore, while these heuristic theories make stronger and possibly more believable claims about the psychological mechanisms underlying human inference, by relinquishing the normative framework of inference in structured probabilistic models, they lose explanatory power of `why' human cognition is the way it is.

In the first part of this thesis, I address these key criticisms of structured probabilistic models by providing a) a more computationally tractable solution to inference and b) an explanation for deviations from normative inference. I do this by taking a renewed look at the environment in which humans operate. The importance of the environment in shaping intelligent behavior, and thereby the importance of considering it when trying to understand intelligence, has been around in psychology as far back as 1943 with th work of Brunswik\cite{brunswik1943organismic}, and has since been periodically reinstated by the works of Simon\cite{simon1956rational}, Anderson\cite{anderson1990adaptive} and Gigerenzer \cite{gigerenzer1999simple}. These works posit that a better lens to look at human cognition is not of pure rationality or normativity, but rather through the lens of `ecological rationality' -- where the mind makes best use of limited cognitive resources, by leveraging underlying structure in the environment. However, most mainstream psychology focuses primarily on the internal frameworks, mechanisms, and representations within the human mind, largely neglecting the structure of the environment and how it might strongly impact these. In the words on Egon Brunswik, ``Psychology has forgotten that it is a science of organism-environment relationships, and has become a science of the organism''.  

In this thesis, I build models of human cognition that leverage environmental structure flexibly to ease the computational burden of exact inference in structured probabilistic models. A key component of this adaptation to the environment comes through the re-use of previous computations, also known as \textit{amortization}. Intuitively, underlying structure in the distribution of queries posited by an environment, will be picked up by a system that tries to efficiently re-use the computations done in response to these queries (see Chapter \ref{chap:amort} for details). These computational savings can lead to more psychologically realistic demands of human cognition, thereby rendering normative inferences in structured models less intractable and more plausible. But there is no free lunch. These computational savings come at a cost. While they might ease some inferences, they also make certain kinds of errors more likely. I show that the errors made by such models exactly mirror the patterns of cognitive biases observed (historically as well as in a series of new experiments) in humans. This program of research therefore jointly addresses both the key concerns with structured probabilistic models of human cognition outlined above.

%Further, it is not clear how these structured models might be learned from experience. Most approaches involve a search over many structures to find the one that fits data best. My virtue of being highly structured, this space of possible structures is large. While significant progress has been made in improving the quality of this search, it remains computationally challenging. In this thesis, we will primarily address the first issue, and discuss the second issue briefly in Chapter \ref{chap:LTI}.

We now turn to the analogous concerns with using structured probabilistic models in artificial intelligence. The intractability of inference in structured probabilistic models in general has been a major engineering impediment in their wide-spread adoption for artificial intelligence. While much recent work has demonstrated their advantages (analogous to their advantages in modeling human cognition)\cite{lake2015human, del2013understanding, zhao2011image, steyvers2007probabilistic}, the difficulty of inference remains a concern. The solution we proposed to this problem, of re-using past computations i.e. amortization, has previously been suggested in and studied in artificial systems, in the form of a recognition model\citep{\cite{dayan1995helmholtz, kingma2014auto} or, in the age of deep-learning, an inference network \citep{kulkarni2015picture, mnih2014neural, rezende2015variational, paige2016inference}. In fact, even in the absence of an explicitly structured probabilistic model, many deep learning methods that simply do discriminative classification, can be interpreted as amortized inference in an implicit model (see Chapter \ref{chap:amort} for details). These models re-use previously completed inference and thereby ease the burden on run-time inference. 

make better parallels to discussion on human inference?

As discussed briefly earlier (and in greater detail in Chapter \ref{chap:amort}), amortization leads to adaptation to the environmental distribution of queries. With such a deep dependence on amortized inference methods in modern machine-learning, understanding the role of the environment in shaping the procedures learned could have tremendous value -- both for better understanding current systems, as well as building better ones. However, most machine learning methods largely neglect the the system's learning or training environment, with progress on a task being gauged largely by how new models and mechanisms perform on unchanging, often arbitrarily selected, standardized data sets. That the training data-set has an immense impact on how the task is learned -- and even whether the task is learned as expected -- is not well studied.

In the second part of my thesis, I discuss how a deeper understanding of the role of the environment in shaping the inference procedures of intelligent systems, can also inform the study and engineering of artificially intelligent systems. I demonstrate how analysis and manipulation of learning environments can provide insight into how to artificially develop central tenets of human intelligence like causal inference and language. These insights also provide new ways to study how humans acquire and implement these complex behaviors.

The crucial insight that ties this thesis together is that humans are not a general purpose computing machines, and neither should artificial systems that hope to emulate human intelligence. While normative approaches like structured probabilistic models give us deep insights into the `what and why' of intelligent behavior, it leaves open the crucial question of `how' intelligent behavior can realistically manifest, within limits on time, data and energy. I argue that algorithmic realizations of ecological rationality -- where intelligent systems make the best use of limited resources by leveraging underlying structure in the environment -- within the framework of structured probabilistic models, can provide a promising way forward. Through a series of studies into human and artificial intelligence, I demonstrate how this insight can lead us to better models of human cognition, as well as better approaches to artificial intelligence.

%This is further highlighted by the fact that humans have limited computing resources, and therefore are in more dire need to adapt to their environments and selectively optimize performance where it matters. Learning about things that don't occur often is often not even possible due to limited experience, and is largely not worth it given the cost on computation.  define ecological rationality.

\section*{Outline}

First and foremost, in Chapter \ref{chap:psych}, I review the theoretical framework surrounding human probabilistic inference. I first review normative Bayesian accounts, followed by a review of other approaches to human probabilistic inference. I highlight the contributions and shortcomings of these approaches, and outline how this thesis suggests a more complete picture by unifying their complementary advantages. In Chapters \ref{chap:approx} and \ref{chap:amort} I review some technical and conceptual background. In Chapter \ref{chap:approx} I cover technical background on approximate probabilistic inference. In Chapter \ref{chap:amort} I introduce a formal notion of computational re-use i.e. amortization and how it can be used within algorithms for approximate inference, in particular how it can give leverage underlying environmental structure.

%In this thesis, I show how adaptation to environments (as necessary under resource constraints) can both a) render inference more tractable on average, and b) explain deviations from normative Bayesian inference.

In Chapters \ref{chap:MCMC} to \ref{chap:LTI}, I discuss how these concepts can be used to build new models for human cognition. Chapter \ref{chap:MCMC} studies human inferences in large hypothesis spaces. This is a challenging problem, where exact inference is intractable. I demonstrate that a sample-based approximation under ecologically rational constraints, can replicate the specific kinds of biases observed in human inference in such large hypothesis spaces. 
% This model relies on two crucial components to replicate these deviations from normativity: first, a small number of samples, and second, initialization based on question framing. I argue that these components are ecologically rational: humans have limited cognitive resources, hindering the possibility of a large number of samples in the approximation, and initialization based on question framing is reasonable since the context of queries in the environment are often related to and informative of the optimal response. 
Further, in small hypothesis spaces, this model returns optimal responses. This allows us to parsimoniously explain both the rationality as well as various kinds of irrationality of human inference within the same framework. Chapter \ref{chap:MCMC_amort} expands on the role of the environment in such sampling frameworks. I empirically demonstrate, as well as build models for, re-use of computation in consecutive related queries. This kind of re-use -- or amortization -- is ecologically rational since queries in the real world are often related, and the best use of limited resources is to re-use previously completed computations.

Chapter \ref{chap:LTI} studies the role of re-use and ecological rationality in greater detail. In particular, I discuss amortization in a variational framework, which is more amenable to flexible re-use than sampling. I demonstrate how this framework can replicate several cognitive biases that involve context-sensitive sub-optimal reactions to different sources of information, 
% -- including base-rate neglect, conservatism and belief bias -- 
as well as various various other effects.
% such as the response variability in probabilistic judgments, effects of experimental design, and certain memory effects. 
This work posits an algorithmic approach, via amortization of inference, to understanding ecologically rational heuristic behavior. I also discuss how this approach can be combined with previously discussed sampling approaches, to jointly and parsimoniously account (with mechanistic commitments that are within the realm of what is psychologically plausible) for a wide range of biased inferences while still retaining the capacity for optimal inference in the limit of infinite experience in the environment, or infinite computational resources.

Chapters \ref{chap:sentences} and \ref{chap:causal} shift gears to focus on the role of the environment in studying and improving modern artificial intelligence (AI).  In Chapter \ref{chap:sentences}, I consider a system for natural language processing. I first discuss this system, like many modern deep-learning based AI systems, can be interpreted as performing amortized inference. I then demonstrate how ecological rationality in this inference procedure can explain the errors this system makes -- analogous to how they explained human biases in Chapter \ref{chap:LTI}. I outline an approach that uses this insight to better asses and potentially also improve these systems. In Chapter \ref{chap:causal}, I demonstrate how we can leverage the strong influence the environment has on the inference procedures learned, to engineer the inference procedures we want. I show that manipulation of the environment of a very simple learning architecture, can give rise to complex behaviors including causal inference procedures and active information seeking behavior. The strong control we have over the environments encountered by artificial systems also allows new investigations into ecological rationality, and how it shapes human cognitive abilities.