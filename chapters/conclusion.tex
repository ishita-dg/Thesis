%!TEX root = ../dissertation.tex
\chapter{Conclusion}
\label{chap:conclusion}

In 1955, Herb Simon put forth the challenge facing more realistic theories of human intelligence:``Broadly stated, the task is to replace the global rationality of economic man with a kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environments in which such organisms exist." This thesis hopes to do exactly that. By taking into account the circumstances under which intelligent behavior manifests -- both the limitations on resources, structure in the environment, and how these two interact -- I provide new computational models of human probabilistic inference, that are psychologically plausible. Without such plausible algorithmic solutions to rational or normative inference in structured Bayesian models, they remain unsatisfying as models of human cognition. We also cannot leverage their many desirable properties in building intelligent machines. The ideas furthered in this thesis, of leveraging environmental structure via flexible re-use of previous computations to simplify inference, bring Bayesian models of intelligent behavior back into business.

Further, these models parsimoniously explain a wide range of empirical findings about non-normative inference. In particular, they explain how humans can sometimes be so close to optimal, and at other times (with the same cognitive resources), so biased -- and biased in so many different context-sensitive ways. These insights also lead to entirely new ways to understand and engineer artificial systems, via manipulation of the environments in which they learn and function. This confluence suggests links between the analysis of ecological rationality in humans and in machines, leading to new lines of research into understanding both.

\section*{Open Questions and Future Work}

\paragraph{Model acquisition}

An important question not addressed directly in this thesis is of how structured probabilistic models are acquired in the first place. In this thesis, we distinguish between `learning about the world' or `potential knowledge', and `learning to think' or `realized knowledge'. Most of the work on human cognition presented here operates solely on the second, i.e. in the realm of internal processes within the mind, \textit{after} all external knowledge has already been gained and represented as a probabilistic model. In some of the studies presented here, we verbally provide the data generating process, i.e. the underlying structured probabilistic model (for example the urn experiments in Chapter \ref{chap:LTI}), and in others, we assume this is known from pre-experimental experience (for example in the scene statistics domain used in Chapters \ref{chap:MCMC} - \ref{chap:LTI}). How might these structured Bayesian models be acquired via direct interaction with the environment?

One way to look at model acquisition is as a higher level probabilistic inference. That is, the representation we acquire of how a domain works is by searching over some space of possible models (a prior over models), assigning probabilities for how well they explain the observed data (the likelihood of that model), and then choosing a model such that it has high posterior probability. This is of course, also a very challenging inference problem.\citep{schulz2012finding, bramley2018grounding} In addition, how do we know what a good hypothesis space of models is? One suggestion is that these are not from some pre-existing hypothesis space but rather built from structured primitives. Several findings show that many primitives of structure might be innate and available at birth before any interaction with the environment.\citep{spelke1998nativism, chomsky1967recent} However, this still leaves open the question of how we search over the large space of models that can be built from these primitives, to find the right one for each domain. It also passes the buck further down to how we know what the right primitives are, and how (potentially via evolution) such primitives came to be innately encoded. 

%The challenges of how this search over models might occur (as well as how the hypothesis space of models is even built), therefore, forms another key criticism of structured Bayesian models for intelligent behavior. 

Explicitly structured probabilistic models however have several desirable features -- like efficient learning\citep{kemp2007learning}, greater generalizability\citep{lake2017building}, and an accurate representation of uncertainty\citep{hacking2006emergence}. An important direction of future research therefore is to find ways to harness these advantages while avoiding the prohibitive costs (and resulting implausibility) of learning these models from scratch via search. One possibility is that structure in the environment (and amortization procedures that reflect this structure) can alleviate these inference costs -- in the same way this thesis suggests that it could alleviate the intractability of inference within a learned model. 

%In Chapter \ref{chap:causal}, we showed that the end-to-end process of inferring a causal structure from observations (inferring a model) in addition to performing causal inference within that model can be amortized over previous experience. Preliminary extensions show that the process of inferring a causal model

\paragraph{Two kinds of learning}

Another interesting direction is to consider the interaction between learning about the world, and learning to make inferences in it. Although we have so far treated these entirely separately (at least in our studies of human cognition), in most real world domains, these are not separate tasks. In fact, we almost never learn models directly, we learn them as an intermediary towards performing some task that requires an inference within that model. 

%This is analogous to the discussion in Chapter \ref{chap:amort} regarding amortization in discriminative models: these models learn both kinds of information entirely end-to-end and have no explicit representation of an intermediate structured representation. This is in contract to a generative model, that represents the underlying generative process, i.e. it explicitly represents such a `model'. 

As an illustrative example, we consider a classic example from reinforcement learning, of latent learning in Tolman's rat mazes\citep{tolman1948cognitive}. Here, rats learned to navigate mazes of very specific shapes, to get to a reward. Simply memorizing the actions required to get to the reward in these mazes would have been sufficient to always receive the reward. \citet{tolman1948cognitive} found however that rats developed a more abstract model, or `cognitive map' of the spatial position of the reward with respect to their starting point. This is evidenced by the finding that the rats find the reward by navigating directly to it, in close to a straight line, when the walls of the maze are removed\footnote{The actual experiment did not remove the walls of the maze but replaced it with a maze that contained several radial arms, and found that rats take close to the shortest path to the reward by choosing the right arm.}. In this case, learning the model is like learning the spatial position of the reward with respect to you. This captures something about the underlying structure of the environment (spatial in this case), and this knowledge can generalize to give reasonable performance in different situations -- like starting from a different initial points, differences in the structure of the maze, or obstacles in the way. Inference in this model corresponds to planning ones actions (within constraints like walls of mazes, and wanting to minimize energy spent) in order to get to the reward. The task that the rats are trained on only really requires the ability to make some very specific inferences. There is no explicit requirement to represent any additional structure like the spatial structure of the environment. But we see that they acquire such a representation nonetheless. In other words, they could learn a purely \textit{discriminative} model (for the purposes of the task they are trained on), but instead learn an at least partially \textit{generative} model. This allows it to generalize beyond the specific task it was trained for, and acquire the reward efficiently when the walls of the maze are removed. This is characteristic of several domains -- while abstract models are usually useful for generalization, they are rarely explicitly taught or tested. Rather, they are acquired as an implicit intermediary to a task that requires  some specific inferences in such a model. In this thesis, we assume that the model has already been learned, and the only remaining challenge is in the inference. In the previous section we discuss how probabilistic inference can be used to learn the model. However, jointly learning a structured model for the environment, and learning to perform efficient inferences in this model, could bring with it own set of unique predictions and implications that future work should explore.

%This is very characteristic of almost all domains where an abstract model is useful -- we never learn this model directly, but as an implicit intermediary to making some inference in that model. \footnote{} 
% learn about the world, as well as learn how to think, simultaneously?

%We have so far taken the structure of the model for granted, assuming that world knowledge has already been acquired and is encompassed in an abstract representation of that domain. However, if the model is never directly trained on (and is only acquired indirectly), the question arises of what degree and kinds of structure should we represent? Structure and abstraction allows for efficient learning (via the `blessing of abstraction'\citep{harlow1949formation, kemp2007learning, goodman1983fact}) and relatedly, also allows generalization far outside the range of observed queries. It is for this very reason however that inference in these models is difficult: by being structured and abstract, they can express a vast range of hypotheses. This permits good generalization, but inferring the right hypothesis for a particular problem in this vast space is difficult. Generalization and ease of inference therefore trade off. 

%If the model itself is also being learned, in parallel with learning how to make inferences in it, a decision needs to be made on how to choose the `right' level of abstraction and structure such that it makes a good trade-off between generalization capacity and ease of inference. This will be influenced strongly by statistics in the environment. The environment will determine what extent and what kinds of generalization are useful: it is not useful to be able to generalize to situations that never occur. The amount of experience, and time pressures for decisions in the environment will also inform how efficient our inferences can and need to be. This thesis studies how to optimize the inference efficiency (by re-using past computation and leveraging structure in the environment) given a fixed model. 


\paragraph{Shaping our environments}

We have assumed that the interaction between the environment and the intelligent systems that live and learn in it is one-directional, only considering the impact of the environment on the procedures learned by the agents that interact with it. However, intelligent agents frequently influence their own environments. This two-way interaction is especially pertinent in domains like language where the production mechanisms themselves are shaped and limited by human cognitive abilities -- our ability to learn and understand compositionally structured languages is learned using data created by other humans' ability to produce these compositionally structured languages. The role of shaping one's environment is also relevant in other domains that are not as directly produced by humans. As far back 1956 in the study of category and concept learning, \citet{bruner2017study} presented a distinction between learning through passive reception of observations and through active selection of observations in support of hypothesis testing. Much subsequent work has expanded upon the significant impact that active information seeking behaviors have on learning\citep{nelson2005finding,markant2014better, gureckis2012self}, suggesting that even very young children can and do engage in behaviors that shape their own learning environments\citep{ruggeri2017toma, gopnik1996scientist, montessori1912montessori}. 

A `rational analysis' approach to active learning posits that humans maximize information gain, subject to the costs of information gathering. However, similar to our quandaries about exact probabilistic inference in humans, exactly computing information gain is nearly intractable. Further, several studies often find biases in people's information seeking tendencies\citep{jonas2001confirmation, beattie1988confirmation}. Future work should consider how processes like caching, re-use and amortization that can ease the computational burden of normative information seeking, replicate the specific kinds of biases observed, and potentially lead to new rational process models of active learning.

\paragraph{Grounding the theory}

A key aspect of this thesis is to more explicitly consider the role of memory in human inference. Even within this framework of using memory as a computational resource, it is yet to be understood what the contributions of different memory mechanisms (episodic, semantic, procedural, etc.) might be. This thesis has been largely agnostic to the specific kinds of re-use and how they might be realized in human memory systems. Future work can more explicitly investigate these different kinds of re-use. In the same vein, biased judgments have been studied extensively, and several models for these have been proposed. We have discussed some of these alternatives in this thesis, as well as how many of them fit into the broader framework of ecological rationality via amortized inference. Future work can work towards better understanding how these many models and different memory mechanisms fit together and inform each other.

Another consideration is how such theories might be implemented in the brain. Future work should look for signatures of amortization in the brain, and better understand which parts of the brain are involved in the different kinds of learning discussed here. We have briefly discussed the neural plausibility of approximation algorithms like Markov chain Monte Carlo, and variational inference in Chapter \ref{chap:approx}. A main proposal of this thesis is a hybrid model that incorporates aspects of both algorithms. Future work should consider how such hybrids might be realized in networks of neurons.

%An advantage of structured models are that they abstract away many details that are `not relevant'. For example in our balls-and-urns domain, it is not necessary to represent the physical structure of the urns. For the purpose of that domain, the urns are simply abstract containers whose physical properties have no significance in the kinds of questions asked. Experience with a balls-and-urns task with wooden urns transfers entirely to one with plastic urns. This abstraction allows generalization. However, which features are `relevant' can change. If we saw the same urn in a task where the goal is to predict the probability that the urn will break when dropped from a certain height and angle, the physical properties of the urn are suddenly relevant in our model of that task, and can no longer be `abstracted away'. Additionally, certain details almost never change, so even though they are `relevant' they need not be represented. For example, we assume the presence of gravity in both tasks: we assume that the balls in the urns will not arbitrarily float out of the urn, and that an urn that is dropped will fall to the ground. Even though this variable -- whether or not gravity is present -- is in fact relevant for the domain, it can still be  The kinds of structure we represent in our models for a domain is heavily influenced by the statistics of the environment?

%A broader implication of actively adapting our inference procedures to environmental structure, is that inference can be `learned' as well, via interaction with the outside world. The transition from `potential' to `realized' knowledge no longer happens entirely inside the mind. This blurs the lines between learning about the world and learning to think -- since both involve learning about some external information, just different kinds of information. 

%`Model-free' approaches on the other hand (including the emergentist connectionist views), blur this distinction entirely. The goal is to directly learn how to react to the world, without an intermediate stop at a model representation.

%Amortization of this inference allows a reconciliation between the two. 
%Do humans meta-learn? Structured and compositional amortization

%With much of the expressive burden being shifted to amortization, we are left with a deeper question. What really is the difference between learning and model, and learning to do inference in it? It leads us to question the notion that the models we learn are abstract and general purpose, agnostic to the questions we will ask of them downstream. To take an extreme example, a complete model of any domain, such that we can make good inferences to almost any downstream query about it, could be to represent each atom in it, and know about its interactions. With adequate computing power, this is sufficient to simulate forward every possible scenario, and make arbitrary inferences. But, in our urns-and-balls domain, it is pointless to represent the structural properties of these balls and urns. We know that for the purpose of that domain, the balls are considered abstract single units, and the urns are simply abstract containers whose physical properties have no significance in the kinds of questions asked. We can abstract away this structure because representing it does not improve our inferences in that domain. If we instead conducted an intuitive physics experiment with the exact same articles, asking question like the probability that the urn will break when dropped from a certain height, or domain, these structural properties suddenly become relevant and need to be represented.

\paragraph{Closing thoughts}

%This thesis explores how humans and machines can make the best use of their limited resources by leveraging underlying statistical structure in their environments. We show that by intelligently re-using previous computations from memory, within approximate inference algorithms, humans adapt to the historical distribution of queries. This reduces the costs of otherwise expensive computations, making structured Bayesian models of human cognition (that are otherwise intractable at the algorithmic level) more plausible (Chapters \ref{chap:approx} and \ref{chap:amort}). Further, these models predict specific modes of failure, that replicate human cognitive biases (Chapters \ref{chap:MCMC}, \ref{chap:MCMC_amort}, and \ref{chap:LTI}). We can also use this lens to understand ecologically rational heuristics in machines (Chapter \ref{chap:sentences}), as well as to engineer new kinds of intelligent behavior simply by manipulating the learning environment (Chapter \ref{chap:causal}).

%The work reported in this thesis highlights the role of amortization in human probabilistic inference: in explaining both its successes and failures. However, a
Amortization as an approach to ecological rationality also has much broader, and further-reaching implications for cognitive science, beyond the topics studied in this thesis. A better understanding of the underlying computational principles of ecological rationality can shed light on one of the longest-standing debates on the basis of human cognition: the conflict between compositional structure and simple statistics, in models of cognition.

Systematicity and compositionality\citep{fodor88} in structured representations permit the kinds of flexible generalization, far beyond direct experiences, that humans commonly exhibit. \citep{griffiths2006, shepard1987toward, wu2018generalization, schulz2017compositional}% \footnote{They also allow more efficient learning via the `blessing of abstraction'\citep{harlow1949formation, kemp2007learning, goodman1983fact}.}  
This flexibility however comes at a cost. While systematic and compositional representations allow recombination of its components in many different ways to provide solutions to new problems, inferring the solution to a particular given problem – by inferring the right combination of components in this large space – is very expensive. In other words, these models are `generative', having an explicit representation of the underlying generative process that produced the observed data. However producing a response to a specific query -- or making an inference -- based on this information, requires additional computation.

Statistical approaches\citep{rogers2004semantic, mcclelland2010letting} on the other hand, do not invoke an intermediate generative model. They instead directly learn to provide responses to queries. Without access to an explicit generative model, we lose the potential to generalize flexibly beyond direct experience. However, `making an inference' is no longer a challenge, since these approaches directly provide responses to queries. In other words, statistical models are usually `discriminative': they do not separately represent the underlying data generating process, and instead directly model the mapping between observations and response.

%These criticisms have been raised and developed largely by the `connectionist' approach to cognition\citep{rogers2004semantic, mcclelland2010letting} that posits instead that structure emerges from interactions with the environment, via low-level learning mechanisms -- rather than via a discrete search over the space of possible structured models. Most modern approaches to artificial intelligence, i.e. deep neural networks, follow in this connectionist tradition. Recent work has also suggested that more structured forms of intelligent behavior can in fact emerge from such `low-level' learning\citep{dasgupta2019causal, wang2018, botvinick2019reinforcement}.
%Models for amortized inference, including the inference network studied in Chapter \ref{chap:LTI}, have striking similarities to connectionist models. This suggests future directions of research that explore this connection, to build new hybrid models that combine the complementary advantages of these approaches.

While structured generative models give very good generalization, inference in them is often intractable. Statistical discriminative models on the other hand are poor at generalization but can make fast, often heuristic, inferences. Each of these therefore have been evoked to model different aspects of human cognition across several fields including word semantics\cite{steyvers2007probabilistic, rogers2004semantic, gleitman1990structural}, probabilistic judgment\citep{oaksford2007bayesian, tversky1974judgment}, concept learning and categorization\citep{bruner2017study, medin1978context, shepard1987toward}, and reinforcement learning\citep{gershman2014retrospective, daw2011model, kool2017cost}. 
%It has also been studied in the context of machine learning\citep{ng2002discriminative, lake2017building}.

A crucial observation however is that these two possibilities simply populate the far ends of a spectrum in the trade-off between generalization and tractable inference. While intelligent systems do generalize flexibly, they need not generalize indiscriminately. They should adapt to the environment to choose what kinds of generalizations are important, and sacrifice other generalizations (in favor of statistical pattern recognition, or memorization) in the interest keeping inference tractable. This allows for intermediate models that lie between the two extremes of entirely compositional generative representations, and entirely statistical discriminative ones. 

Where on this spectrum is `optimal' for an environment or domain, will be determined by the ecological distribution of queries we encounter in it. We do not represent the world in its full generality, rather we represent the world conditioned on what we will have to do with that representation, which is usually so to respond to specific distributions of queries. This thesis provides a powerful new theory for how such ecological rationality can come about via the amortization of previous computations. This provides a mechanism for learning representations that could trade-off flexible generalization and tractable inference, in a domain-sensitive way. These insights pave the way toward hybrid models that combine the complementary advantages of structured generative models and statistical discriminative models. Not only does this have significant implications for our understanding of human cognition, these insights can also be used to build better, and more human-like, artificial intelligence.


%In Chapters \ref{chap:sentences} and \ref{chap:causal}, we began to explore this, by demonstrating the emergence of representations that are not universally generalizable but only generalize in \textit{certain ways}, by augmenting environments and thereby manipulating which representations are ecologically rational.  





