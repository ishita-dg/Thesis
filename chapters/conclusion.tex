%!TEX root = ../dissertation.tex
\chapter{Conclusion}
\label{chap:conclusion}

In 1955, Herb Simon put forth the challenge facing more realistic theories of human intelligence: "Broadly stated, the task is to replace the global rationality of economic man with a kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environments in which such organisms exist." This thesis hopes to do exactly that. By taking into account the circumstances under which intelligent behavior manifests -- both the limitations on resources, structure in the environment, and how these two interact -- we provide new computational models of human probabilistic inference, that are psychologically plausible. Without plausible algorithmic solutions to rational or normative inference in structured Bayesian models, they remain unsatisfying as models of human cognition. We also cannot leverage their many desirable properties in building intelligent machines. The ideas furthered in this thesis, of leveraging environmental structure via flexible re-use of previous computations to simply inference, bring Bayesian models of intelligent behavior back into business.

Further, these models parsimoniously explain a wide range of empirical findings about non-normative inference, and how humans can sometimes be so close to optimal, and in other domains (with the same cognitive resources), so biased -- and biased in so many different context-sensitive ways. These insights also lead to entirely new ways to understand and engineer artificial systems, via manipulation of the environments in which they learn and function. This confluence suggests links between the analysis of ecological rationality in humans and in machines, leading to new lines of research into understanding both.

\section*{Open Questions and Future Work}

An important question not addressed directly in this thesis is of how structured probabilistic models are acquired in the first place. In our studies of human cognition, we have distinguished between `learning about the world' or `potential knowledge', and `learning to think' or `realized knowledge'. Most of the work on human cognition presented in this work operate solely on the second, i.e. in the realm of internal processes within the mind, \textit{after} all external knowledge has already been gained and represented as a structured probabilistic model. In some of these examples, we verbally provide the data generating process, i.e. the underlying structured probabilistic model (for example the urn experiments in Chapter \ref{chap:LTI}), but in several others, we assume this is known from pre-experimental experience (for example in the scene statistics domain used in Chapters \ref{chap:MCMC} - \ref{chap:LTI}). How might these structured Bayesian models be acquired via interaction with the environment?

One way to look at model acquisition is as a higher level probabilistic inference. That is, the representation we acquire of how a domain works is by searching over some space of possible models (over which we might have prior or even innate beliefs for which ``kinds" of models are more probable), assigning probabilities for how well they explain the observed data (the likelihood of that model), and then choosing a model such that it has high posterior probability. This is of course, also a very challenging inference problem. Another key criticism of structured Bayesian models for intelligent behavior, from the `connectionist' tradition has centered around the implausibility of this search.\cite{rogers2004semantic, mcclelland2010letting} This tradition posits instead that structure emerges from interactions with the environment, via low-level learning mechanisms, rather than via a discrete search over the space of possible models. Most modern approaches to artificial intelligence, i.e. deep neural netwroks, follow in this tradition. An important direction of future research therefore is to ameliorate the computational burden of learning a model. One possibility is that structure in the environment and amortization can alleviate this, the same way we suggest it alleviates the intractability of inference within a learned model. Models for amortized inference, including the inference network studied in Chapter \ref{chap:LTI}, have striking similarities to connectionist models. This suggests future directions of research that explore this connection, both theoretically as well as towards building new hybrid models that combine the complementary advantages of these approaches.

%In Chapter \ref{chap:causal}, we showed that the end-to-end process of inferring a causal structure from observations (inferring a model) in addition to performing causal inference within that model can be amortized over previous experience. Preliminary extensions show that the process of inferring a causal model

Another interesting direction is to consider the interaction between these two kinds of learning. That is, between learning about the world, and learning to think. In most real world domains, learning about the model, and learning to make inferences in it are not separate tasks. In fact, we almost never learn models directly, we learn them as an intermediary towards performing some task that requires an inference within that model. As an Illustrative example, we consider a classic case of latent learning in Tolman's rat mazes\citep{tolman1948cognitive}. Here, rats learned to navigate mazes of very specific shapes, to get to a reward. Simply memorizing the actions required to get to the reward in these mazes would have been sufficient to always receive the reward. \citet{tolman1948cognitive} found however that rats developed a more abstract model, or `cognitive map' of the spatial position of the reward with respect to their starting point. This was evidenced by the fact that the rats find the reward by navigating directly to it in close to a straight line, when the walls of the maze are removed\footnote{The actual experiment did not remove the walls of the maze but replaced it with a maze that contained several radial arms, and found that rats take close to the shortest path to the reward by choosing the right arm.}. In this case, learning the model is like learning the spatial position of the reward with respect to you. Inference in the models is planning ones actions (within constraints like walls of mazes, and wanting to minimize energy spent) in order to get to the reward. The only tasks that the rats are directly trained on requires them to do both end-to-end. This a very characteristic problem in all domains where an abstract model is useful -- we never learn this model directly, but only end-to-end with making some inference in that model. \footnote{This problem has been studied extensively in the literature on reinforcement learning regarding model-based and model-free methods. We discuss this literature briefly in Chapter \ref{chap:amort}.} 

% learn about the world, as well as learn how to think, simultaneously?

We have so far assumed that world knowledge has already been acquired and is encompassed in a model. This model is assumed to be structured, and is an abstract representation of information in an environment. This abstraction allows for efficient learning (via the `blessing of abstraction'\cite{harlow1949formation, kemp2007learning, goodman1983fact}) and relatedly, also allows generalization far outside the range of observed queries. The representations learned are not tied strongly to lower-dimensional structure in the distribution of queries observed so far, but rather to more abstract structure. It is for this very reason that inference in these models is difficult: by being structured and abstract, they can express a vast range of hypotheses. This permits good generalization, but inferring the right hypothesis for a particular problem in this vast space, or representing a distribution over them, is difficult. Generalization and ease of inference therefore trade off. 

If the model itself is also being learned, in parallel with learning how to make inferences in it, a decision needs to be made on how to choose the `right' level of abstraction and structure such that it makes a good trade-off between generalization capacity and ease of inference. This will be influenced strongly by statistics in the environment. The environment will determine what extent and what kinds of generalization are useful: it is not useful to be able to generalize to situations that never occur. The amount of experience, and time pressures for decisions in the environment will also inform how efficient our inferences can and need to be. This thesis studies how to optimize the efficiency of inference procedures by re-using past computation and leveraging structure in the environment given a fixed model, future work should consider the problem of jointly optimizing generalization capacity and inference efficiency, for a more complete picture of the underpinnings of frugal and flexible intelligent behavior. In the same vein as Chapter \ref{chap:causal}, these insights can also be leveraged to rise rise to the `right' kinds of representations in artificial systems.

Ecological rationality, and biased judgments have been studied extensively, and several models for these have been proposed. We have discussed some of these alternatives in this thesis, as well as how many of them fit into the broader framework of amortized inference. Future work can work towards better understanding how these many models fit together and inform each other.

%An advantage of structured models are that they abstract away many details that are `not relevant'. For example in our balls-and-urns domain, it is not necessary to represent the physical structure of the urns. For the purpose of that domain, the urns are simply abstract containers whose physical properties have no significance in the kinds of questions asked. Experience with a balls-and-urns task with wooden urns transfers entirely to one with plastic urns. This abstraction allows generalization. However, which features are `relevant' can change. If we saw the same urn in a task where the goal is to predict the probability that the urn will break when dropped from a certain height and angle, the physical properties of the urn are suddenly relevant in our model of that task, and can no longer be `abstracted away'. Additionally, certain details almost never change, so even though they are `relevant' they need not be represented. For example, we assume the presence of gravity in both tasks: we assume that the balls in the urns will not arbitrarily float out of the urn, and that an urn that is dropped will fall to the ground. Even though this variable -- whether or not gravity is present -- is in fact relevant for the domain, it can still be  The kinds of structure we represent in our models for a domain is heavily influenced by the statistics of the environment?

%A broader implication of actively adapting our inference procedures to environmental structure, is that inference can be `learned' as well, via interaction with the outside world. The transition from `potential' to `realized' knowledge no longer happens entirely inside the mind. This blurs the lines between learning about the world and learning to think -- since both involve learning about some external information, just different kinds of information. 

%`Model-free' approaches on the other hand (including the emergentist connectionist views), blur this distinction entirely. The goal is to directly learn how to react to the world, without an intermediate stop at a model representation.

%Amortization of this inference allows a reconciliation between the two. 
%Do humans meta-learn? Structured and compositional amortization

%With much of the expressive burden being shifted to amortization, we are left with a deeper question. What really is the difference between learning and model, and learning to do inference in it? It leads us to question the notion that the models we learn are abstract and general purpose, agnostic to the questions we will ask of them downstream. To take an extreme example, a complete model of any domain, such that we can make good inferences to almost any downstream query about it, could be to represent each atom in it, and know about its interactions. With adequate computing power, this is sufficient to simulate forward every possible scenario, and make arbitrary inferences. But, in our urns-and-balls domain, it is pointless to represent the structural properties of these balls and urns. We know that for the purpose of that domain, the balls are considered abstract single units, and the urns are simply abstract containers whose physical properties have no significance in the kinds of questions asked. We can abstract away this structure because representing it does not improve our inferences in that domain. If we instead conducted an intuitive physics experiment with the exact same articles, asking question like the probability that the urn will break when dropped from a certain height, or domain, these structural properties suddenly become relevant and need to be represented.
