\relax 
\providecommand\hyper@newdestlabel[2]{}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Amortization gives rise to ecologically rational heuristics}{79}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{tversky1974judgment,slovic1971comparison,grether1980bayes,fischhoff1983hypothesis}
\citation{mellers2001frequency,gigerenzer1996narrow,samuels2012ending}
\citation{tversky1974judgment,gigerenzer1996reasoning,shah2008heuristics}
\citation{dasgupta2017hypotheses,sanborn2016bayesian,griffiths2012bridging}
\citation{gigerenzer2009homo,parpart2018heuristics,belousov2016catching}
\citation{bar1980base,birnbaum1983base,grether1980bayes,kahneman1973psychology}
\citation{phillips1966conservatism,peterson1965sensitivity}
\citation{gigerenzer2008heuristics,marewski2014strategy}
\citation{johnson85,beach1978contingency,lieder2017strategy}
\citation{erev05,rieskamp06}
\citation{marewski2011cognitive,schulz2016simple}
\citation{Griffiths15}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{80}{section.4.1}}
\FN@pp@footnote@aux{11}{80}
\citation{mercier2017enigma}
\citation{dasgupta2017hypotheses}
\citation{gershman2014amortized,dasgupta2018remembrance}
\citation{dayan1995helmholtz,kingma2014auto}
\citation{mnih2014neural,rezende2015variational,paige2016inference}
\citation{mnih2014neural}
\FN@pp@footnote@aux{12}{81}
\FN@pp@footnote@aux{13}{81}
\citation{phillips1966conservatism,edwards1968conservatism}
\citation{peterson1965sample,peterson1965sensitivity,peterson1964uncertainty}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Under-reaction to probabilistic information}{82}{section.4.2}}
\citation{benjamin18}
\citation{edwards1968conservatism}
\citation{peterson1968sampling,wheeler1968subjective}
\citation{grinnell1971bayesian}
\citation{phillips1966conservatism}
\citation{edwards1968conservatism}
\citation{edwards1968conservatism}
\newlabel{eq:bias}{{4.4}{84}{Under-reaction to probabilistic information}{equation.4.2.4}{}}
\citation{ducharme1970response}
\citation{benjamin18}
\citation{ducharme1970response}
\citation{griffin1992weighing,kraemer2004people,benjamin2016model}
\citation{kahneman1973psychology,kahneman1972subjective}
\citation{kahneman1973psychology}
\citation{phillips1966conservatism}
\citation{koehler1996base,barbey2007base}
\citation{grether1980bayes,ganguly2000asset}
\citation{barberis1998model}
\citation{eddy1982probabilistic}
\citation{kennedy1997base}
\FN@pp@footnote@aux{14}{85}
\citation{kahneman1972subjective}
\citation{grether1980bayes}
\citation{grether1980bayes}
\citation{tenenbaum2001rational}
\citation{tversky1983extensional}
\newlabel{eq:bias2}{{4.5}{86}{Under-reaction to probabilistic information}{equation.4.2.5}{}}
\citation{gennaioli2010comes}
\citation{dougherty1999minerva}
\citation{mcclelland1998familiarity,shiffrin1997model}
\citation{fischhoff1984diagnosticity}
\citation{bar1980base,ofir1988pseudodiagnosticity}
\citation{benjamin18}
\citation{peterson1964mode}
\citation{edwards1968conservatism}
\citation{benjamin18}
\citation{massey2005detecting}
\citation{gershman2017complex}
\citation{andrieu2003introduction}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Learning to infer}{88}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Approximate inference}{88}{subsection.4.3.1}}
\citation{denison2013rational,vul2014one,gershman2012multistability}
\citation{sanborn2016bayesian,dasgupta2017hypotheses}
\citation{buesing2011neural,orban2016neural,haefner2016perceptual}
\citation{saeedi2017variational}
\citation{jordan1999introduction}
\newlabel{eq:montecarlo}{{4.6}{89}{Approximate inference}{equation.4.3.6}{}}
\citation{dayan1995helmholtz,kingma2014auto,mnih2014neural,rezende2015variational,paige2016inference}
\citation{whittington2019theories}
\citation{friston2008hierarchical,gershman2019does}
\citation{dayan1995helmholtz,kingma2014auto,mnih2014neural,rezende2015variational,paige2016inference,stuhlmuller2013learning,eslami2014just,wang2018meta,rosenthal2011optimal,marino2018learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Amortization}{90}{subsection.4.3.2}}
\citation{feng2014multitasking,alon2017graph}
\newlabel{eq:amort}{{4.9}{91}{Amortization}{equation.4.3.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf  {Schematics of different inference methods}. (A) Memoryless inference recomputes the variational parameters ${"φ}$ from scratch for each new set of observations, resulting in an approximate posterior $Q_{"φ}$ that is unique for each $d$. (B) Amortized inference allows some variational parameters to be shared across queries, optimizing them such that $Q_{"φ}$ is a good approximation \emph  {in expectation} over the query distribution. (C) Schematic of how we implemented this framework with a neural network function approximator in the Learned Inference Model, with low capacity (1 hidden unit). (D) Schematic of a neural network function approximator in the Learned Inference Model, with high capacity (5 hidden units).\relax }}{92}{figure.caption.32}}
\newlabel{fig:amo_schematic}{{4.1}{92}{\textbf {Schematics of different inference methods}. (A) Memoryless inference recomputes the variational parameters $\phi $ from scratch for each new set of observations, resulting in an approximate posterior $Q_\phi $ that is unique for each $d$. (B) Amortized inference allows some variational parameters to be shared across queries, optimizing them such that $Q_\phi $ is a good approximation \emph {in expectation} over the query distribution. (C) Schematic of how we implemented this framework with a neural network function approximator in the Learned Inference Model, with low capacity (1 hidden unit). (D) Schematic of a neural network function approximator in the Learned Inference Model, with high capacity (5 hidden units).\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Memoryless inference}}}{92}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Amortized inference}}}{92}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Learned Inference Model with 1 hidden unit}}}{92}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Learned Inference Model with 5 hidden units}}}{92}{subfigure.1.4}}
\citation{ranganath2014black}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf  {Schematic demonstration of how the approximate posterior depends on the query distribution}. (A) The true posterior probability $P$ (indicated by colors on the heatmap), as a function of the prior and likelihood for a generative model in which $h \sim \text  {Bernoulli}(p_0)$ and $d|h \sim \text  {Bernoulli}(p_l)$. The contour lines depict the query distribution. (B) The approximate posterior $Q$ computed by the Learned Inference Model, averaged over the query distribution. The approximation is better for areas that are sufficiently covered by the query distribution. (C) The average KL divergence between the true and approximate posteriors. Higher divergence occurs in areas that are covered less by the query distribution. \relax }}{93}{figure.caption.33}}
\newlabel{fig:query_schematic}{{4.2}{93}{\textbf {Schematic demonstration of how the approximate posterior depends on the query distribution}. (A) The true posterior probability $P$ (indicated by colors on the heatmap), as a function of the prior and likelihood for a generative model in which $h \sim \text {Bernoulli}(p_0)$ and $d|h \sim \text {Bernoulli}(p_l)$. The contour lines depict the query distribution. (B) The approximate posterior $Q$ computed by the Learned Inference Model, averaged over the query distribution. The approximation is better for areas that are sufficiently covered by the query distribution. (C) The average KL divergence between the true and approximate posteriors. Higher divergence occurs in areas that are covered less by the query distribution. \relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {True posterior P}}}{93}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Approximate posterior Q}}}{93}{subfigure.2.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {KL$ [Q||P ]$}}}{93}{subfigure.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}The Learned Inference Model}{93}{subsection.4.3.3}}
\citation{dougherty1999minerva,shi2010exemplar,dasgupta2018remembrance,stewart2006decision,hertwig2009description}
\citation{orhan2017efficient}
\citation{feng2014multitasking,alon2017graph}
\FN@pp@footnote@aux{15}{94}
\citation{benjamin18}
\citation{benjamin18}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Understanding under-reaction}{95}{section.4.4}}
\citation{benjamin18}
\citation{grether1980bayes}
\citation{benjamin18}
\citation{benjamin18}
\citation{benjamin18}
\newlabel{eq:sysneg}{{4.10}{96}{Understanding under-reaction}{equation.4.4.10}{}}
\FN@pp@footnote@aux{16}{96}
\citation{benjamin18}
\citation{benjamin18}
\citation{benjamin18}
\citation{massey2005detecting}
\newlabel{RF1}{98}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {Simulation of inferential errors in binary symmetric problems with uniform priors}. $P(h | d)$ represents true posterior probabilities, $Q(h | d)$ represents subjective posterior probabilities. (A) Data aggregated by \cite  {benjamin18}. (B) Learned Inference Model simulations. Left: subjective posterior log-odds vs. Bayesian posterior log-odds. Middle: estimated sensitivity to the likelihood $\mathaccentV {hat}05E{{"α}}_L$ vs sample size $N$. Right: estimated sensitivity to the likelihood vs. diagnosticity ${"θ}$. The shaded curves show the linear and nonlinear (LOESS) regression functions with 95\% confidence bands\relax }}{98}{figure.caption.34}}
\newlabel{fig:fftQCboxplot}{{4.3}{98}{\textbf {Simulation of inferential errors in binary symmetric problems with uniform priors}. $P(h | d)$ represents true posterior probabilities, $Q(h | d)$ represents subjective posterior probabilities. (A) Data aggregated by \cite {benjamin18}. (B) Learned Inference Model simulations. Left: subjective posterior log-odds vs. Bayesian posterior log-odds. Middle: estimated sensitivity to the likelihood $\hat {\alpha }_L$ vs sample size $N$. Right: estimated sensitivity to the likelihood vs. diagnosticity $\theta $. The shaded curves show the linear and nonlinear (LOESS) regression functions with 95\% confidence bands\relax }{figure.caption.34}{}}
\citation{griffin1992weighing}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces \textbf  {Simulation of inferential errors in binary symmetric problems with non-uniform priors}. $P(h | d)$ represents true posterior probabilities, $Q(h | d)$ represents subjective posterior probabilities. Plots show prior log-odds on the x axis, and the subjective prior log-odds calculated as the subjective posterior log-odds adjusted for subjective response to the likelihood (as modulated by $\mathaccentV {hat}05E{{"α}}_L$). (A) Data aggregated by \cite  {benjamin18}. (B) Simulation with low-capacity (2 hidden nodes) Learned Inference Model. (C) Simulation with high-capacity (8 hidden nodes) Learned Inference Model. The shaded curves show the linear and nonlinear (LOESS) regression functions with 95\% confidence bands.\relax }}{99}{figure.caption.35}}
\newlabel{fig:Benj_prior}{{4.4}{99}{\textbf {Simulation of inferential errors in binary symmetric problems with non-uniform priors}. $P(h | d)$ represents true posterior probabilities, $Q(h | d)$ represents subjective posterior probabilities. Plots show prior log-odds on the x axis, and the subjective prior log-odds calculated as the subjective posterior log-odds adjusted for subjective response to the likelihood (as modulated by $\hat {\alpha }_L$). (A) Data aggregated by \cite {benjamin18}. (B) Simulation with low-capacity (2 hidden nodes) Learned Inference Model. (C) Simulation with high-capacity (8 hidden nodes) Learned Inference Model. The shaded curves show the linear and nonlinear (LOESS) regression functions with 95\% confidence bands.\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}The effect of sample size}{99}{subsection.4.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces \textbf  {Simulations of inferential errors with high capacity and a biased query distribution}. $P(h | d)$ represents true posterior probabilities, $Q(h | d)$ represents subjective posterior probabilities. (A) Simulation of high-capacity (8 hidden units) Learned Inference Model. (B) Simulation of low-capacity (2 hidden units) Learned Inference Model with biased query distribution. Left: subjective posterior log-odds vs. Bayesian posterior log-odds. Middle: estimated sensitivity to the likelihood $\mathaccentV {hat}05E{{"α}}_L$ vs. sample size $N$. Right: estimated sensitivity to the likelihood vs. diagnosticity ${"θ}$. The shaded curves show the linear and nonlinear (LOESS) regression functions with 95\% confidence bands.\relax }}{100}{figure.caption.36}}
\newlabel{fig:Benj_control}{{4.5}{100}{\textbf {Simulations of inferential errors with high capacity and a biased query distribution}. $P(h | d)$ represents true posterior probabilities, $Q(h | d)$ represents subjective posterior probabilities. (A) Simulation of high-capacity (8 hidden units) Learned Inference Model. (B) Simulation of low-capacity (2 hidden units) Learned Inference Model with biased query distribution. Left: subjective posterior log-odds vs. Bayesian posterior log-odds. Middle: estimated sensitivity to the likelihood $\hat {\alpha }_L$ vs. sample size $N$. Right: estimated sensitivity to the likelihood vs. diagnosticity $\theta $. The shaded curves show the linear and nonlinear (LOESS) regression functions with 95\% confidence bands.\relax }{figure.caption.36}{}}
\citation{griffin1992weighing}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Stimuli used in \cite  {griffin1992weighing}.\relax }}{101}{table.caption.37}}
\newlabel{tab:GT}{{4.1}{101}{Stimuli used in \cite {griffin1992weighing}.\relax }{table.caption.37}{}}
\citation{grether1980bayes}
\citation{griffin1992weighing}
\citation{griffin1992weighing}
\citation{griffin1992weighing}
\citation{griffin1992weighing}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces \textbf  {Strength and weight in probabilistic judgment}. (A) Regression coefficients reported in \cite  {griffin1992weighing}. (B) Regression coefficients estimated from simulations of the Learned Inference Model. Error bars represent the standard error of the mean.\relax }}{103}{figure.caption.38}}
\newlabel{fig:samplesize}{{4.6}{103}{\textbf {Strength and weight in probabilistic judgment}. (A) Regression coefficients reported in \cite {griffin1992weighing}. (B) Regression coefficients estimated from simulations of the Learned Inference Model. Error bars represent the standard error of the mean.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces \textbf  {Variance explained by strength and weight independently}. These plots show regressions between the log of the strength or weight of the evidence against the log of the posterior log-odds. (A) For samples drawn from the true generative process, the strength explains more variance in the posterior. (B) For the stimuli used in \cite  {griffin1992weighing}, the weight explains almost none of the variance in the log posterior log-odds, whereas the strength explains a much higher amount of the variance. \relax }}{104}{figure.caption.39}}
\newlabel{fig:GTR2}{{4.7}{104}{\textbf {Variance explained by strength and weight independently}. These plots show regressions between the log of the strength or weight of the evidence against the log of the posterior log-odds. (A) For samples drawn from the true generative process, the strength explains more variance in the posterior. (B) For the stimuli used in \cite {griffin1992weighing}, the weight explains almost none of the variance in the log posterior log-odds, whereas the strength explains a much higher amount of the variance. \relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{104}{subfigure.7.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{104}{subfigure.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Manipulating the query distribution}{104}{subsection.4.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{Subjects}{105}{subsection.4.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{Design and procedure}{105}{subsection.4.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces \textbf  {Screen shots of urn experiment}. (A) In the condition with informative priors and uninformative likelihoods, the wheel of fortune had urn probabilities of $0.7, 0.8$, or $0.9$. The proportions of blue balls in the urns was $0.5$ or $0.6$. (B) In the condition with uninformative priors and informative likelihoods, the wheel of fortune had urn probabilities of $0.5$ or $0.6$. The proportions of blue balls in the urns was $0.7, 0.8$, or $0.9$.\relax }}{106}{figure.caption.40}}
\newlabel{fig:urnscreen}{{4.8}{106}{\textbf {Screen shots of urn experiment}. (A) In the condition with informative priors and uninformative likelihoods, the wheel of fortune had urn probabilities of $0.7, 0.8$, or $0.9$. The proportions of blue balls in the urns was $0.5$ or $0.6$. (B) In the condition with uninformative priors and informative likelihoods, the wheel of fortune had urn probabilities of $0.5$ or $0.6$. The proportions of blue balls in the urns was $0.7, 0.8$, or $0.9$.\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Behavioral results}{107}{figure.caption.40}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces \textbf  {Results of urn experiment}. The y-axis shows estimates for the regression coefficients ${"α}_L$ and ${"α}_P$ (see Equation \ref  {eq:sysneg}), and the x-axis represents the experimental condition. (A) Subjects weighted the prior more in the informative prior than in the informative likelihood condition. (B) Subjects weighted the likelihood more in the informative likelihood than in the informative prior condition. (C) The Learned Inference Model weights the prior more in the informative prior condition as compared to in the informative likelihood condition. (D) The Learned Inference Model weights the likelihood more in the informative likelihood condition as compared in the informative prior condition. Error bars represent the standard errors of the regression coefficients.\relax }}{108}{figure.caption.41}}
\newlabel{fig:urnresults}{{4.9}{108}{\textbf {Results of urn experiment}. The y-axis shows estimates for the regression coefficients $\alpha _L$ and $\alpha _P$ (see Equation \ref {eq:sysneg}), and the x-axis represents the experimental condition. (A) Subjects weighted the prior more in the informative prior than in the informative likelihood condition. (B) Subjects weighted the likelihood more in the informative likelihood than in the informative prior condition. (C) The Learned Inference Model weights the prior more in the informative prior condition as compared to in the informative likelihood condition. (D) The Learned Inference Model weights the likelihood more in the informative likelihood condition as compared in the informative prior condition. Error bars represent the standard errors of the regression coefficients.\relax }{figure.caption.41}{}}
\citation{koehler1996base}
\@writefile{toc}{\contentsline {subsubsection}{Modeling results}{109}{figure.caption.41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Manipulating the query distribution between vs. within subjects}{109}{subsection.4.4.3}}
\citation{fischhoff1979subjective}
\citation{birnbaum1983bayesian}
\citation{schwarz1991ease}
\citation{dawes1993equating}
\citation{ajzen1977intuitive}
\citation{kahneman1973psychology}
\citation{ajzen1977intuitive}
\citation{tversky1973availability,bar1980base,lyon1976dominance}
\citation{fischhoff1979subjective}
\citation{lyon1976dominance}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces \textbf  {Base rate neglect within and between subjects}. The y-axis shows the reaction to the prior as measured in predictions from the Learned Inference Model, the x-axis shows the different conditions. Reaction to the prior here is measured by the difference between the responses given to test queries in which the base rate was $85\%$ and those in which the base rate was $15\%$. Thus, a greater difference indicates a stronger reaction to prior information. The model simulations of the within-subjects design show a stronger reaction to the base rates than the simulations of the between-subjects design (which shows no reaction to the base rate at all). Both of these conditions produce under-reaction to the base rate compared to the Bayes-optimal judgment.\relax }}{111}{figure.caption.42}}
\newlabel{fig:FC}{{4.10}{111}{\textbf {Base rate neglect within and between subjects}. The y-axis shows the reaction to the prior as measured in predictions from the Learned Inference Model, the x-axis shows the different conditions. Reaction to the prior here is measured by the difference between the responses given to test queries in which the base rate was $85\%$ and those in which the base rate was $15\%$. Thus, a greater difference indicates a stronger reaction to prior information. The model simulations of the within-subjects design show a stronger reaction to the base rates than the simulations of the between-subjects design (which shows no reaction to the base rate at all). Both of these conditions produce under-reaction to the base rate compared to the Bayes-optimal judgment.\relax }{figure.caption.42}{}}
\citation{bar1980base,fischhoff1984diagnosticity,ofir1988pseudodiagnosticity}
\citation{benjamin18}
\citation{sasaki2007belief}
\citation{beach1970sample}
\citation{massey2005detecting}
\FN@pp@footnote@aux{17}{112}
\FN@pp@footnote@aux{18}{112}
\citation{gershman2017blessing}
\citation{gershman2017blessing}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Extension to a continuous domain}{113}{subsection.4.4.4}}
\FN@pp@footnote@aux{19}{113}
\citation{gershman2017blessing}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces \textbf  {Inferential errors in a continuous domain}. (A) Reanalysis of data from the payoff prediction task collected by \cite  {gershman2017blessing}. (B) Simulations of the Learned Inference Model. Each panel shows subjective updates from prior to posterior (${"Δ}$Data) on the y-axis and the update of a rational (hierarchical) model (${"Δ}$Rational) on the x-axis. Error bars represent the standard error of the mean. Gray lines represent $y=x$.\relax }}{114}{figure.caption.43}}
\newlabel{fig:slotmachine}{{4.11}{114}{\textbf {Inferential errors in a continuous domain}. (A) Reanalysis of data from the payoff prediction task collected by \cite {gershman2017blessing}. (B) Simulations of the Learned Inference Model. Each panel shows subjective updates from prior to posterior ($\Delta $Data) on the y-axis and the update of a rational (hierarchical) model ($\Delta $Rational) on the x-axis. Error bars represent the standard error of the mean. Gray lines represent $y=x$.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Data from humans}}}{114}{subfigure.11.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Simulation results}}}{114}{subfigure.11.2}}
\citation{evans1983conflict,newstead1992source,oakhill1989believability,janis1943relationship}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Further evidence for amortization: belief bias and memory effects}{115}{section.4.5}}
\citation{evans2002background,cohen2017beliefs}
\citation{cohen2017beliefs}
\citation{dasgupta2017hypotheses,lieder2017anchoring}
\citation{cohen2017beliefs}
\citation{cohen2017beliefs}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Belief bias}{116}{subsection.4.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces \textbf  {Belief bias}. Top: experimental data. Bottom: simulations of the Learned Inference Model. (A) Empirical results for the believable condition \citep  {cohen2017beliefs}. (B) Empirical results for the unbelievable condition. (C) Simulated results for the believable condition. (D) Simulated results for the unbelievable condition. The correlation between the actual and estimated posterior is closer to 1 (i.e., exact Bayesian inference) in the believable condition than in the unbelievable condition. The Learned Inference Model reproduces this effect. \relax }}{117}{figure.caption.44}}
\newlabel{fig:cohen}{{4.12}{117}{\textbf {Belief bias}. Top: experimental data. Bottom: simulations of the Learned Inference Model. (A) Empirical results for the believable condition \citep {cohen2017beliefs}. (B) Empirical results for the unbelievable condition. (C) Simulated results for the believable condition. (D) Simulated results for the unbelievable condition. The correlation between the actual and estimated posterior is closer to 1 (i.e., exact Bayesian inference) in the believable condition than in the unbelievable condition. The Learned Inference Model reproduces this effect. \relax }{figure.caption.44}{}}
\citation{dasgupta2018remembrance}
\FN@pp@footnote@aux{20}{118}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Memory effects}{118}{subsection.4.5.2}}
\citation{fox1998belief,tversky1994support,dasgupta2017hypotheses}
\citation{dasgupta2018remembrance}
\citation{dasgupta2018remembrance}
\citation{dasgupta2018remembrance}
\citation{blei2003latent}
\citation{ranganath2014black}
\citation{dasgupta2018remembrance}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces \textbf  {Memory effect}. (A) Observed subadditivity effect in query 2 reported in \cite  {dasgupta2018remembrance}. Cues that were similar to a previous query showed a higher effect than cues that were less similar, indicating strategic reuse of past computation. (B) Simulated subadditivity effect. Provided that the model was trained to exhibit a subadditivity effect in a first query, this effect remained stronger for similar queries than for dissimilar queries. Error bars represent the standard error of the mean. \relax }}{120}{figure.caption.45}}
\newlabel{fig:amo}{{4.13}{120}{\textbf {Memory effect}. (A) Observed subadditivity effect in query 2 reported in \cite {dasgupta2018remembrance}. Cues that were similar to a previous query showed a higher effect than cues that were less similar, indicating strategic reuse of past computation. (B) Simulated subadditivity effect. Provided that the model was trained to exhibit a subadditivity effect in a first query, this effect remained stronger for similar queries than for dissimilar queries. Error bars represent the standard error of the mean. \relax }{figure.caption.45}{}}
\citation{dasgupta2017hypotheses}
\citation{hennig2015probabilistic}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Amortization as Regularization}{121}{section.4.6}}
\citation{zhu_sanborn_chater_2018}
\citation{rasmussen2003baysian}
\citation{vul2014one,dasgupta2017hypotheses}
\FN@pp@footnote@aux{21}{122}
\citation{dasgupta2017hypotheses,gershman2012multistability}
\citation{zhu_sanborn_chater_2018}
\citation{zhu_sanborn_chater_2018}
\citation{erev1994simultaneous,hilbert12}
\citation{edwards1968conservatism}
\FN@pp@footnote@aux{22}{123}
\FN@pp@footnote@aux{23}{123}
\newlabel{binom}{{\TextOrMath  {\textdagger }{\dagger }}{123}{}{Hfootnote.23}{}}
\citation{zhu_sanborn_chater_2018}
\citation{costello2018surprising}
\citation{stewart2006decision}
\citation{zhu_sanborn_chater_2018}
\citation{zhu_sanborn_chater_2018}
\citation{stewart2006decision}
\citation{zhu_sanborn_chater_2018}
\citation{geman1992neural}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces \textbf  {Correction prior.} (A) Simulation results from the correction prior model in \cite  {zhu_sanborn_chater_2018} exhibiting conservatism. Black line represents the optimal response and the colored lines show estimates from different parameterizations of the model. (B) Quadratic relation between the variance of subjective probability estimates and mean subjective probability estimates, as observed by \cite  {zhu_sanborn_chater_2018}. Points show data points from previous empirical studies. The line shows best fit quadratic fit to this data. (C) The Learned Inference Model replicates the conservatism effect. Points represent mean estimates from our model, the pink line represents the best fit linear regression to these points, the black line represents the optimal response. (D) The Learned Inference Model replicates the variance effect. Points represent variance of the subjective responses from our model for different mean subjective responses. The pink line represents the best fit quadratic fit to these points.\relax }}{125}{figure.caption.46}}
\newlabel{fig:CP}{{4.14}{125}{\textbf {Correction prior.} (A) Simulation results from the correction prior model in \cite {zhu_sanborn_chater_2018} exhibiting conservatism. Black line represents the optimal response and the colored lines show estimates from different parameterizations of the model. (B) Quadratic relation between the variance of subjective probability estimates and mean subjective probability estimates, as observed by \cite {zhu_sanborn_chater_2018}. Points show data points from previous empirical studies. The line shows best fit quadratic fit to this data. (C) The Learned Inference Model replicates the conservatism effect. Points represent mean estimates from our model, the pink line represents the best fit linear regression to these points, the black line represents the optimal response. (D) The Learned Inference Model replicates the variance effect. Points represent variance of the subjective responses from our model for different mean subjective responses. The pink line represents the best fit quadratic fit to these points.\relax }{figure.caption.46}{}}
\citation{zhu_sanborn_chater_2018}
\citation{zhu_sanborn_chater_2018}
\citation{griffiths2006optimal,kording2006bayesian,knill1996perception,oaksford2007bayesian}
\citation{griffin1992weighing,benjamin18,kahneman1972subjective,kahneman1973psychology,grether1980bayes}
\citation{gershman2015computational,lieder2019resource}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}General Discussion}{126}{section.4.7}}
\citation{lake2017building}
\citation{cohen2017beliefs}
\citation{dasgupta2018remembrance}
\citation{zhu_sanborn_chater_2018}
\citation{brunswik1955representative}
\citation{simon1955behavioral}
\citation{todd2007environments}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Related Work}{128}{subsection.4.7.1}}
\citation{thomas2008diagnostic}
\citation{thomas2014memory}
\citation{bordalo2017memory}
\citation{gershman2015computational}
\citation{gluck1988conditioning}
\citation{shanks1991connectionist}
\citation{bhatia2017associative}
\citation{koehler1996base}
\citation{brandstatter2006priority}
\citation{todd2002testing}
\citation{sanborn2016bayesian}
\citation{buesing2011neural,haefner2016perceptual}
\citation{dasgupta2017hypotheses,dasgupta2018remembrance}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Integrating with sampling-based approaches}{130}{subsection.4.7.2}}
\citation{de2001variational,gu2015neural}
\citation{dasgupta2017hypotheses}
\citation{lieder2017anchoring,lieder2018empirical}
\citation{li2017approximate,naesseth2017variational,ruiz2019contrast}
\citation{bramley2017formalizing,bramley2018grounding,rule2018learning}
\citation{gershman2014retrospective,gershman2017imaginative,momennejad2018offline}
\citation{villejoubert2002inverse}
\citation{krynski2007role}
\citation{hertwig2009description,hertwig2018experience}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Connections to other models for judgment errors}{132}{subsection.4.7.3}}
\FN@pp@footnote@aux{24}{132}
\citation{krynski2007role,ajzen1977intuitive}
\citation{gigerenzer2011heuristic,dhami2001bailing,reyna2006physician}
\citation{lieder2019resource,gershman2015computational}
\citation{lieder2018overrepresentation,lieder2017anchoring,parpart2018heuristics}
\citation{schulz2017compositional}
\citation{dasgupta2017hypotheses}
\citation{dasgupta2018remembrance}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Limitations and future directions}{134}{subsection.4.7.4}}
\citation{alon2017graph,feng2014multitasking}
\citation{grether1980bayes,ganguly2000asset,phillips1966conservatism}
\citation{barberis1998model}
\citation{gigerenzer1996narrow}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.5}Conclusion}{136}{subsection.4.7.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Implementation details}{136}{section.4.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Blackbox variational inference}{136}{subsection.4.8.1}}
\citation{jordan1999introduction}
\citation{ranganath2014black}
\citation{benjamin18}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Function approximation architecture}{137}{subsection.4.8.2}}
\citation{gershman2017blessing}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Ruling out alternative models in the continuous domain}{139}{section.4.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces \textbf  {Performance of the L-HBM}. Simulation results of a hierarchical Bayesian model that infers the underlying parameters in the the experiment reported by \cite  {gershman2017blessing}. The Y-axis shows the L-HBM's updates from prior to posterior (${"Δ}$Data) and the X-axis shows the update of a rational (hierarchical) model (${"Δ}$Rational; a HBM that knows the true parameters for the underlying generative process). Error bars represent the standard error of the mean. Gray line represents $y=x$.\relax }}{139}{figure.caption.47}}
\newlabel{fig:App:hierBayes}{{4.15}{139}{\textbf {Performance of the L-HBM}. Simulation results of a hierarchical Bayesian model that infers the underlying parameters in the the experiment reported by \cite {gershman2017blessing}. The Y-axis shows the L-HBM's updates from prior to posterior ($\Delta $Data) and the X-axis shows the update of a rational (hierarchical) model ($\Delta $Rational; a HBM that knows the true parameters for the underlying generative process). Error bars represent the standard error of the mean. Gray line represents $y=x$.\relax }{figure.caption.47}{}}
\FN@pp@footnotehinttrue 
\@setckpt{chapters/chapter3}{
\setcounter{page}{141}
\setcounter{equation}{19}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{9}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{15}
\setcounter{table}{1}
\setcounter{parentequation}{0}
\setcounter{r@tfl@t}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{2}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{pp@next@reset}{25}
\setcounter{@fnserial}{24}
\setcounter{DefaultLines}{2}
\setcounter{DefaultDepth}{0}
\setcounter{L@lines}{0}
\setcounter{L@depth}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{24}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{28}
\setcounter{eu@}{0}
\setcounter{eu@i}{0}
\setcounter{mkern}{3}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{section@level}{0}
}
