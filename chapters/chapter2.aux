\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{griffiths2012bridging,oaksford2007bayesian}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Re-use of previous solutionf for faster inference}{49}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{thaker17,dasgupta17,vul2014one,gershman12,denison13,sanborn2016bayesian,ullman2012theory}
\citation{stuhlmuller2013learning,gershman2014amortized}
\citation{jordan1999introduction}
\citation{dasgupta17,vul2014one}
\citation{dayan1995helmholtz,hinton1995wake}
\citation{rezende2014stochastic,paige2016inference,kingma2013auto,ritchie2016neurally}
\citation{yildirim2015efficient}
\citation{dasgupta17}
\citation{dasgupta17}
\citation{lieder2017empirical,lieder2017anchoring,vul2014one}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Hypothesis generation and amortization}{53}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Monte Carlo sampling}{53}{subsection.2.1.1}}
\citation{saeedi17}
\citation{vul2014one,lieder2017anchoring,gershman2015computational}
\citation{mackay2003information}
\citation{stewart06}
\citation{hamrick15}
\citation{bonawitz14,gershman12,vul08,lieder2017anchoring}
\citation{ullman2012theory}
\citation{suchow2017evolution}
\citation{gershman12,Moreno11}
\citation{fox1998belief}
\citation{tversky94}
\citation{sloman04,hadjichristidis1999opening}
\citation{lieder2012burn,lieder2017empirical}
\newlabel{eq:MH-step}{{2.4}{55}{Monte Carlo sampling}{equation.2.1.4}{}}
\citation{dasgupta17}
\citation{tversky94}
\citation{sloman04}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Unpacking induced biases in human hypothesis generation and evaluation.\relax }}{56}{table.caption.18}}
\newlabel{tab:biases}{{2.1}{56}{Unpacking induced biases in human hypothesis generation and evaluation.\relax }{table.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Demonstration of how MCMC sampling can give rise to sub- and super-additivity for different unpacked versions of the question : ``In the presence of a table, what is the probability that there is also another object starting with C?''. The color gradient indicates probability density. (a) The chain initialized with a typical unpacking starts at `chair', a high probability hypothesis, denoted by a darker shading, while the chain initialized with an atypical unpacking starts at `canoe', a low probability hypothesis, denoted by a lighter shading. (b) For the purposes of illustration we show the same new intermediate probability proposal of `toothbrush' being made to both chains. In the model, this proposal is randomly generated for each chain. (c) Since the probability of `toothbrush' is significantly higher than `canoe' the proposal is accepted by the atypically unpacked chain. But conversely since it is significantly less probable than `chair', is likely rejected by the typically unpacked chain. (d) The tendency for the typically unpacked chain to tarry in the high probability region of the queried object set, gives rise to sub-additivity, whereas the tendency for the atypically unpacked to get easily derailed into regions outside the queried object set gives rise to super-additivity.\relax }}{57}{figure.caption.19}}
\newlabel{fig:demo}{{2.1}{57}{Demonstration of how MCMC sampling can give rise to sub- and super-additivity for different unpacked versions of the question : ``In the presence of a table, what is the probability that there is also another object starting with C?''. The color gradient indicates probability density. (a) The chain initialized with a typical unpacking starts at `chair', a high probability hypothesis, denoted by a darker shading, while the chain initialized with an atypical unpacking starts at `canoe', a low probability hypothesis, denoted by a lighter shading. (b) For the purposes of illustration we show the same new intermediate probability proposal of `toothbrush' being made to both chains. In the model, this proposal is randomly generated for each chain. (c) Since the probability of `toothbrush' is significantly higher than `canoe' the proposal is accepted by the atypically unpacked chain. But conversely since it is significantly less probable than `chair', is likely rejected by the typically unpacked chain. (d) The tendency for the typically unpacked chain to tarry in the high probability region of the queried object set, gives rise to sub-additivity, whereas the tendency for the atypically unpacked to get easily derailed into regions outside the queried object set gives rise to super-additivity.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Different initializations}}}{57}{subfigure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The same proposal made to both chains}}}{57}{subfigure.1.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Proposal gets accepted or rejected (Equation \ref {eq:MH-step})}}}{57}{subfigure.1.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Sub- and super-additivity}}}{57}{subfigure.1.4}}
\citation{greene13}
\citation{lda}
\citation{dasgupta17}
\citation{dasgupta17}
\citation{oliva2007role}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Amortized inference}{59}{subsection.2.1.2}}
\citation{dasgupta17}
\citation{stuhlmuller2013learning,rezende2014stochastic,paige2016inference}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Theory schematic. (Left) Standard, memoryless framework in which a recognition model $Q_{"θ}(h|d)$ approximates the posterior over hypothesis $h$ given data $d$. The recognition model is parametrized by ${"θ}$ (e.g., a set of samples in the case of Monte Carlo methods). Memoryless inference builds a separate recognition model for each query. (Right) Amortized framework, in which the recognition model shares parameters across queries. After each new query, the recognition model updates the shared parameters. In this way, the model ``learns to infer.''\relax }}{61}{figure.caption.20}}
\newlabel{fig:schematic}{{2.2}{61}{Theory schematic. (Left) Standard, memoryless framework in which a recognition model $Q_\theta (h|d)$ approximates the posterior over hypothesis $h$ given data $d$. The recognition model is parametrized by $\theta $ (e.g., a set of samples in the case of Monte Carlo methods). Memoryless inference builds a separate recognition model for each query. (Right) Amortized framework, in which the recognition model shares parameters across queries. After each new query, the recognition model updates the shared parameters. In this way, the model ``learns to infer.''\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Two amortization strategies}{61}{subsection.2.1.3}}
\citation{nosofsky1986attention,medin1978context}
\citation{dasgupta17}
\citation{dasgupta17}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Simulation of subadditivity and superadditivity effects under sample-based (top) and summary-based (bottom) amortization strategies. In all panels, the y-axis represents the unstandardized effect size for $\mathcal  {Q}2$. Left panels show the effects of changing the sample size for $\mathcal  {Q}1$; right panels show the effects of changing the sample size for $\mathcal  {Q}2$. When sample size for one query is changed, sample size for the other query is held fixed at 230 \citep  [the sample size estimated by][]{dasgupta17}.\relax }}{63}{figure.caption.21}}
\newlabel{fig:changeN}{{2.3}{63}{Simulation of subadditivity and superadditivity effects under sample-based (top) and summary-based (bottom) amortization strategies. In all panels, the y-axis represents the unstandardized effect size for $\mathcal {Q}2$. Left panels show the effects of changing the sample size for $\mathcal {Q}1$; right panels show the effects of changing the sample size for $\mathcal {Q}2$. When sample size for one query is changed, sample size for the other query is held fixed at 230 \citep [the sample size estimated by][]{dasgupta17}.\relax }{figure.caption.21}{}}
\citation{dasgupta17}
\citation{dasgupta17,thaker17}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Adaptive amortization}{65}{subsection.2.1.4}}
\citation{dasgupta17}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Experiment 1}{66}{section.2.2}}
\citation{dasgupta17}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Participants}{67}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Procedure}{67}{subsection.2.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Experimental setup. Participants were asked to estimate the conditional probability using a slider bar within a 20-second time limit.\relax }}{68}{figure.caption.22}}
\newlabel{fig:screenshot}{{2.4}{68}{Experimental setup. Participants were asked to estimate the conditional probability using a slider bar within a 20-second time limit.\relax }{figure.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Experimental stimuli and queries for Experiment 1.\relax }}{68}{table.caption.23}}
\newlabel{tab:scenarios}{{2.2}{68}{Experimental stimuli and queries for Experiment 1.\relax }{table.caption.23}{}}
\citation{dasgupta17}
\citation{dasgupta17}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Results}{69}{subsection.2.2.3}}
\citation{dasgupta17}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Experiment 1: Differences between $\mathcal  {Q}2$ responses for each condition and an average packed baseline. A negative relative mean estimate indicates a superadditivity and a positive relative mean estimate a subadditivity effect. Error bars represent the standard error of the mean.\relax }}{71}{figure.caption.24}}
\newlabel{fig:meandiffs1}{{2.5}{71}{Experiment 1: Differences between $\mathcal {Q}2$ responses for each condition and an average packed baseline. A negative relative mean estimate indicates a superadditivity and a positive relative mean estimate a subadditivity effect. Error bars represent the standard error of the mean.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Discussion}{72}{subsection.2.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Trial-by-trial analyses of Experiment 1. Difference between $\mathcal  {Q}1$ responses and true probability (as assessed by our MCMC model) plotted against the same quantity for $\mathcal  {Q}2$. Lines show the least-squares fit with standard error bands.\relax }}{73}{figure.caption.25}}
\newlabel{fig:correspondence1}{{2.6}{73}{Trial-by-trial analyses of Experiment 1. Difference between $\mathcal {Q}1$ responses and true probability (as assessed by our MCMC model) plotted against the same quantity for $\mathcal {Q}2$. Lines show the least-squares fit with standard error bands.\relax }{figure.caption.25}{}}
\citation{thaker17,dasgupta17}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Experiment 2}{74}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Participants}{74}{subsection.2.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Experimental stimuli and queries for Experiment 2.\relax }}{75}{table.caption.26}}
\newlabel{tab:scenarios2}{{2.3}{75}{Experimental stimuli and queries for Experiment 2.\relax }{table.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Procedure}{75}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Results}{76}{subsection.2.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Experiment 2: Differences between $\mathcal  {Q}2$ responses for each condition and an average packed baseline. A negative relative mean estimate indicates a superadditivity and a positive relative mean estimate a subadditivity effect. Error bars represent the standard error of the mean.\relax }}{78}{figure.caption.27}}
\newlabel{fig:meandiffs2}{{2.7}{78}{Experiment 2: Differences between $\mathcal {Q}2$ responses for each condition and an average packed baseline. A negative relative mean estimate indicates a superadditivity and a positive relative mean estimate a subadditivity effect. Error bars represent the standard error of the mean.\relax }{figure.caption.27}{}}
\citation{gershman2015computational,griffiths2015,lieder2017anchoring}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Discussion}{80}{subsection.2.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Trial-by-trial analyses of Experiment 2. Relationship between difference between $\mathcal  {Q}1$ responses and true probability (as assessed by our MCMC model) and $\mathcal  {Q}2$ responses and true probability. Lines show the least-squares fit with standard error bands.\relax }}{81}{figure.caption.28}}
\newlabel{fig:correspondence2}{{2.8}{81}{Trial-by-trial analyses of Experiment 2. Relationship between difference between $\mathcal {Q}1$ responses and true probability (as assessed by our MCMC model) and $\mathcal {Q}2$ responses and true probability. Lines show the least-squares fit with standard error bands.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Experiment 3}{82}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Participants}{82}{subsection.2.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Experimental stimuli and queries for Experiment 3. Kullback-Leibler (KL) divergence between the posteriors for $\mathcal  {Q}1$ and $\mathcal  {Q}2$ are shown in parentheses.\relax }}{83}{table.caption.29}}
\newlabel{tab:scenarios3}{{2.4}{83}{Experimental stimuli and queries for Experiment 3. Kullback-Leibler (KL) divergence between the posteriors for $\mathcal {Q}1$ and $\mathcal {Q}2$ are shown in parentheses.\relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Procedure}{83}{subsection.2.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Results}{83}{subsection.2.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Experiment 3: Differences between $\mathcal  {Q}2$ responses for each condition and an average packed baseline. A negative relative mean estimate indicates a superadditivity effect and a positive relative mean estimate a subadditivity effect. Error bars represent the standard error of the mean.\relax }}{85}{figure.caption.30}}
\newlabel{fig:meandiffs3}{{2.9}{85}{Experiment 3: Differences between $\mathcal {Q}2$ responses for each condition and an average packed baseline. A negative relative mean estimate indicates a superadditivity effect and a positive relative mean estimate a subadditivity effect. Error bars represent the standard error of the mean.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Discussion}{85}{subsection.2.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Trial-by-trial analyses of Experiment 3. Relationship between difference between $\mathcal  {Q}1$ responses and true probability (as assessed by our MCMC model) and $\mathcal  {Q}2$ responses and true probability. Lines show the least-squares fit with standard error bands.\relax }}{86}{figure.caption.31}}
\newlabel{fig:correspondence3}{{2.10}{86}{Trial-by-trial analyses of Experiment 3. Relationship between difference between $\mathcal {Q}1$ responses and true probability (as assessed by our MCMC model) and $\mathcal {Q}2$ responses and true probability. Lines show the least-squares fit with standard error bands.\relax }{figure.caption.31}{}}
\citation{dasgupta17}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}General Discussion}{87}{section.2.5}}
\citation{gershman2014amortized}
\citation{lieder2012burn,vul2014one,griffiths2015,gershman2015computational}
\citation{heit2011predicting,hayes2013relationship,hayes2013similar,hawkins2016dynamic}
\citation{heit2011predicting}
\citation{jazayeri2007new}
\citation{stocker2008bayesian,luu2016choice}
\citation{fleming2017self}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Related work}{88}{subsection.2.5.1}}
\citation{lieder2018overrepresentation,gershman2010neural,lieder2012burn,vul2014one}
\citation{wang2013quantum,wang2014context}
\citation{trueblood2011quantum}
\citation{wang2013quantum}
\citation{costello18}
\citation{marchiori15}
\citation{costello18}
\citation{costello18}
\citation{Dougherty1999,Thomas2008,thomas2014memory,dougherty2003hypothesis,dougherty2003probability}
\citation{dasgupta17}
\citation{dasgupta17}
\citation{daw2005uncertainty,daw2011model,kool2017cost}
\citation{keramati2016adaptive}
\citation{stuhlmuller2013learning,rezende2014stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Future directions}{91}{subsection.2.5.2}}
\@setckpt{chapters/chapter2}{
\setcounter{page}{95}
\setcounter{equation}{5}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{6}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{4}
\setcounter{parentequation}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{4}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{DefaultLines}{2}
\setcounter{DefaultDepth}{0}
\setcounter{L@lines}{0}
\setcounter{L@depth}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{11}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{18}
\setcounter{eu@}{0}
\setcounter{eu@i}{0}
\setcounter{mkern}{4}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{section@level}{0}
}
