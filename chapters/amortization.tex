%!TEX root = ../dissertation.tex
\chapter{Amortization: Memory as a computational resource}
\label{chap:amort}

In this chapter, I will discuss first what the computational problem that amortization addresses. I then discuss how it links to ecological rationality, and then outline some concrete algorithms for how it might be implemented -- focusing on amortization in the approximate algorithms for probabilistic inference discussed in Chapter \ref{chap:approx}. I then discuss the implicit invocation of amortized inference in models of human cognition, focusing in particular on two domains: Inference and planning. Planning can be seen as a specific case of inference\citep{botvinick2012planning}, but these have historically been studied as separate problems, with various approaches to each developing independently. So we first address amortization in these two separately and then discuss how the methods developed in each can better relate and inform one another, when seen through the common lens of amortization.

\section{Two kinds of knowledge}

Amortization means to spread the costs over a period of time. In our case, the costs are computational costs. In this section, I discuss what these computational costs being amortized are. 

There are two distinct aspects to having a response to a query: a) to have all the relevant information from the external world and b) to have internally computed the response based on this information. Once the relevant information is learned, all knowledge possible from that information is termed `possible knowledge' -- for example, once we learn the rules of mathematics, the proofs of all the theorems in the world are included in this `possible knowledge'. However, only a small subset of these proofs can and will actually be computed by an average mathematician. This subset is termed `realized knowledge'. Arriving at each of these proofs requires some work -- even if we already possess all possible knowledge in a domain, going from that to realized knowledge requires computation. These computations cost resources. It is these costs are what we wish to amortize.

\section{Amortization and ecological rationality}

In different environments, with different distributions of queries, different levels of amortization can be optimal. For example, if a query is rarely experienced, there may not exist adequate previous experience from which to build and recover a strong response-prior. Further, the cost of running an inference in the rare cases that it is encountered may not be worth the trade-off of storing and recovering pre-computed solutions or procedures. However, with more experience, a stronger response-prior can develop, allowing fast off the shelf re-use of solutions that have worked well in the past. As phrased in \cite{logan1988toward}:
\begin{quote}
``Automaticity is memory retrieval: Performance is automatic when it is based on singlestep direct-access retrieval of past solutions from memory. The theory assumes that novices begin with a general algorithm that is sufficient to perform the task. As they gain experience, they learn specific solutions to specific problems, which they retrieve when they encounter the same problems again.''
\end{quote}

More experience in a certain domain and greater familiarity with it leads to easier and faster inferences, \textit{even after the stage of acquiring all `possible' knowledge about the domain is complete}. This can be explained by amortization, i.e. stronger response-priors built over time, that aid and speed up internal computations required to form a compiled response. Another prediction of this kind of re-use is that if the queries faced in our experience of the word are biased, the response-prior learned might end up being much simpler, and not fully represent the complexities in the space of all possible query-response pairs. This is because it only sees a small subset of that space. This might be very adaptive to the common queries but be very misleading in uncommon ones. This is related to the notion of ecological rationality -- where seemingly simple responses, that engage computations much less expensive than full inference in a world-model, can give surprisingly good results in the real world. This might be because the real world doesn't ask random uncorrelated queries and actually provides a lot of experience in certain query spaces. Within these limited spaces, it's possible to learn good -- and simple -- response priors. Biasing responses with these provides good responses to the most common queries. This also allows grounds for explaining the observed poor performance in humans when faced with uncommon queries. If the computations were being carried out from scratch using a mental model, these would be no difference between common and uncommon queries.


% We outline the computational challenge in going from possible to realized knowledge in each of these cases, and then explore how various methods to solve this problem can be seen as amortization. 
% 
% We discuss how different amortization strategies can arise depending on the structure in the environment and limitations on computational resources, and outline empirical evidence for these behaviours in natural intelligence.

\section{Algorithms for amortization}


\section{Amortization in cognitive science}

\subsection{Amortization in Inference}

In many real-world situations, people have to combine information from many sources, in order to make judgments about probabilistic outcomes. Bayesian inference provides a normative computational account of what should be done. An extensive research program has sought to establish signature of the use of abstract, normative baysian inference in humans. Exactly computing this is difficult. Information about the world can be encoded in teh form of a generative model P(h, d) that consists of a likelihood P(d | h) of observed data d, given latent hypothesis h, and a prior over latent hypothesis h. This encompasses as `possible' knowledge. However, to know the probability of different underlying hypotheses h, given the observed data d, the following computations need to be done.
Given data $d$, Bayes' rule stipulates how a rational agent should update its prior probabilistic beliefs $P(h)$ about hypothesis $h$:
\begin{align}
    P(h|d) = \frac{P(d|h)P(h)}{\sum_{h'} P(d|h') P(h')},
\end{align}
where $P(h|d)$ is the agent's posterior distribution, expressing its updated beliefs, and $P(d|h)$ is the likelihood, expressing the probability of the observed data under candidate hypothesis $h$. In case of many underlying hypotheses (as is often the case), this denominator is difficult and ofetn intractable to calculate. Therefore, despite having the requisite `possible ' knowledge in the form of the generative model P(h,d), the `realized' knowledge of the posterior probabilities are non-trivial abd remain a computational challenge.

Correspondingly, despite much evidence that people have learned and udnerstood the generative mdoel, an extensive and long-running program of research demonstrates that peoples' posterior inferences deviate from the bayesian optimal in many ways. These have historically been understood as the use of heuristics and biases.

While several biases have more recently been modelled as the optimal use of finite-resources, the modelling of heuristic strategies has remained more elusive. While several ratioanl meta-reasoning approaches provide a computational framework for the metacognitive task of choosing a heuristic to use in each situation, they cannot yet answer the question of where the heuristics come from or how they are selected.

The use of heuristic-based strategies has been observed in experts in various domains such as legal decisions \citep{dhami2001bailing}, and medicine \citep{reyna2006physician}, where the most general, normative decision strategy involves several variables and is often too complex for easy full consideration. \cite{garcia2009take} find, in a domain of crimiality and law enforcement, that expert behavior is better described by heuristic strategies, while laypeople are better decribed by a full regression to the relevant variables.  In realms of limited resources, heuristic strategies have in fact been found to give comparable accuracies to the application of the most general model that accounts for all factors \citep{gigerenzer2011heuristic}, and been characterized as optimal approximations under such constraints \citep{belousov2016catching, parpart2018heuristics}. These provide preliminary evidence that these heuristic strategies are in fact learned, from experience. 

% Much empirical evidence from various domains suggest in fact that people consistently make judgments and decisions that reflect a violation the structure of the learned abstract world model. \footnote{Some of these deviations have also been explained as approximate solutions to the difficult problem of exactly computing responses in mental models. Our approach allows us to relate these two approaches, and we expand on this in the General Discussion.}


In thinking of a heuristic as a learned function from query to response, the role of memory becomes more obvious. Previous experience stored in memory, provides a set of input queries and output responses -- where the response was either computed using a noisy general purpose, normative strategy, or externally provided say by a teacher. These pairs can be used to fit a function, i.e. find a heuristic, that is separate from the normative decision strategy. This also alleviates the startegy selection problem because it isn't that people come in with an arsenal of heuristics and then select one, but rather memory and historical experience in that domain determine the heuristic to be used. Like learning a value function. Previous work \citep{gluck1988conditioning, dasgupta2019theory, shanks1991connectionist} has demonstrated that different approximate solutions to general-purpose probabilistic inference can arise from such a mapping, and that these vary depending on the distribution of queries. Also integrates with episodic re-use of specific examples (connection to \cite{dasgupta2018remembrance}), by informing when to re-use (analogous to episodic RL, learning a similarity function).



\subsection{Amortization in Planning}

While the main focus of this paper is on inference and not planning, amortization has implicitly played a very crucial role in our understanding of how planning might be implemented. In this section, we briefly review the literature on how amortization fits into the existing framework for approximate planning, and draw parallels to our primary discussion on amortization in inference.

The goal of planning is to leverage information one has about the world in order to achieve a specific goal. This problem has been commonly studied in a Markov Decision Process paradigm, in the context of Reinforcement Learning (RL). The framework here is that an agent has a fixed set of actions ($a \in A$) it can perform on the world, in order to receive reward (r). This reward depends on both the state of the world the agent is in ($s \in S$) and current action as defined by a reward function $r = R(s, a)$. The goal is to maximize this earned reward, over some fixed or discounted time horizon. The effect of an action will depend on the state of the world the agent is in, and can cause a transition from one state $s$ to another state $s'$ as defined by a transition model $P(s' | s, a)$. A common assumption, that makes the problem more tractable, is that the transitions and reward structure are Markov -- i.e. that when taking action $a$ in state $s$, which state we transition to ($P(s' | s, a)$) as well as the reward earned ($R(s,a)$) depend only on the current state and current action and is independent of the history of any previous states or actions.

In a planning problem, the transition probabilities and reward functions are known -- this consists of all the possible knowledge about this domain. The challenge in converting this to realized knowledge is to construct a \textit{policy} $\pi: S \rightarrow A$ that determines what action to take at each state, in order to maximize the reward earned. The number of possible trajectories through the MDP is exponentially large, and evaluating every possible policy -- even given all the possible knowledge -- is computationally very challenging and often entirely intractable. In a traditional RL setting, the transition probabilities and reward functions are not assumed to be known, and also needs to be learned from experience (see box on Learning about the world vs Learning to think). Most established approaches in this domain therefore do not keep these two kinds of learning distinct -- we will primarily be discussing the planning problem, but most discussions will relate back to the broader RL problem as well.

% In the most general form of the problem, the agent does not know which state it is in and must additionally infer the state it is in from observations O. If an observation does not uniquely identify the underlying state, then the Markov assumption no longer holds. The goal is to learn a policy i.e. a mapping from observations to action, that maximizes reward.

The two poles in traditional approaches to the RL problem are: a) learn a world model i.e. learn the states, as well as the reward and transition structures. Using these, predict the outcome of possible actions in this model and choose actions that lead to reward. This approach is often termed model-based RL, where learning about the world and learning to think are kept entirely distinct. Additionally, finding the right response at run-time (i.e. planning) in this approach is not amortized and is an expensive process that has to be done in real-time. Or b) directly learn which actions lead to reward through experience. This is done by trial and error and is often termed model-free RL, where learning about the world and learning to think (i.e. planning) are entirely merged. The simplest form of model-free RL is to just learn a table of states and actions, along with their corresponding value. Therefore, in addition to the two kinds of learning being merged, this also differs from model-based RL in that finding the right response at run-time is entirely amortized -- it simply involves looking up a table.

The first is more flexible, since it allows one to predict the effects of actions in situations never previously encountered directly, and is quickly able to adapt its policy to changing reward structures. The second is less flexble since (in its most basic form), requires each observation-action pair to have been experienced in order to know its value. Further, if the reward structure changes, then all of these observation-action pairs need to be revisited in order to revise the new values of each of these. On the flip-side, the model-based strategy is very computationally expensive at run-time -- it requires the simulation of several steps into the future, and a combinatorially high number of possible trajectories to evaluate the value of an action. Whereas the model-free strategy uses previous experience in the domain to inform future the value of actions, and saves significant computational energy at run-time.

The model-based representation is "possible knowledge" that is, it contains within it all the required information to maximize reward in the environment. However, compiling it into an actionable policy, is computationally expensive. The model-free representation however contains compiled knowledge that directly informs actionable policy, at the cost of throwing away the flexibility of abstraction. One way to integrate these advantages is to use a learned world model to perform planning in advance, and compile the results into rapidly recoverable query-response rules which can be easily used for real-time inference. This architecture is often referred to as DYNA, as was discussed briefly in the previous section. In this section, rather than overlaying these two approaches as with DYNA, we explore instead the continuum between these two diametrically opposite approaches. We investigate how various approaches to approximate planning can be seen as different points on an amortization spectrum. Here, the distinction between the two kinds of knowledge -- possible and compiled / realized -- is less clear since both have to be acquired. Instead we focus on how different frameworks trade-off abstraction (i.e. a flexible representation that encompasses all `possible' knowledge) and computational resources (i.e. a compiled response that represents `realized' knowledge and directly informs strategies). We outline how different points on this spectrum might be optimal for different environments, depending on the complexity of the world and the variability in the query distribution.

Model-free value functions, pruning. Episodic RL.
Temporal abstraction, options. Successor representation -- since much of the important structure in the environment is baked into the representation, simple computations with this representation can give good responses. 

\subsection{A fruitful exchange}

