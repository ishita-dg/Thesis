%!TEX root = ../dissertation.tex
\chapter{General Introduction}
\label{chap:intro}

The flexibility and efficiency of human intelligence far surpasses what is possible with current artificially intelligent systems. Significant progress towards understanding this flexibility has been made possible by structured, Bayesian models of cognition. Expand on how structured they can be.

However, a crucial shortcoming of these models is intractable learning and intractable inference. Despite significant advances in how they explain behavior in humans, they remain "as-if" models that do not provide a complete model of how humans actually work. This view has been fueled by much evidence that shows that people aren't quite doing this anyway, with biases and heuristics. 

Further, the large computational costs of these models has also torpedoed the adoption of such structured models in modern AI.

A crucial shortcoming of these approaches is that they focus purely on the computational processes taking place within the human mind. Even approaches that make process-level claims, focus solely on cognitive mechanisms. What is missing is a serious consideration of the environments in which intelligent systems act. The analogous concern also exists with the study of AI, where progress is gauged largely by how new models and mechanisms perform on unchanging, often arbitrarily selected, standardized data sets. A crucial insight is that humans are not a general purpose computing machine, as several "fully normative" models would have you believe. Therefore, neither should AI that hopes to emulate human intelligence. Humans have developed -- both over evolutionary and lifetime time scales -- to solve specific kinds of problems. Our environment strongly affects our learning and inference procedures. This is further highlighted by the fact that humans have limited computing resources, and therefore are in more dire need to adapt to their environments and selectively optimize performance where it matters. Learning about things that don't occur often is often not even possible due to limited experience, and is largely not worth it given the cost on computation.  define ecological rationality.

This leads us to a solution to the conundrum that humans are flexible, but models that reflect this flexibility are intractable. This thesis expands on this. I incorporate the role of the environment in how Bayesian models of cognition can be approximated. This provides a mechanistic accounts of how inference might be carried out within resources limitations. We also find that several errors observed in human inference can be modeled by ecologically rational limitations on general-purpose approximate inference, as well as test new predictions of these models in new experiments.

Herb Simon / Brunswick = importance f the environment.

I also discuss how a deeper understanding of the role of the environment in shaping the inference procedures of intelligent systems, can inform the study and engineering of artificially intelligent systems. AI has similarly largely neglected the crucial role of the environment in shaping the inference strategies learned by intelligent systems, choosing instead to focus only on the internal mechanisms. I demonstrate how analysis and manipulation of learning environments can provide insight into how to artificially develop central tenets of intelligence like causal inference and language, as well as inform the underlying mechanisms of how humans acquire and implement these complex behaviors.

The direct contributions of this thesis are threefold. First I develop a framework based on ecologically rational approximate inference that alleviates the intractability of inference in structured Bayesian models. This allows these powerful models to be taken more seriously, as more than as-if models but rather as mechanistic models of human cognition. Second, the process of approximation leads to specific kinds of deviations from normativity, that allow us to model various context-dependent deviations from normativity observed in human cognition. And finally, I suggest entirely new ways to understand and engineer artificial systems, via manipulation of the environments in which they learn and function. This also suggests links between the analysis of ecological rationality in humans and in machines, leading to new lines of research into how to understand both.

This thesis moves toward a solution to these concerns via by better considering the environment in which intelligent systems act. I will study how utilizing structure in the environment can bring plausibility to Bayesian models of human cognition by making inference in them more feasible. I show how this underlying structure can be learned and leveraged by considering the inference itself a learning problem, and using past inferences to improve future inferences via the re-use of computations (also called amortization of computations). This addresses several of the criticisms of computational-level theories that invoke structured Bayesian models by alleviating their computational costs, paving the path for these to be mechanistically grounded theories of human cognition. I demonstrate, with several new empirical results, that humans do indeed utilize environmental structure in their inferences via re-use of computation (Chapters \ref{chap:MCMC_amort} and \ref{chap:LTI}). 

The second crucial contribution of this thesis is that it can explain both how and why humans can sometimes be so close to optimal, and in other domains (with the same cognitive resources), so biased -- and biased in so many different context-sensitive ways. While previous approaches have attempted to bridge this gap, they fail to explain this domain sensitivity. By allowing for re-use of computations, we can account for both these aspects of domain sensitivity as follows. First, the extent of experience in a domain informs how easy and effective it is to re-use computations in that domain. More effective re-use in turn predicts better inferences with the same run-time resources. Therefore, if the amount of experience in different domains vary, we can get close to optimal inferences in a familiar domain, with poor inferences in unfamiliar ones. Second, the underlying structure discovered in different domains might be different. This can give to different context-sensitive inference procedures that make different kinds of errors and biases.

Potentially move some of the grandiose things to the conclusion, and just set up the problem here.


\paragraph{Outline}

First and foremost, in Chapter \ref{chap:psych}, I review the theoretical framework surrounding human probabilistic inference. I first review rational Bayesian accounts, followed by a review of boundedly rational approaches. Finally I discuss ecological rationality -- a specific subset of these boundedly rational approaches that explicitly posits adaptation to structure in the environment. I highlight the contributions and shortcomings of these approaches, and outline how this thesis suggests a more complete picture by unifying their complementary advantages. In Chapters \ref{chap:approx} and \ref{chap:amort} I review the technical and conceptual background for this thesis. In Chapter \ref{chap:approx} I cover technical background on approximate probabilistic inference. In Chapter \ref{chap:amort} I introduce a formal notion of computational re-use i.e. amortization and how it can be used within algorithms for approximate inference. I also discuss in each chapter the history of these concepts as applied -- explicitly or implicitly -- in models of human cognition. In subsequent chapters, I demonstrate how these concepts can be used to build new and better models of human inference.

%In this thesis, I show how adaptation to environments (as necessary under resource constraints) can both a) render inference more tractable on average, and b) explain deviations from normative Bayesian inference.

In Chapter \ref{chap:MCMC}, I study human inferences in large hypothesis spaces. This is a challenging problem, where exact inference is intractable. I demonstrate that a sample-based approximation under ecologically rational constraints, can replicate the specific kinds of biases observed in human inference in such large hypothesis spaces. 
% This model relies on two crucial components to replicate these deviations from normativity: first, a small number of samples, and second, initialization based on question framing. I argue that these components are ecologically rational: humans have limited cognitive resources, hindering the possibility of a large number of samples in the approximation, and initialization based on question framing is reasonable since the context of queries in the environment are often related to and informative of the optimal response. 
Further, in small hypothesis spaces, this model returns optimal responses. This allows us to parsimoniously explain both the rationality as well as various kinds of irrationality of human inference within the same framework. Chapter \ref{chap:MCMC_amort} expands on the role of the environment in such sampling frameworks. I empirically demonstrate, as well as models for, re-use of computation in consecutive related queries. This kind of re-use -- or amortization -- is ecologically rational since queries in the real world are often related, and the best use of limited resources is to re-use previously completed computations.

Chapter \ref{chap:LTI} studies the role of amortization in greater detail. In particular, I discuss amortization in a variational framework, which is more amenable to flexible re-use than sampling. I demonstrate how this framework can replicate several cognitive biases that involve context-sensitive sub-optimal reactions to different sources of information, 
% -- including base-rate neglect, conservatism and belief bias -- 
as well as various various other effects.
% such as the response variability in probabilistic judgments, effects of experimental design, and certain memory effects. 
This work posits an algorithmic approach, via amortization of inference, to understanding ecologically rational heuristic behavior. I also discuss how this approach can be combined with previously discussed sampling approaches, to jointly and parsimoniously account (with mechanistic commitments) for a wide range of biased inferences while still retaining the capacity for optimal inference in the limit of infinite experience in the environment, or infinite computational resources.

Chapters \ref{chap:sentences} and \ref{chap:causal} shift gears to focus on the role of the environment in studying and improving modern artificial intelligence (AI).  In Chapter \ref{chap:sentences}, I consider a system for natural language processing. I first discuss this system, like many modern deep-learning based AI systems, can be interpreted as performing amortized inference. I then demonstrate how ecological rationality in this inference procedure can explain the errors this system makes -- analogous to how they explained human biases in Chapter \ref{chap:LTI}. I outline an approach that uses this insight to better asses and potentially also improve these systems. In Chapter \ref{chap:causal}, I demonstrate how we can leverage the strong influence the environment has on the inference procedures learned, to engineer the inference procedures we want. I show that manipulation of the environment of a very simple learning architecture, can give rise to complex behaviors including causal inference procedures and active information seeking behavior. The strong control we have over the environments encountered by artificial systems also allows new investigations into ecological rationality, and how it shapes human cognitive abilities.