\relax 
\providecommand\hyper@newdestlabel[2]{}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Learning amortized alorithms in machines}{174}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{introduction}{{6}{174}{Learning amortized alorithms in machines}{chapter.6}{}}
\citation{waldmann2013causal,cartwright2004causation}
\citation{saxe2006perception,meltzoff2007infants,bonawitz2010just,carey2009origin}
\citation{rehder2014independence,rehder2017failures,fernbach2010neglect,fernbach2013cognitive}
\citation{lombrozo2010causal}
\citation{krynski2007role}
\citation{duan2016RL2,wang2016}
\citation{wang2016,wang2018}
\citation{ritter2018been}
\citation{ortega2019meta}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{175}{section.6.1}}
\citation{geiger1990identifying,spirtes2000causation,verma1991equivalence}
\citation{jordan2002graphical}
\citation{Dasgupta644534,gershman2015computational,gigerenzer2009homo,lieder2017strategy,todd2007environments}
\citation{bengio2019meta,heckerman1995learning,magliacane2018domain,parascandolo2017learning,mitrovic2018causal}
\citation{goodman2011learning}
\citation{grant2018recasting}
\citation{goodman2011learning}
\citation{schulz2012finding,bramley2018grounding}
\citation{mcclelland2010letting}
\citation{siegelmann1995computational,hornik1991approximation}
\citation{rehder2014independence,rehder2017failures,fernbach2010neglect,fernbach2013cognitive}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Related Work}{176}{section.6.2}}
\citation{bengio2019meta}
\citation{denil2016learning}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Problem Specification}{177}{section.6.3}}
\newlabel{sec:probspec}{{6.3}{177}{Problem Specification}{section.6.3}{}}
\citation{pearl2000,spirtes2000causation,dawid07fundamentals}
\citation{pearl2000,pearl16causal}
\citation{heckerman1995learning}
\citation{pearl2000}
\FN@pp@footnote@aux{28}{178}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Task Setup and Agent Architecture}{179}{section.6.4}}
\newlabel{sec:task}{{6.4}{179}{Task Setup and Agent Architecture}{section.6.4}{}}
\FN@pp@footnote@aux{29}{180}
\citation{hochreiter97long}
\citation{mnih2016}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Experiments}{182}{section.6.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Experiment 1: Observational Environments}{183}{subsection.6.5.1}}
\newlabel{sec:expt1}{{6.5.1}{183}{Experiment 1: Observational Environments}{subsection.6.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces a) Active vs Random Conditional, b)Associative Baseline vs Active Conditional, where intervened node has a parent\relax }}{184}{figure.caption.64}}
\newlabel{fig:expt1_extra}{{6.1}{184}{a) Active vs Random Conditional, b)Associative Baseline vs Active Conditional, where intervened node has a parent\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces \textbf  {Experiment 1. Agents do causal reasoning from observational data.} a) Average performance of the agents tested in this experiment. b) Performance split by the presence or absence of at least one parent (Parent and Orphan respectively) on the externally intervened node. c) Quiz phase for a test CBN. Green (red) edges indicate a weight of $+1$ ($-1$). Black represents the intervened node, green (red) nodes indicate a positive (negative) value, white indicates a zero value. The blue circles indicate the agent's choice. Left panel: The undirected version of ${\cal  G}$ and the nodes taking the mean values prescribed by $p(X_{1:N\setminus j }|X_j=-5)$, including backward inference to the intervened node's parent. The Associative Baseline's choice is consistent with maximizing these (incorrect) node values. Right panel: ${\cal  G}_{\rightarrow X_j=-5}$ and the nodes taking the mean values prescribed by $p_{\rightarrow X_j=-5}(X_{1:N\setminus j }|X_j=-5)$. The Active-Conditional Agent's choice is consistent with maximizing these (correct) node values.\relax }}{185}{figure.caption.65}}
\newlabel{fig:expt1}{{6.2}{185}{\textbf {Experiment 1. Agents do causal reasoning from observational data.} a) Average performance of the agents tested in this experiment. b) Performance split by the presence or absence of at least one parent (Parent and Orphan respectively) on the externally intervened node. c) Quiz phase for a test \CBN . Green (red) edges indicate a weight of $+1$ ($-1$). Black represents the intervened node, green (red) nodes indicate a positive (negative) value, white indicates a zero value. The blue circles indicate the agent's choice. Left panel: The undirected version of ${\cal G}$ and the nodes taking the mean values prescribed by $p(X_{1:N\setminus j }|X_j=-5)$, including backward inference to the intervened node's parent. The Associative Baseline's choice is consistent with maximizing these (incorrect) node values. Right panel: ${\cal G}_{\rightarrow X_j=-5}$ and the nodes taking the mean values prescribed by $p_{\rightarrow X_j=-5}(X_{1:N\setminus j }|X_j=-5)$. The Active-Conditional Agent's choice is consistent with maximizing these (correct) node values.\relax }{figure.caption.65}{}}
\citation{janzing2009telling,hoyer2009nonlinear}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Experiment 2: Interventional Environments}{186}{subsection.6.5.2}}
\newlabel{sec:expt2}{{6.5.2}{186}{Experiment 2: Interventional Environments}{subsection.6.5.2}{}}
\citation{fernbach2013cognitive,fernbach2010neglect}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces \textbf  {Experiment 2. Agents do causal reasoning from interventional data.} a) Average performance of the agents tested in this experiment. See main text for details. b) Performance split by the presence or absence of unobserved confounders (abbreviated as Conf. and Unconf.). c) Quiz phase for a test CBN. See Figure \ref  {fig:expt1} for a legend. Here, the left panel shows the full $\cal  G$ and the nodes taking the mean values prescribed by $p(X_{1:N\setminus j }|X_j=-5)$. We see that the Active-Cond Agent's choice is consistent with choosing based on these (incorrect) node values. The right panel shows ${\cal  G}_{\rightarrow X_j=-5}$ and the nodes taking the mean values prescribed by $p_{\rightarrow X_j=-5}(X_{1:N\setminus j }|X_j=-5)$. We see that the Active-Int. Agent's choice is consistent with maximizing on these (correct) node value.\relax }}{187}{figure.caption.66}}
\newlabel{fig:expt2}{{6.3}{187}{\textbf {Experiment 2. Agents do causal reasoning from interventional data.} a) Average performance of the agents tested in this experiment. See main text for details. b) Performance split by the presence or absence of unobserved confounders (abbreviated as Conf. and Unconf.). c) Quiz phase for a test \CBN . See Figure \ref {fig:expt1} for a legend. Here, the left panel shows the full $\cal G$ and the nodes taking the mean values prescribed by $p(X_{1:N\setminus j }|X_j=-5)$. We see that the Active-Cond Agent's choice is consistent with choosing based on these (incorrect) node values. The right panel shows ${\cal G}_{\rightarrow X_j=-5}$ and the nodes taking the mean values prescribed by $p_{\rightarrow X_j=-5}(X_{1:N\setminus j }|X_j=-5)$. We see that the Active-Int. Agent's choice is consistent with maximizing on these (correct) node value.\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Active and Random Interventional Agents\relax }}{187}{figure.caption.67}}
\newlabel{fig:expt2_active}{{6.4}{187}{Active and Random Interventional Agents\relax }{figure.caption.67}{}}
\FN@pp@footnote@aux{30}{188}
\citation{bengio2019meta,janzing2009telling,hoyer2009nonlinear}
\citation{wang2018}
\citation{andreas2016neural,battaglia2018relational,ganin2018synthesizing}
\citation{hester2017deep,hessel2018multi,espeholt2018impala}
\citation{blaisdell2006causal,gopnik2004theory,premack1994levels}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Discussion and Future Work}{189}{section.6.6}}
\citation{duan2016RL2,ortega2019meta,wang2016}
\citation{krynski2007role,lombrozo2010causal}
\citation{todd2007environments,gigerenzer2009homo}
\citation{fernbach2010neglect,fernbach2013cognitive,rehder2014independence,rehder2017failures}
\citation{hochreiter97long}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Agent architecture}{190}{section.6.7}}
\citation{mnih2016}
\citation{andrychowicz2016learning}
\citation{finn2017model}
\citation{vinyals2016matching}
\citation{santoro2016meta}
\citation{duan2016RL2,wang2016}
\newlabel{sec:meta-formalism}{{6.8}{191}{Formalism for Memory-based Meta-learning \label {sec:meta-formalism}}{section.6.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Formalism for Memory-based Meta-learning }{191}{section.6.8}}
\citation{duan2016RL2,wang2016}
\citation{wang2016,wang2018}
\citation{santoro2016meta}
\citation{pearl88probabilistic,bishop06pattern,kollerl09probabilistic,barber12bayesian,murphy12machine}
\@writefile{toc}{\contentsline {section}{\numberline {6.9}Formalisms for causal inference}{192}{section.6.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9.1}Causal Bayes Nets}{192}{subsection.6.9.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces (a): Directed acyclic graph. The node $X_3$ is a collider on the path $X_1 \rightarrow X_3 \leftarrow X_2$ and a non-collider on the path $X_2\rightarrow X_3\rightarrow X_4$. (b): Cyclic graph obtained from (a) by adding a link from $X_4$ to $X_1$.\relax }}{193}{figure.caption.68}}
\newlabel{fig:indep_HMM}{{6.5}{193}{(a): Directed acyclic graph. The node $X_3$ is a collider on the path $X_1 \rightarrow X_3 \leftarrow X_2$ and a non-collider on the path $X_2\rightarrow X_3\rightarrow X_4$. (b): Cyclic graph obtained from (a) by adding a link from $X_4$ to $X_1$.\relax }{figure.caption.68}{}}
\@writefile{toc}{\contentsline {paragraph}{Assessing statistical independence in Bayesian networks.}{193}{figure.caption.68}}
\citation{pearl2000,pearl16causal}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9.2}An intuitive example of cause-effect reasoning in a CBN}{194}{subsection.6.9.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces (a): A CBN\nobreakspace {}${\cal  G}$ with a confounder for the effect of exercise ($E$) on heath ($H$) given by age ($A$). (b): Intervened CBN\nobreakspace {}${\cal  G}_{\rightarrow E=e}$.\relax }}{194}{figure.caption.69}}
\newlabel{fig:CBN}{{6.6}{194}{(a): A \CBN ~${\cal G}$ with a confounder for the effect of exercise ($E$) on heath ($H$) given by age ($A$). (b): Intervened \CBN ~${\cal G}_{\rightarrow E=e}$.\relax }{figure.caption.69}{}}
\FN@pp@footnote@aux{31}{194}
\citation{pearl16causal}
\FN@pp@footnote@aux{32}{195}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9.3}Counterfactual reasoning}{195}{subsection.6.9.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6.10}RL Baselines}{196}{section.6.10}}
\@writefile{toc}{\contentsline {section}{\numberline {6.11}Additional Experiments}{196}{section.6.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.11.1}Experiment 3: Counterfactual Setting}{197}{subsection.6.11.1}}
\newlabel{sec:expt3}{{6.11.1}{197}{Experiment 3: Counterfactual Setting}{subsection.6.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Experiment 3. Agents do counterfactual reasoning. a) Performance of the agents tested in this experiment. Note that performance can be above 1.0 since the counterfactual agent can theoretically perform better than the optimal interventional baseline, which doesn't have access to noise information. See main text for details. b) Performance split by if the maximum node value in the quiz phase is degenerate (Deg.) or distinct (Dist.). c) Quiz phase for an example test-CBN. See Figures in Main text for a legend. Here, the left panel shows ${\cal  G}_{\rightarrow X_j=-5}$ and the nodes taking the mean values prescribed by $p_{\rightarrow X_j=-5}(X_{1:N\setminus j }|X_j=-5)$. We see that the Active-Int. Agent's choice is consistent with maximizing on these node values, where it makes a random choice between two nodes with the same value. The right panel panel shows ${\cal  G}_{\rightarrow X_j=-5}$ and the nodes taking the exact values prescribed by the means of $p_{\rightarrow X_j=-5}(X_{1:N\setminus j }|X_j=-5)$, combined with the specific randomness inferred from the previous time step. As a result of accounting for the randomness, the two previously degenerate maximum values are now distinct. We see that the Active-CF. agent's choice is consistent with maximizing on these node values.\relax }}{198}{figure.caption.70}}
\newlabel{fig:expt3}{{6.7}{198}{Experiment 3. Agents do counterfactual reasoning. a) Performance of the agents tested in this experiment. Note that performance can be above 1.0 since the counterfactual agent can theoretically perform better than the optimal interventional baseline, which doesn't have access to noise information. See main text for details. b) Performance split by if the maximum node value in the quiz phase is degenerate (Deg.) or distinct (Dist.). c) Quiz phase for an example test-CBN. See Figures in Main text for a legend. Here, the left panel shows ${\cal G}_{\rightarrow X_j=-5}$ and the nodes taking the mean values prescribed by $p_{\rightarrow X_j=-5}(X_{1:N\setminus j }|X_j=-5)$. We see that the Active-Int. Agent's choice is consistent with maximizing on these node values, where it makes a random choice between two nodes with the same value. The right panel panel shows ${\cal G}_{\rightarrow X_j=-5}$ and the nodes taking the exact values prescribed by the means of $p_{\rightarrow X_j=-5}(X_{1:N\setminus j }|X_j=-5)$, combined with the specific randomness inferred from the previous time step. As a result of accounting for the randomness, the two previously degenerate maximum values are now distinct. We see that the Active-CF. agent's choice is consistent with maximizing on these node values.\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{198}{subfigure.5.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{198}{subfigure.5.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{198}{subfigure.6.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{198}{subfigure.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Active and Random Counterfactual Agents\relax }}{199}{figure.caption.71}}
\newlabel{fig:expt3_active}{{6.8}{199}{Active and Random Counterfactual Agents\relax }{figure.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.11.2}Experiment 4: Non-linear Causal Graphs}{199}{subsection.6.11.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Results for non-linear graphs. (a) Comparing average episode reward for agents trained with different data. (b) Comparing information phase intervention policies. \relax }}{199}{figure.caption.72}}
\newlabel{fig:nonlinear_results}{{6.9}{199}{Results for non-linear graphs. (a) Comparing average episode reward for agents trained with different data. (b) Comparing information phase intervention policies. \relax }{figure.caption.72}{}}
\FN@pp@footnote@aux{33}{200}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.11.3}Experiment 5: Larger Causal Graphs}{200}{subsection.6.11.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Results for $N=6$ graphs. (a) Comparing average episode reward for agents trained with different data. (b) Comparing information phase intervention policies. \relax }}{200}{figure.caption.73}}
\newlabel{fig:N6_results}{{6.10}{200}{Results for $N=6$ graphs. (a) Comparing average episode reward for agents trained with different data. (b) Comparing information phase intervention policies. \relax }{figure.caption.73}{}}
\FN@pp@footnotehinttrue 
\@setckpt{chapters/chapter5}{
\setcounter{page}{201}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{11}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{r@tfl@t}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{2}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{pp@next@reset}{34}
\setcounter{@fnserial}{33}
\setcounter{DefaultLines}{2}
\setcounter{DefaultDepth}{0}
\setcounter{L@lines}{0}
\setcounter{L@depth}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{33}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{48}
\setcounter{eu@}{0}
\setcounter{eu@i}{0}
\setcounter{mkern}{4}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{section@level}{0}
}
