%!TEX root = ../dissertation.tex
%\begin{savequote}[75mm]
%Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
%\qauthor{Quoteauthor Lastname}
%\end{savequote}




\chapter{Correlated sampling replicates biases based on framing}
\label{chap:MCMC}

because people tend to only generate a small number of hypotheses \citep{klein1999,ross96,Gettys1979,weber1993,dougherty1997}.
%
%\newthought{There's something to be said} for having a good opening line. 
%
%% For an example of a full page figure, see Fig.~\ref{fig:myFullPageFigure}.
%\begin{figure}
%\includegraphics[width=\textwidth]{figures/fig1}
%\caption[Short figure name.]{This is a figure that floats inline and here is its caption.
%\label{fig:myInlineFigure}}
%\end{figure}


%% Requires fltpage2 package
%%
% \begin{FPfigure}
% \includegraphics[width=\textwidth]{figures/figures/fullpage}
% \caption[Short figure name.]{This is a full page figure using the FPfigure command. It takes up the whole page and the caption appears on the preceding page. Its useful for large figures. Harvard's rules about full page figures are tricky, but you don't have to worry about it because we took care of it for you. For example, the full figure is supposed to have a title in the same style as the caption but without the actual caption. The caption is supposed to appear alone on the preceding page with no other text. You do't have to worry about any of that. We have modified the fltpage package to make it work. This is a lengthy caption and it clearly would not fit on the same page as the figure. Note that you should only use the FPfigure command in instances where the figure really is too large. If the figure is small enough to fit by the caption than it does not produce the desired effect. Good luck with your thesis. I have to keep writing this to make the caption really long. LaTex is a lot of fun. You will enjoy working with it. Good luck on your post doctoral life! I am looking forward to mine. \label{fig:myFullPageFigure}}
% \end{FPfigure}
% \afterpage{\clearpage}


%\section{Introduction}

In his preface to \emph{Astronomia Nova} (1609), Johannes Kepler described how he struggled to find an accurate mathematical description of planetary motion. Like most of his contemporaries, he started with the hypothesis that planets move in perfect circles. This necessitated extraordinary labor to reconcile the equations of motion with his other assumptions, ``because I had bound them to millstones (as it were) of circularity, under the spell of common opinion.'' It was not the case that Kepler simply favored circles over ellipses (which he ultimately accepted), since he considered several other alternatives prior to ellipses. Kepler's problem was that he failed to generate the right hypothesis.

%\footnote{In fact, Kepler had tried fitting an oval to his observations only to reject it, and then labored for another seven years before finally trying an ellipse and realizing that it was mathematically equivalent to an oval. As he recounted, ``The truth of nature, which I had rejected and chased away, returned by stealth through the back door, disguising itself to be accepted... Ah, what a foolish bird I have been!''} 

Kepler is not alone: the history of science is replete with examples of ``unconceived alternatives'' \citep{stanford10}, and many psychological biases can be traced to failures of hypothesis generation, as we discuss below. In this paper, we focus on hypothesis generation in the extensively studied domain of probabilistic inference. The generated hypothesis are a subset of a tremendously large space of possibilities. Our goal is to understand how humans generate that subset.

\begin{table}[htbp]
\centering
\caption{Biases in human hypothesis generation and evaluation.}
\label{tab:biases}
\begin{tabular}{p{.25\textwidth}>{\raggedright}p{.45\textwidth}>{\raggedright}p{.20\textwidth}}
\toprule
\textbf{Name}&\textbf{Description}& \textbf{Reference}\tabularnewline
\midrule \tabularnewline
Subadditivity& Perceived probability of a hypothesis is higher when the hypothesis is described as a disjunction of typical component hypotheses (unpacked to typical examples). & \citet{fox1998belief} \tabularnewline\tabularnewline
Superadditivity& Perceived probability of a hypothesis is lower when the hypothesis is described as a disjunction of atypical component hypotheses (unpacked to atypical examples). & \citet{super}, \citet{hadjichristidis1999opening} \tabularnewline\tabularnewline
Weak evidence effect& The probability of an outcome is judged to be lower when positive evidence for a weak cause is presented & \citet{weak}\tabularnewline\tabularnewline
Dud alternative effect & The judged probability of a focal outcome is higher when implausible alternatives are presented & \citet{dud}\tabularnewline\tabularnewline
Self-generation effect & The probability judgment over hypotheses that participants have generated themselves is lower as compared to the same hypotheses generated by others& \citet{koriat1980,conf} \tabularnewline\tabularnewline
Crowd within & The mean squared error of an estimate with respect to the true value reduces with the number of guesses. This reduction is more pronounced when the guesses are averaged across participants rather than within participants. & \citet{vul08}
\tabularnewline\tabularnewline
Anchoring and Adjustment & Generated hypotheses are biased by the hypothesis that is prompted at the start. & \citet{tversky} \tabularnewline
\bottomrule
\end{tabular}
\end{table}

Most previously proposed models of hypothesis generation rely on cued recall from memory based on similarity to previously observed scenarios \citep[c.f.][]{Thomas2008}. The probability of a generated hypothesis depends on the strength of its memory, and the number of such hypotheses generated is constrained by the available working memory resources. However, in most naturally encountered combinatorial hypothesis spaces, the number of possible hypotheses is vast and only ever sparsely observed. \cite{Goodman2008} showed that, when inferring Boolean concepts, people can generate previously unseen hypotheses by using compositional rules, instead of likening the situation to previously observed situations. So it seems that humans do not generate hypotheses only from the manageably small subset of previously observed hypotheses in memory and instead are able to generate hypotheses from the formidably large combinatorial space of all the conceivable possibilities. Given how large this space is, resource constraints at the time of inference suggest that only a subset are actually generated.

In this paper, we develop a normative theoretical framework for hypothesis generation in the domain of probabilistic inference, arguing that the brain copes with the intractability of inference by stochastically sampling hypotheses from the combinatorial space of possibilities. Although this sampling process is asymptotically exact, time pressure and cognitive resource constraints limit the number of samples that can be generated, giving rise to systematic biases. We explore what sampler designs can reproduce the phenomena listed in Table~\ref{tab:biases}, and then test our theory's novel predictions in two experiments.

\section{A rational process model of hypothesis generation}

Much of the recent work on probabilistic inference in human cognition has been deliberately agnostic about its underlying mechanisms, in order to make claims specifically about the subjective probability models people use in different domains \citep{chater2006}. Because the posterior distribution $P(h|d)$ is completely determined by the joint distribution $P(h,d) = P(d|h)P(h)$, an idealized reasoner's inferences can be perfectly predicted given this joint distribution. By comparing different assumptions about the joint distribution (e.g., the choice of prior or likelihood) under these idealized conditions, researchers have attempted to adjudicate between different models. Importantly, any algorithm that computes the exact posterior will yield identical predictions, which is what licenses agnosticism about mechanism. This method of abstraction is the essence of the ``computational level of analysis'' \citep{marr1976understanding}, and is closely related to the competence/performance distinction in linguistics and ``as-if'' explanations of choice behavior in economics.

The phenomena listed in Table~\ref{tab:biases} do not yield easily to a purely computational-level analysis, since different choices for the probabilistic model do not account for the systematic errors in approximating them. For this reason, we turn to ``rational process'' models \citep[see][for a review]{Griffiths2012BridgingCognition}, which make explicit claims about the mechanistic implementation of inference. Rational process models are designed to be approximations of the idealized reasoner, but make distinctive predictions under resource constraints. In particular, we explore how sample-based approximations lead to particular cognitive biases in a large space of hypotheses, when the number of samples is limited.
With an infinite number of samples, different sampling algorithms are indistinguishable as they all converge to the ideal response, but these algorithms display different behaviors at small sample sizes. We narrow the space of candidate sampling algorithms by studying these behaviors and comparing their predictions to observed cognitive biases.


\subsection{A specific Markov chain Monte Carlo algorithm}

The space of MCMC algorithms is vast \citep{robert13}, but for the purposes of modeling psychological phenomena many of the algorithms generate indistinguishable predictions. Our goal in this section is to specify one such algorithm, without making a strong claim that people adhere to it in every detail. We focus on qualitative features of the algorithm that align with aspects of human cognition. Nonetheless, we shall see that the algorithm makes accurate quantitative predictions about human probabilistic judgments.

The most well-known and widely-used version of MCMC is the Metropolis-Hastings algorithm. Here, at step $n$ in the Markov chain, new suggestions $h'$ are drawn from a proposal distribution $Q(h'|h_n)$, where $h_n$ is the hypothesis at step $n$. This proposal is accepted or rejected according to:
\begin{align}
P(h_{n+1}=h'|h_n) = \min\left[1, \frac{P(d|h')P(h') Q(h_n|h')}{P(d|h_n) P(h_n) Q(h'|h_n)} \right].
\end{align}

If the proposal is rejected, then the chain stays at the same hypothesis, $h_{n+1}=h_n$. Although the posterior cannot be directly evaluated, we assume it can be computed up to a normalizing constant, since $P(h|d) \propto P(d|h) P(h)$. The acceptance function forces moves to higher probability hypotheses, while also stochastically exploring lower probability hypotheses. This process repeats until $N$ samples have been generated. Once the burn-in period has elapsed, the amount of time the chain spends at a particular hypothesis is proportional to its posterior probability. The unique members of the set of accepted samples constitute the generated hypotheses, and the number of times they appear provides their judged probability.

We recap here two psychologically appealing properties of the algorithm mentioned in the previous section. First, we see that it relies solely on being able to gauge relative probabilities and not on having good estimates for any absolute probabilities. Second, the acceptance function engenders an interaction between generation and evaluation by ensuring that if one is at a high probability hypothesis, proposals are more likely to be rejected and therefore not generated. This follows the intuition  that if one finds a good (high probability) hypothesis, one is less likely to generate more hypotheses. Conversely, if one is at a bad (low probability hypothesis), more proposals will be accepted.

%The hypotheses generated and the probabilities returned at a specific chain length $N$ are obtained by averaging over the results of multiple ($M$ in number) chains, each being run for $N$ steps. 
%In the case of unpacked hypotheses, each of the example hypotheses (total $K$ unpacked examples) initializes a new chain. Each of the $K$ chains is simulated $M$ times, initialized at the same example hypothesis and run for $\lfloor N/K \rfloor $ steps (with the remainder added randomly to one of the $K$ chains so that over the $M$ simulations, each chain runs for $N/K$ steps in expectation). We do not account for order effects - i.e. the order in which the examples are presented does not affect the model's predictions. To balance this assumption, we randomize the order of the presented examples in our experiments to nullify any order effect that may empirically be present. To minimize confounding effects, the packed control is modeled as similarly as possible - with only the initialization being different in each . Again, we have $K$ chains, each of which is run $M$ times, but each chain is initialized by randomly sampling the initialization distribution (which in our case is the prior over the focal space) instead of with one of the prompted examples, and similarly run for $\lfloor N/K \rfloor $ steps (with the remainder added randomly to one of the $K$ chains so that over the $M$ simulations, each chain runs for $N/K$ steps in expectation) each.

The next step is to specify the proposal distribution. For simplicity, we assume that the proposal is symmetric, $Q(h'|h) = Q(h|h')$. This reduces the acceptance function to:
\begin{equation}
\label{eq:acceptance}
P(h_{n+1}=h'|h_n) = \min\left[1, \frac{P(d|h')P(h')}{P(d|h_n) P(h_n)} \right].
\end{equation}

We also assume that the proposal distribution is ``local'': the proposal distribution preferentially proposes hypotheses that are in some way ``close'' to the current one. This ensures that the next generated hypothesis is close to the current one with high probability. The alternate possibility is to instead have a ``global'' proposal distribution - for example one that proposes the next hypothesis uniformly at random from the space of all possible hypotheses, instead of favoring those closer to the current one.

MCMC algorithms always exhibit some autocorrelation irrespective of the proposal distribution, because the same state can occur consecutively several times if proposals are repeatedly rejected. However, we are also interested in the next \textit{new} hypothesis that is generated, not exact repetitions of the same hypothesis. A more nuanced notion of autocorrelation  takes into account the fact that sampled hypotheses can be ``similar'' (though not identical) when the proposal distribution is centered on a local neighborhood of the current hypothesis, as opposed to if the proposal is a ``global'' one. This kind of locality in determining the next state given the current one, has been studied previously in the context of traversing and searching semantic networks \citep{abbott2015random} and combinatorial spaces \citep{smith2013multiply}. This locality has been shown to be optimal as a foraging strategy \citep{hills2012optimal} as well as consistent with human behavioral data. Since the generation of hypotheses is largely analogous to a search through the combinatorial space of conceivable possibilities, locality in the proposal distribution that moderates this search can be expected.

The question then is how we should define locality. This is relatively easy to answer in domains where the inference is over a one-dimensional continuous latent variable like in \cite{Lieder2013}; for example, one can use a normal distribution centered at the current hypothesis. For the discrete combinatorial hypothesis spaces studied in this paper, we assume that there is some natural clustering of the hypotheses based on the observations they tend to generate (their centroids). We use the Euclidean distance between centroids as a measure of distance between clusters. In our simulations, we assume for simplicity that all hypotheses within a cluster are equidistant and that all clusters are equidistant from each other. The proposal distribution chooses hypotheses in the same cluster with a higher probability than those outside the cluster, but it treats all hypotheses within a cluster equiprobably. While this structure induces locality in the proposal distribution, we are not making a strong claim about the nature or role of clustering in hypothesis generation. We speculate about more sophisticated proposal distributions in the section on Future Work. 

%Through the course of our simulations, we subjectively compare the results from our model to a Markov chain that uses such a ``global'' proposal in order to highlight the importance of having correlation between consecutively generated hypotheses in explaining the effects noted in Table \ref{tab:biases}.

Finally, we need to specify how the chain is initialized. For cases where certain hypotheses are presented explicitly or primed in the query, we assume that the chain starts at one of those hypotheses. However, in cases where no hypotheses is explicitly prompted, we assume that the initial hypothesis is drawn from the prior over the hypotheses of interest. This assumption is consistent with evidence that hypotheses with high base rates are more likely to be generated \citep{weber1993}. There may also be initialization schemes that mix explicit prompts and sampling from the prior---for example a prompt that encourages sampling from a specific subset of the hypothesis space. We speculate about more sophisticated initialization schemes in the section on Future Work.
\section{Model simulations}

In this section we apply our model to a range of empirical phenomena, using a disease-symptom Bayesian network as our running example. For each simulation, we run the Markov chain many times and average the results, in order to emulate multiple participants in an experiment.

\subsection{Diagnostic hypotheses in a disease-symptom network}

Our model is generally applicable to domains where the inference is carried out over a large space of possibilities that is sparsely observed and thus requires one to generate previously unobserved possibilities. A data set containing medical symptoms is a prototypical example of this problem: a patient could have any combination of more than one disease and many such combinations will not have been encountered before by an individual clinician. This combinatorial structure makes medical diagnosis computationally difficult---exact inference in a Bayesian network is known to be NP-hard \citep{cooper1990complexity}. To address this problem, approximate probabilistic inference algorithms (including Monte Carlo methods) are now widely-established \citep[e.g.,][]{Shwe1991, Jaakkola1999, Heckerman1990}. It is thus reasonable to conjecture that diagnostic reasoning by humans could be captured by similar approximate inference algorithms. Suggestively, a number of the judgment biases listed in Table \ref{tab:biases} have been documented in clinical settings \citep{redelmeier1995probability,elstein1978medical,weber1993}; our goal is to investigate whether the MCMC model can reproduce these biases.

In the disease-symptom network, the observations are the presence or absence of symptoms and the latent variables are the presence or absence of diseases ($S$ possible symptoms and $D$ possible diseases). The diagnostic problem is to compute the posterior distribution over $2^D$ binary vectors, where each vector encodes the presence ($h_d=1$) or absence ($h_d=0$) of diseases $d = 1,\ldots,D$. The diseases are connected to the symptoms via a noisy-or likelihood, following \cite{shwe1991probabilistic}:
\begin{align}
P(k_s = 1|h) = 1 - (1-\epsilon) \prod_{d=1}^D (1-w_{ds})^{h_d},
\end{align}
where $k_s = 1$ when symptom $s = 1, \ldots, S$ is present (0 otherwise), $\epsilon \in [0,1]$ is a base probability of observing a symptom, and $w_{ds} \in [0,1]$ is a parameter expressing the probability of observing symptom $s$ when only disease $d$ is present. Intuitively, the noisy-or likelihood captures the idea that each disease has an independent chance to produce a symptom.

As our goal is to use this set-up purely for illustrative purposes, we use a simplified fictitious disease-symptom data set designed to resemble real-world contingencies (Table~\ref{tab:qmr}). We designated two distinct clusters of four diseases each (gastrointestinal diseases and respiratory diseases); these two clusters have largely disjoint sets of symptoms, and the symptoms within a cluster are largely overlapping. We allow any combination of diseases to be present, making even this small number of diseases a fairly large space of 256 possible hypotheses.


\begin{table}[htbp]
%\setstretch{\ddoublespacing}
\caption{Parameters used for noisy-or model.}
  \begin{center}
\begin{tabular}{>{\raggedright}m{0.16\textwidth} m{0.07\textwidth} m{0.04\textwidth} m{0.05\textwidth} m{0.04\textwidth} m{0.08\textwidth} m{0.08\textwidth} m{0.08\textwidth} m{0.1\textwidth} m{0.04\textwidth}} 
  \toprule

     \multirow{2}{*}{Symptoms} &
      \multicolumn{9}{c}{Diseases} \\
 & lung cancer & TB & resp. flu & cold & gastro-enteritis & stomach cancer & stomach flu & food poisoning & base \\
\midrule
Prior & 0.001 & 0.05 & 0.1 & 0.2 & 0.1 & 0.05 & 0.15 & 0.2 & 1.0\\
%\hline \hline
cough & 0.3 & 0.7 & 0.05 & 0.5 & 0.0 & 0.0 & 0.0 & 0.0 & 0.01 \\
%\hline
fever & 0.0 & 0.1 & 0.5 & 0.3 & 0.0 & 0.0 & 0.1 & 0.2 & 0.01 \\
%\hline
chest pain & 0.5 & 0.5 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.01 \\
%\hline
short breath & 0.5 & 0.2 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.01 \\
%\hline
nausea & 0.0 & 0.0 & 0.2 & 0.1 & 0.5 & 0.1 & 0.5 & 0.7 & 0.01 \\ 
%\hline
fatigue & 0.0 & 0.0 & 0.2 & 0.3 & 0.1 & 0.05 & 0.2 & 0.4 & 0.01 \\
%\hline
bloating & 0.0 & 0.0 & 0.0 & 0.0 & 0.3 & 0.05 & 0.1 & 0.5 & 0.01 \\
%\hline
abdom. pain & 0.0 & 0.0 & 0.01 & 0.0 & 0.1 & 0.5 & 0.0 & 0.0 & 0.01 \\
%\hline
\bottomrule
\end{tabular}
\end{center}
\label{tab:qmr}

\end{table}

\subsection{Subadditivity}

As described above, a resource-rational algorithm will arrest computation after a small number of samples, once accuracy is balanced against the cost of sampling \citep{Vul2014}. This gives rise to \emph{subadditivity} (see Table \ref{tab:biases}): the probability of a disjunction (in ``packed'' form) is judged to be less than the probability of the same disjunction presented explicitly as the union of its sub-hypotheses (in ``unpacked'' form) \citep{tversky94,Dougherty2003}, despite the fact that mathematically these are equal. For example, the probability of a gastrointestinal disease is judged to be less than the sum of the probabilities of each possible gastrointestinal disease.

Let us define a few terms here that we use in our simulations of these unpacking effects. The space of hypotheses that the disjunction refers to is called the \textit{focal space} of the query. For example, when queried about the probability of a gastrointestinal diseases, the focal space is the set of all hypotheses that include at least one gastrointestinal disease. When unpacking this disjunction, we do not unpack to every single member of the focal space. Instead, we unpack to a few examples and to a \textit{catch-all hypothesis} that refers to all other members of the focal space that were not explicitly unpacked. For example: "Food poisoning, stomach cancer or any other gastrointestinal disease" where a few example components of the focal space are explicitly unpacked (food poisoning and stomach cancer) and presented along with a catch-all hypothesis (any other gastrointestinal disease).

Our model offers the following explanation of subadditivity: when a packed hypothesis is unpacked to typical examples and a catch-all hypothesis, the typical examples (that are part of the focal space) are explicitly prompted, causing the Markov chain to start there and thus include them in the cache of generated hypotheses. If the examples had not been explicitly prompted and instead a packed hypothesis had been presented, the chain initializes randomly and perhaps these high probability typical examples are never generated and so not included in the cache. Initializing the chain at a typical (high probability) state gives the chain a head-start in generating high probability hypotheses in the focal space and thus results in a larger probability judgment for that focal space.

To illustrate this effect in our medical diagnosis model, consider the following queries: 
\begin{itemize}
\item Packed query: Given the symptoms \emph{fever, nausea} and \emph{fatigue}, what is the probability that these symptoms were caused by the presence of a gastrointestinal disease? 
\item Unpacked query (typical examples): Given the symptoms \emph{fever, nausea} and \emph{fatigue}, what is the probability that these symptoms were caused by the presence of food poisoning, stomach flu, or any other gastrointestinal diseases?
\end{itemize}
The difference between the probability estimates between these two conditions is shown in Figure \ref{fig:subadd}.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/sub.pdf}
\caption{\textbf{Subadditivity}. MCMC estimates were made for the following queries: Given the symptoms \emph{fever, nausea} and \emph{fatigue}, (a) Packed: what is the probability that these symptoms were caused by the presence of a gastrointestinal disease? (b) Unpacked to typical examples: what is the probability that these were caused by the presence of food poisoning, stomach flu, or any other gastrointestinal diseases? The estimate for the unpacked condition is higher than that of the packed condition. The difference between these estimates is represented by the red line. This effect diminishes as the number of samples increases.}
\label{fig:subadd}
\end{figure}

Experiments in \cite{Dougherty2003} show that the effect size of subadditivity decreases as the participants are given more time to answer the question. In our model, as more samples are taken, it becomes more and more likely that the packed chain also finds the high probability examples prompted in the unpacked scenario on its own. So the head-start given to the unpacked chain gets gradually washed out and the effect size of subadditivity decreases. If we assume that as more time passes, people take more samples (up until a resource-rational limit on the number of samples), and that the time-points measured are before the resource-rational sample limit is met, our model replicates these time-dependence effects as seen in Figure \ref{fig:subadd}.

\subsection{Superadditivity and related effects}

Taking a limited number of samples with an MCMC sampler can also give rise to an effect opposite to the one described in the previous section, known as \emph{superadditivity} (see Table \ref{tab:biases}): the probability of a disjunction (in ``packed'' form) is judged to be \textit{greater} than the probability of the same disjunction presented explicitly as the union of its sub-hypotheses (in ``unpacked'' form) \citep{super,hadjichristidis1999opening}, despite the fact that mathematically they should be equal. This effect occurs when  unpacking to atypical (low probability) examples and subadditvity prevails when unpacking to typical (high probability) examples.

The key feature that produces this effect is the acceptance function of the MCMC sampler and the feedback it causes between the generation and evaluation processes. If a chain is at a low probability hypothesis (such as when a low probability hypothesis is explicitly prompted in the form of an atypical unpacking), the chain is likely to accept more of the proposals made by the proposal distribution. Therefore this chain could generate many alternate hypotheses outside the focal space. In contrast, a chain at a higher probability hypothesis (for example, if it was randomly initialized in the focal space instead of being initialized at a particularly atypical example) will reject more of these proposals and remain at the initial hypothesis. So most of these proposals will not be generated. The probability estimate for the focal space $\mathcal{A}$ is given by 

\begin{align}
\sum_{h \in \mathcal{A}} \hat{P}(h | d) = 
\sum_{h \in \mathcal{A}}\frac{1}{N}\sum_{n=1}^N \mathbb{I}[h_n=h] = 
\frac{\sum_{h \in \mathcal{A}} \sum_{n=1}^N \mathbb{I}[h_n=h]}
{\sum_{h \in \mathcal{A}} \sum_{n=1}^N \mathbb{I}[h_n=h] + \sum_{h' \notin \mathcal{A}} \sum_{n=1}^N \mathbb{I}[h_n=h']}
\end{align}

Being in $\mathcal{A}$ or not divides the total hypothesis space of $\mathcal{H}$ into two mutually exclusive parts. Therefore, the generation of more hypotheses outside the focal space (on average) when initialized at a consistently low probability (atypical) hypothesis in the focal space lowers the resulting probability estimate of the focal hypothesis space. This results in superadditive judgments.
 
To elucidate this effect in our medical diagnosis model, we use the following ``unpacked to atypical examples'' query: Given the symptoms \emph{fever, nausea} and \emph{fatigue}, what is the probability that these symptoms were caused by the presence of gastroenteritis, stomach cancer, or any other gastrointestinal disease? The difference between the probability estimates from the two conditions is shown in Figure \ref{fig:superadd}.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/super.pdf}
\caption{\textbf{Superadditivity}. MCMC-estimates were made for the following queries: Given the symptoms \emph{fever, nausea} and \emph{fatigue}, (a) Packed: What is the probability that these symptoms were caused by the presence of gastrointestinal disease? (b) Unpacked to atypical examples: What is the probability is that these symptoms were caused by the presence of gastroenteritis, stomach cancer, or any other gastrointestinal disease? The estimate for the unpacked condition is lower than that of the packed condition. The difference between these estimates is represnted by the red line. This effect diminishes as the number of samples increases.}
\label{fig:superadd}
\end{figure}

Previous accounts of subadditivity \citep[e.g.,][]{Thomas2008, MINERVAsub} cannot explain superadditivity; any unpacked example only increases the probability judgment of the unpacked query with respect to the packed query. This weakness of MINERVA-DM has been observed by \cite{Costello2014} in the context of its failure to model binary complementarity---an effect which their noise-based analysis can capture. However, their analysis still fails to completely capture superadditvity, as it constrains unpacked judgments to be greater than (and, only for binary complements, equal to) the packed judgment, never less than the packed judgment. \cite{super} explain superadditivity by suggesting that atypical examples divert attention from more typical examples and thus lower the probability estimate. But an explanation at the level of a rational process model is, to our knowledge, lacking in the literature. 

Some other cognitive effects can also be modeled by the same mechanism that gives rise to superadditivity. One example is the \emph{weak evidence effect}: the perceived probability of an outcome is lowered by the presence of evidence supporting a weak cause \citep{weak}. While the added evidence increases the net posterior probability of the queried focal space, it also results in initialization at a very low probability hypothesis (an atypical or weak hypothesis). This initialization lowers the probability estimate as in the superadditivity effect and overwhelms the effects of the added evidence. This causes the ultimate perceived probability to be lower than if the positive evidence had not been presented and the chain was initialized randomly (on average at a higher probability hypothesis than the presented weak one) in the focal space.

To elucidate this effect in our medical diagnosis model, we use the following query:
\begin{itemize}
\item Control: Given the symptoms \emph{fever, nausea}  and \emph{fatigue}, what is the probability that these symptoms were caused by the presence of gastrointestinal disease?
\item Evidence for a weak cause: Given the symptoms \emph{fever, nausea}  and \emph{fatigue}, what is the probability that these symptoms were caused by the presence of gastrointestinal disease, assuming the patient's grandmother was diagnosed with stomach cancer?
\end{itemize}
The increase in support of the weak cause (stomach cancer), by making available the presence of familial history, is implemented in our model by increasing the prior probability of stomach cancer in this patient from 0.05 to 0.06 (see Table \ref{tab:qmr}). While this small change isn't expected to elicit a large difference in the probability of of gastrointestinal diseases between the two cases, it certainly does make it more (rather than less) probable compared to the control. However, it also causes the chain to be initialized at the weak hypothesis of stomach cancer by prompting it, resulting in the generation of more alternative hypotheses outside the focal space and a lower probability judgment than in the first case (Figure \ref{fig:weak}).

\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/weak.pdf}
 \caption{\textbf{Weak evidence effect}. MCMC estimates were made for the following queries: Given the symptoms \emph{fever, nausea}  and \emph{fatigue}, (a) Control: What is the probability that these symptoms were caused by the presence of gastrointestinal disease? (b) Evidence for a weak cause: What is the probability that these symptoms were caused by the presence of gastrointestinal disease, assuming the patient's grandmother was diagnosed with stomach cancer? The increase in support of the weak cause (stomach cancer) is modeled by increasing the prior probability of stomach cancer from 0.05 to 0.06. The estimate from the weak evidence chain is lower than that from the control chain. The difference between these estimates is represented by the red line. The effect diminishes as the number of samples increases.}
 \label{fig:weak}
\end{figure}

Another such bias is the \emph{Dud alternative effect}: presenting low probability (or ``dud'') alternate hypotheses increases the perceived probability of the focal space of hypotheses \citep{dud}. This can be viewed as the superadditivity effect in the complement (alternate) hypothesis space. The queries being contrasted here are initialized in the space complementary to the focal space---i.e., the space of alternatives. Initialization at a low probability alternative when it is explicitly prompted in the question results in a superadditive judgment (i.e., a lower probability judgment) of the complement space. This lower probability estimate for the complement space entails a higher probability estimate for the focal space $\mathcal{A}$.

To elucidate this effect in our medical diagnosis model, we use the following queries:
\begin{itemize}
\item Control: Given the symptoms \emph{fever, nausea} and \emph{fatigue}, what is the probability that the patient has a respiratory disease (as opposed to the symptoms being caused by the presence of a gastrointestinal disease)?
\item Dud alternative: Given the symptoms \emph{fever, nausea} and \emph{fatigue}, what is the probability that the patient has a respiratory disease (as opposed to the symptoms being caused by the presence of gastroenteritis, stomach cancer, or any other gastrointestinal disease)?
\end{itemize}
We see in Figure \ref{fig:dud} that the model predicts that the scenario with dud alternatives produces higher probability judgments than the control. Findings in \cite{dud} also suggest that the magnitude of this effect decreases with the amount of processing time given to participants. The model also replicates this phenomenon, if we assume that more time means more samples, and that the time points queried are before the resource-rational limit on the number samples is reached.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/dud.pdf}
\caption{\textbf{Dud alternative effect}. MCMC estimates were made for the following queries: Given the symptoms \emph{fever, nausea}  and \emph{fatigue}, (a) Control: What is the probability that the patient has a respiratory disease (as opposed to the symptoms being caused by the presence of a gastrointestinal disease)?,  (b) With dud alternatives: What is the probability that the patient has a respiratory disease (as opposed to the symptoms being caused by the presence of gastroenteritis, stomach cancer, or any other gastrointestinal disease)? The estimate from the control chain is higher than from the chain for which dud alternatives are presented. The difference between these estimates is represented by the red line and the effect diminishes as the number of samples increases}
\label{fig:dud}
\end{figure}

\subsection{Self-generation of hypotheses}

In this section, we focus on the \emph{self-generation effect}: the probability judgment of a set of hypothesis that are generated and reported by a subject themselves is lower than when the same set of reported hypotheses is presented to a new subject \citep{conf,koriat1980}. Our model provides the following explanation: Self-reported hypotheses generated by a chain are the modes it discovers after having explored the space and having generated several alternate hypotheses. However, in a situation where these high probability hypotheses are directly presented, the chain starts at the mode and is likely to get stuck---i.e., not accept any of the proposals and thus not generate them at all. This, in the small sample limit, results in the generation of fewer alternate hypotheses. As in the previous section, fewer alternate hypotheses leads to a higher probability judgment.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/confidence.pdf}
\caption{\textbf{Self-generation effect}. MCMC estimates for the following query: Given the symptoms \emph{fever} and \emph{fatigue}, (a) Self-generated: What are the two most likely respiratory diseases to have caused these symptoms? Estimate the probability that these symptoms are caused by either of these two diseases. (b) Other-generated: What is the probability that these symptoms were caused by the presence of a cold or respiratory flu (two most likely respiratory diseases to have caused these symptoms returned by the first chain)? The estimate from the other-generated chain is higher than from the self-generated chain. The difference between these estimates is represented by the red line and the effect decreases as the number of samples increases}
\label{fig:conf}
\end{figure}

We simulate an experiment analogous to the experiments in \cite{conf} by querying the model as follows: Given  the symptoms \emph{fever} and \emph{fatigue}, what are the two most likely respiratory disease to have caused these symptoms? To simulate the answer to this query, a randomly initialized ``self-generated'' chain is run and the 2 hypotheses over which this chain returns the highest probabilities are returned. In this case, these are \emph{a cold} and \emph{respiratory flu}. The net probability estimate of the generated hypotheses \emph{cold or respiratory flu} is tracked over time for the chain that generated them. A separate ``other-generated'' chain is queried as follows: Given the symptoms \emph{fever} and \emph{fatigue}, What is the probability that these symptoms were caused by the presence of a \textit{cold} or \textit{respiratory flu}? Thus, this chain is initialized at these high probability hypotheses of cold and respiratory flu. The difference between the probability estimates from these two chains is shown in Figure~\ref{fig:conf}.

%*************read koriat

While this effect has previously been understood in terms of the generation of alternatives \citep{conf}, a rational process model specifying a mechanism for this differential generation of alternatives is novel. Our explanation of this effect is largely contingent upon a link between generation and evaluation. In both self-generated and other-generated scenarios, the same hypothesis was generated, but evaluated differently depending on how many alternatives were generated. An MCMC chain can ``get stuck'' at a high probability hypothesis when initialized there by rejecting most of the new proposals, resulting in fewer generated alternatives. 

%  Further, the effect as replicated by the importance sampler will be very small, as it only causes the other-generated hypotheses to be sampled one extra time, predicting a difference in judgments of one divided by the number of samples taken. We expect the number of samples to be of the order of hundreds and so this effect will be negligibly small.

\subsection{Anchoring and adjustment}

In a classic experiment, \citet{tversky} had participants observe a roulette wheel that was predetermined to stop on either 10 or 65. Participants subsequently guessed the percentage of African countries in the United Nations. Participants who saw the wheel stopping on 10 guessed lower values than participants whose wheel stopped at 65. This and other findings led \citet{tversky} to hypothesize the ``anchoring and adjustment'' heuristic, according to which people anchor on a salient reference (even if it is irrelevant) and incrementally adjust away from the anchor towards the correct answer.

\citet{Lieder2013} showed that the anchoring and adjustment heuristic is a basic consequence of MCMC algorithms, due to the inherent autocorrelation of samples. Consistent with this account, our model posits that anchors, even when irrelevant, can serve to initialize the Markov chain. Locality guarantees that the chain will adjust incrementally away from the initial state, though anchoring will occur more generally as long as the rejection probability is non-zero. An MCMC algorithm with global proposals will capture anchoring to some extent because of its non-zero rejection probability and resulting auto-correlation of samples. However, without locality, estimates would not adjust incrementally away from the initial state. In other words, any MCMC algorithm will over-represent the initial anchoring hypothesis in the small sample limit, but only an MCMC algorithm with local proposals will also over-represent other hypotheses \textit{close} to the initial anchoring hypothesis.

We illustrate this effect in Figure \ref{fig:anch} using MCMC with local proposals on the disease-symptom network. The space of diseases in our example is clustered into respiratory and gastrointestinal diseases. The given symptoms are \textit{fever} and \textit{fatigue}. Chains initialized in different clusters show an initial within-cluster bias (i.e. not just a bias towards the initial anchoring hypothesis, but also to other hypotheses in its cluster), and this bias diminishes with the number of samples.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/anchoring.pdf}
\caption{\textbf{Anchoring and adjustment}. The y axis represents the difference in the probabilities of respiratory flu and stomach flu given the symptoms \textit{fever} and \textit{fatigue} as returned by two different chains that are initialized differently. The chains are initialized in the two different clusters, at hypotheses other than the focal hypotheses of \textit{respiratory} or \textit{stomach flu}. Before reaching convergence, the chain initialized in cluster 1 of respiratory diseases places higher probability on respiratory flu, than the chain initialized in cluster 2 of gastrointestinal diseases. The net difference between the two chains diminishes as the number of samples increases.}
\label{fig:anch}
\end{figure}

\subsection{The crowd within}

Error in estimates of numerical quantities decrease when the estimates are averaged across individuals, a phenomenon known as the \emph{wisdom of crowds} \citep{surowiecki2005wisdom}. This is expected if the error in the estimate of one individual is statistically independent from the error of the others, such that averaging removes the noise. Any unbiased stochastic sampling algorithm replicates this result, because taking more samples gets one closer to the asymptotic regime, where the estimates are exact and the error tends to zero.

This error analysis was extended by \cite{vul08} to the effects of averaging across multiple estimates from a single individual. They found that averaging estimates reduced error---a phenomenon they named the \emph{crowd within}. However, they also found that this error reduction was less compared to the reduction obtained by averaging across individuals. One explanation for this observation is that the error in the estimates given by the same individual are not entirely independent. We propose that the dependence between multiple estimates arises from an autocorrelated stochastic sampling algorithm like MCMC.
%Eric: maybe we could say that the correlation for one individual is less than for many. This way we are not making a strong claim about errors cause by groups.
This effect is illustrated in Figure \ref{fig:asymptote}. We presented the following query to the model: Given symptoms are \emph{fever}, \emph{nausea} and \emph{fatigue}, what is the probability that these symptoms are caused by the presence of a respiratory disease rather than a gastrointestinal disease? We ran several chains ($N_c = 24$) initialized randomly in the space of all possible diseases, with each run generating the same number of samples ($N_s = 200$). Each chain is initialized at the last sample of the previous chain\footnote{We could also induce correlation between consecutive estimates by continuing the chain---i.e., carrying over the estimates from the first guess to the second one, instead of re-initializing. However, if we continue the chain, the second estimate is made with more samples and will always be lower error on average than the first one. \cite{vul08} find this to not be the case empirically.}, for another $N_s$ steps and a new set of $N_c$ estimates are obtained, corresponding to the second guesses of the $N_c$ individuals. This process is continued until we have 7 estimates from each of the $N_c = 24$ participants. The samples are then averaged either within or across individuals (chains). We find results analogous to those in \cite{vul08}---the error of the responses monotonically declines with the number of samples, and the error reduction is greater when averaging across (compared to within) individuals.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{figures/error.pdf}
\caption{\textbf{The crowd within}. Errors in the MCMC estimates for the following query: Given the symptoms \emph{nausea} and \emph{shortness of breath}, what is the probability that these were caused by the presence of a respiratory disease? The estimates are averaged either over samples from the same individual (blue) or over samples from different individuals (red)}
\label{fig:asymptote}
\end{figure}

Our MCMC model can replicate this effect because it generates auto-correlated samples. The last sample from one estimate is where the chain for the next estimate is initialized. As the sampling process is auto-correlated, subsequent samples in the second chain (in the small sample size limit) are correlated to its initial sample. Similarly, earlier samples from the first chain are correlated to its last sample. Because the samples from the two chains are correlated via the common sample, the probability estimates they generated are correlated as well. This auto-correlation exists irrespective of proposal distribution because of the non-zero rejection probability, but is strengthened by locality in the proposals because this increases correlation.

\subsection{Summary of simulation results and comparison with importance sampling}

To highlight the distinctive predictions of MCMC, it is useful to compare it with other sampling algorithms that have been explored in the psychological literature. \emph{Importance sampling} also uses a proposal distribution $Q(h)$, but unlike MCMC it samples multiple hypotheses independently and in parallel. These samples are then weighted to obtain an approximation of the posterior:
\begin{align}
\hat{P}_N(h|d) = \frac{1}{N} \sum_{n=1}^N \mathbb{I}[h_n=d] w_n,
\end{align}
where $w_n$ is an ``importance weight'' for sample $n$ computed according to:
\begin{align}
w_n \propto \frac{P(h_n,d)}{Q(h_n)}.
\end{align}
Intuitively, the importance weight corrects for the fact that the importance sampler draws samples from the wrong distribution. \citet{shi10} have shown how this algorithm can be used to simulate human performance on a wide range of tasks. They also identified a correspondence between importance sampling and exemplar models, which have been widely applied in psychology. In related work, \citet{shi2009neural} demonstrated how importance sampling could be realized in a biologically plausible neural circuit \citep[see also][]{abbott13}.

Some of the effects we have replicated in this work could also be captured by an importance sampling algorithm with limited samples. \citet{Thomas2008} have proposed a model, HyGene, that is similar in spirit to an importance sampler with limited samples, with a memory driven proposal distribution that selects the hypotheses to be generated. HyGene explains subadditivity in terms of a failure to retrieve all the relevant hypotheses from memory due to stochastic noise in the retrieval process and limited working memory capacity.

The self-generation effect can to some extent be reproduced by importance sampling because prompting a hypothesis causes it to be sampled an extra time. So the probability of the focal space will be slightly larger if hypotheses in it are explicitly prompted (other-generated and presented to the participant) than if it they are generated without prompting (self-generated). However, Experiment 2 in \cite{conf} shows that in a situation where all the alternatives are specified, prompting specific hypotheses (as in the other-generated scenarios), does not result in a higher probability judgment than when these hypotheses are not prompted (as in the self-generated scenarios). The MCMC algorithm captures this finding because in a small hypothesis space, the Markov chain will visit all the hypotheses with the right frequency irrespective of initialization. By contrast, the importance sampler predicts a higher probability for other-generated hypotheses, contrary to the empirical finding.

This brings us to a key difference between importance sampling and MCMC: Importance sampling generates all hypotheses in parallel---the generation of new hypotheses has no dependence on hypotheses that have already been generated. Without this dependence, there is no interaction between the generation and evaluation processes. MCMC captures this dependence by sequentially generating hypotheses. Our model's explanation of the self-generation effect, superadditivity, the weak evidence effect and the dud alternative effect rests on this dependence. The Markov chain can get `stuck' (at least temporarily) by rejecting proposals, thus generating fewer alternatives. If, on the other hand, the current hypothesis has low probability, more alternatives are generated and the probability estimate of the focal space is reduced.

The important sampler does not produce these effects, because its mechanism for generating new hypotheses is independent of the probability of the current one. If anything, prompting a hypothesis within the focal space, no matter how atypical, causes it to be sampled, \textit{increasing} the importance sampler's estimate for the probability of the focal space, contradicting superadditivity.

Another key difference between MCMC and importance sampling is that MCMC generates correlated samples, whereas consecutive samples from an importance sampler are totally independent. This prevents the importance sampler from reproducing the effects in Table \ref{tab:biases} that rely on correlated sampling, such as the anchoring effect and the crowd within.

Importance sampling has also been adapted to inference in dynamical systems by generating hypotheses online---a technique known as \emph{particle filtering}. This approach has been fruitfully applied to a number of domains in psychology, such as multiple object tracking \citep{vul2009explaining} and change detection \citep{brown09}. Although hypotheses are generated sequentially by particle filtering, it is important to note that this structure is dictated by the sequential nature of the generative process. For example, in multiple object tracking, the object positions are dynamic latent variables; particle filtering generates new hypotheses about the positions after each new data point is observed. In contrast, the sequences of hypotheses generated by MCMC algorithms are purely mental, in the sense that hypotheses are generated sequentially regardless of whether the generative process is itself sequential. In this paper, we focus on non-sequential generative models in order to stress this point.

%\textbf{**Address particle filters for stationary distributions comment**}

\section{Overview of experiments}

We now turn to novel experimental tests of our theory. As discussed in the Introduction, the primary impetus for considering rational process models based on approximate inference is that inference in many real-world problems is computationally intractable. However, studying complex inference problems experimentally is challenging because it becomes harder to control participants' knowledge about the generative model. In the case of medical diagnosis, we can rely on the extensive training of clinicians, but it is unclear whether conclusions from these studies are generalizable to non-expert populations. Thus, for our experiments we sought a more naturalistic inference problem.

One domain in which humans have rich, quantifiable knowledge is visual scene understanding. Extensive research suggests that the visual system encodes information about natural scene statistics \citep{barlow2001exploitation,simoncelli2001natural}. This suggests that we can use generative models of natural scene statistics as proxies for human scene knowledge. We can then leverage such models to test theories of hypothesis generation in this domain.

Since low-level scene statistics like the distribution of oriented edges are not consciously accessible, we focus on object-level co-occurrence statistics, which have recently been quantified by \citet{greene13}. Specifically, \citet{greene13} measured the co-occurrence frequency of objects in a database of labeled natural images. To obtain a generative model from these co-occurrence statistics, we fit the latent Dirichlet allocation (LDA) model \citep{lda}, which captures the distribution of co-occurrences in terms of latent topics. Each topic specifies a distribution over objects in a scene, and each scene is modeled as a probabilistic mixture of topics. LDA captures the fact that microwaves are likely to co-occur with toasters, and cars are likely to co-occur with mailboxes.

For our purposes, the important point is that we can use this model to compute conditional probabilities over hidden objects in a scene, given a set of observed objects. Formally, let $h \in \mathcal{H}$ denote a hypothesis about $k$ hidden objects in a scene, among all such possible hypotheses $\mathcal{H}$. Given a set of observed objects $d$, the inference problem is to compute the conditional probability $P(h \in H|d)$ that $h$ is in some set $H \subset \mathcal{H}$ (e.g., hypotheses in which at least one of the hidden objects is an electrical appliance, or hypotheses in which the name of at least one of the hidden objects starts with a particular letter). This conditional probability can be approximated using MCMC in the hypothesis space.

In our experiments, we present participants with a set of observed objects, and ask them to estimate the probability that the hidden objects belong to some subset of possible objects. By manipulating the query, we attempt to alter the initialization of participants' mental sampling algorithm, allowing us to quantitatively test some of the predictions of our model. 

Due to the relative complexity of this domain (compared to the simplified fictitious disease-symptom domain we have used so far for illustrative purposes), we refrain from making claims about the structure of proposal locality here and only test the predictions of our model that are immune to the choice of proposal distribution. Specifically, we focus on subadditivity and superadditivity.

%Considering these are natural images and have been labeled by people, we expect that the results from the topic model provide a fair approximation of people's generative model for scenes. This then provides access to the probability distribution over topics given observed objects, as well as the probability distribution over objects given a topic. Therefore, given the presence of some object in a natural scene, we can simulate probability judgments over the presence of other objects in the same scene. More precisely - the topic model can give us the joint probability of the presence of any two objects. We can generate the posterior probability over all other objects (the unknown variables, Y), given the presence of a specific object (the data, say X) by marginalizing this joint distribution over Y. The posterior probability  $P_{X} (Y)$ over all objects Y (except X, the one that is already known to be present) is approximated by Monte Carlo sampling as before. The probability of the presence of any Y, conditioned on the presence of X is then given by Equation~\label{eq:conv}.


% An important point to note is that this value does not affect any of our qualitative results because the function in Equation \ref{eq:conv} is monotonic in $P_X(Y)$ irrespective of the value of $k$. So the prediction that some probability judgments will be greater or less than another one is unaffected by the specific value of $k$.

\section{Experiment 1}

Our first prediction is the occurrence of both superadditivity and subadditivity in the same domain. The key factor is the typicality of the examples prompted by the unpacked query. We predict that if the query prompts typical examples from the focal space, probability judgments of that focal space will be higher than in the packed condition where no hypotheses are prompted (subadditivity). By contrast, if the question prompts atypical examples from the focal space, probability judgments of that focal space will be lower than in the packed condition where no hypotheses are prompted (superadditivity).

Using LDA as the probabilistic model, the data consist of visible objects in a scene, and the hypotheses are hidden objects. The focal space of hypotheses is given by a query such as \emph{all objects starting with `c'}. The focal space was unpacked into several either highly probable (typical) examples or highly improbable (atypical) examples, as well as a catch-all hypothesis. In the packed condition, the focal space is queried without any unpacked examples.

\subsubsection{Participants}
59 participants (26 females, mean age=35.76, SD=11.63) were recruited via Amazon's Mechanical Turk and received \$1 for their participation plus a performance-dependent bonus.

\subsubsection{Materials and procedure}
Participants were asked to imagine playing a game with a friend in which the friend specifies an object in a scene that they cannot see themselves. The task is to estimate the probability of certain sets of other objects in the same scene. For example, the friend could specify ``pillow''. In the unpacked condition, participants were then asked to estimate the conditional probability of a focal space presented as a few examples and a catch-all hypothesis (e.g., ``an armchair, an apple, an alarm clock or any other object starting with an A''). In the packed condition, the query did not contain any examples.




\begin{table}[htbp]
% \setstretch{0.5\baselineskip}
\caption{Queries in Experiment 1. The letter determines the focal space (e.g., all objects beginning with A), conditioned on the cue object. Typical and atypical unpackings are shown for each focal space.}
\label{tab:scenarios}

  \begin{center} 
\begin{tabular}{m{.12\textwidth}  p{.08\textwidth} >{\raggedright}p{.32\textwidth} >{\raggedright}p{.36\textwidth}}
  \toprule
\bf{Cue object} & \bf{Letter} & \bf{Unpacked-typical} & \bf{Unpacked-atypical}\tabularnewline
\midrule
Pillow & A & armchair, alarm clock, apple & arch, airplane, artichokes\tabularnewline
Rug & B & book, bouquet, bed & bird, buffalo, bicycle\tabularnewline
Table & C & chair, computer, curtain & cannon, cow, canoe\tabularnewline
Telephone & D & display case, dresser, desk & drinking fountain, dryer, dome\tabularnewline
Computer & E & envelope,electrical outlet, end table & eggplant, electric mixer, elevator door\tabularnewline
Armchair & F & fireplace, filing cabinet, fan& fire hydrant, fountain, fish tank\tabularnewline
Stove & L & light, lemon, ladle& leavers, ladder, lichen\tabularnewline
Chair & P & painting, plant, printer& porch, pie, platform\tabularnewline
Bed & R & rug, remote control, radio & railroad, recycling bins, rolling pin\tabularnewline
Kettle & S & stove, shelves, soap& suitcase, shoe, scanner\tabularnewline
Sink & T & table, towel, toilet & trumpet, toll gate, trunk\tabularnewline
Lamp & W & window, wardrobe, wine rack & wheelbarrow, water fountain, windmill\tabularnewline
\bottomrule

\end{tabular}
\end{center}
\end{table}



Each participant responded to one query for each of 9 different scenarios shown in Table~\ref{tab:scenarios}, with 3 unpacked-atypical, 3 unpacked-typical, and 3 packed questions. We randomized the order of the scenarios as well as the assignment of scenarios to condition for each participant.

On every trial, participants first saw the cue object, followed by a hypothesis (either packed, unpacked-typical or unpacked-atypical). Participants had 20 seconds to estimate the probability of the hypothesis on a scale from 0 (not at all likely) to 100 (certain). For every timely response per trial they gained an additional reward of \$0.1. A screenshot of the experiment is shown in Figure~\ref{fig:screenshot}.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.9]{figures/screenshot.png}
\caption{\textbf{Experimental setup}. Participants were asked to estimate the conditional probability using a slider bar within a 20-second time limit.}
\label{fig:screenshot}
\end{figure}

\subsubsection{Model fitting}
Our model has two free parameters: the number of hidden objects in the scene ($k$) and the number of samples ($N$). These parameters were fit to the behavioral data from both Experiment 1 and Experiment 2 combined, using a coarse grid search to optimize the mean-squared error between the experimental probability estimates and the probability estimates from the model. The value of $k$ that best fit the data was $k=6$, and the number of samples $N= 230$. This value of $k$ is in the same ballpark as values found for average number of uniquely labeled objects in natural scenes from data collected in \cite{greene13}. This value for $N$ as the number of samples is higher than numbers found in some previous work like \cite{Vul2014} etc, but it is important to note that each unique hypothesis can appear several times in the sample set. So even if the number of samples is larger than in previous studies, the number of unique hypotheses is comparable.

\subsubsection{Results and discussion}
We compared the mean probability judgments for each condition (Figure~\ref{fig:results1}). Consistent with our hypothesis, we found subadditivity in the unpacked-typical condition, with significantly higher probability estimates compared to the control condition $[t(116) = 4.08, p<0.001]$, and superadditivity in the unpacked-atypical condition, with significantly lower probability estimates compared to the control condition $[t(116) = -3.42, p<0.001]$. This pattern of results was captured by our MCMC model.

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5]{figures/estimates.pdf}
\caption{\textbf{Experiment 1 results}. Mean probability estimates for each condition. Error bars represent the 95\% confidence interval of the mean. Red dots show estimates from the MCMC model with 230 samples, assuming 6 hidden objects in the scene.}
\label{fig:results1}
\end{figure}

Our results confirm the prediction that subadditivity and superadditivity will occur within the same paradigm, depending on the typicality of unpacking. A related result was reported by \cite{super}, who found subadditivity only when the definition of the focal space was fuzzy and typical unpacking may have led to the consideration of a larger focal space. We consider this study in more detail in the General Discussion.

\section{Experiment 2}

In Experiment 1, we demonstrated that the typicality of unpacked examples has a powerful effect on biases in probability estimation. In Experiment 2, we provide converging evidence by showing that different biases can be induced for the same unpacked examples by changing the cue object.

Typicality depends on an interaction between the cue and the examples: in the presence of a road, a crosswalk is typical and a coffee-maker is atypical, but the opposite is true in the presence of a sink. Our model predicts that subadditivity will occur when unpacked examples are typical for a given cue object, whereas superadditivity will occur when the same examples are atypical for a different cue object.

%This gives us scenarios where the unpacking causing subadditivity for cue object 1 produces superadditivity for cue object 2 and vice versa. The resulting scenarios are shown in Table~\ref{tab:scenarios2} below. The examples that showed detectable effects in the MCMC model after a chain length of 100 were retained.

%we could only choose query objects that were largely typical in the presence of one cue object but became largely atypical in the presence of a different cue object. So none of the objects we considered could have a very low (or a very high) prior - because then they would be largely atypical (or typical) independent of cue-word. As all the query objects in this experiment were restricted to having an intermediate prior, they are neither \textit{very} typical nor \textit{very} atypical for either cue-word, resulting in weaker effects. Further, as generating these kind of scenarios was generally more difficult, we also had to restrict the unpacking to two objects instead of three as in the experiment before. We therefore expected all of the effects, to be less pronounced than before.

\subsubsection{Participants}
180 participants (84 females, mean age= 34.25, SD=11.16) were recruited via Amazon's Mechanical Turk web service and received \$0.5 for their participation plus a performance-dependent bonus.

\subsubsection{Materials and procedure}
The experimental procedure was identical to Experiment 1, except for the choice of scenarios (Table~\ref{tab:scenarios2}). Each participant responded to one unpacked-typical, one unpacked-atypical and one packed scenario in random order.



\begin{table}[htbp]
\caption{Queries in Experiment 2. The letter determines the focal space (e.g., all objects beginning with A), conditioned on the cue object. Conditioned on cue object 1, unpacking 1 is predicted to cause subadditivity and unpacking 2 is predicted to cause superadditivity. These predictions reverse for cue object 2.}
\label{tab:scenarios2}
  \begin{center} 
\begin{tabular}{p{.18\textwidth} p{.18\textwidth} p{.08\textwidth} >{\raggedright}p{.25\textwidth} >{\raggedright}p{.25\textwidth}}
  \toprule
\bf{Cue object 1}&\bf{Cue object 2}&\bf{Letter}&\bf{Unpacking 1} &\bf{Unpacking 2}\tabularnewline
\midrule
Pillow & Faucet & B & bed skirt, bedspread\ & bucket, bread\tabularnewline
Road & Sink & C & cabin, crosswalk & cup, coffee maker\tabularnewline
Cabinet & Road & T & toothpaste, tray & terrace, tunnel\tabularnewline
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Results and discussion}
As shown in Figure~\ref{fig:results2}, we observed a superadditivity effect: probability estimates were significantly higher in the packed condition compared to the atypical unpacking for both cue object 1 $[t(165)=3.31, p<0.01]$ and cue object 2 $[t(162)=4.31, p<0.01]$. We did not observe a subadditivity effect for either cue object 1 [$t(171)=0.73, p>0.05]$ or cue object 2 $[t(168)=0.08, p>0.05]$. Importantly, we found a significant interaction between the cue-object and the unpacking of the objects $[F(498, 2)=12.69, p<0.001]$. In particular, when conditioning on cue object 2, using ``Unpacking 1'' (see Table~\ref{tab:scenarios2}) leads to significantly lower estimates than using ``Unpacking 2'' $[t(251)=2.52 , p<0.01]$. Additionally, when conditioning on cue object 1, using ``Unpacking 2'' produces significantly lower estimates than using ``Unpacking 1''; $[t(165)=-3.31, p<0.001]$. These results show that typicality of the unpackings and, by proxy the sub- and super-additive effects, crucially depend on the conditioned cue object.

% unpacking the first cue objects via Unpacking 1 lead to estimates that are not significantly different from the packed estimates $[t(171)=0.74, p>0.05]$, while unpacking it via ``Unpacking 2'' produces significantly lower estimates $[t(165)=-3.31, p<0.001]$. These results seem to 

Our fitted model matches the experimental data well ($r=0.96$, $p<0.001$), only slightly underestimating the superadditive effect with cue object 2 and unpacking 1. We can conclude from the fact that this cue-dependent swap can be even partially carried out---for example, the superadditivity effect certainly does get swapped---indicates that these effects are not modulated solely by the prior typicality or inherent availability of the unpacked examples. The same unpacking that induces superadditivity in the presence of one cue object, does not induce it in the presence of the second cue object. Furthermore, a new unpacking can be chosen such that it induces superadditivity in the presence of the second cue object but not in the presence of the first. These results support a sampling process that is modulated by the cue objects, i.e. the observed data.

\begin{figure}
\centering
\includegraphics[scale=0.5]{figures/estimate2new.pdf}
\caption{\textbf{Experiment 2 results}. Mean probability estimates for each condition. Error bars represent the 95\% confidence interval of the mean. Red dots show estimates from the MCMC model with 230 samples, assuming 6 hidden objects in the scene. Unpacking 1 is typical for cue object 1 and atypical for cue object 2; unpacking 2 is typical for cue object 2 and atypical for cue object 1.}
\label{fig:results2}
\end{figure}

%In Experiment 1, we found that while the subadditivity effect was weaker than superadditivity, it was nonetheless still captured. For this experiment, only superadditivity is observable. We attribute the mismatch between our results from experiment 1 and experiment 2 to the fact that the effects are expected to have been weaker in experiment 2 and, given that subadditivity is already a weaker effect than superadditivity, may already have died away in the 20 second window provided. The model supports this explanation---its predictions in Fig. \ref{fig:results2} show that for a chain length of 230 (the best fit chain length for both experiments combined) the effect for subadditivity is almost totally gone, i.e. the probability estimate from the typically unpacked scenario is comparable in value to the estimate from the packed hypothesis.


%\cite{super} found similar results for queries on ``well-defined categories''. However, we believe the the reason that both these experiments observe superadditivity and not subadditivity, are different. We expound on this in the General Discussion.

\section{General discussion}

We have presented a rational process model of inference in complex hypothesis spaces. The main idea is to recast hypothesis generation as a Markov chain stochastically traversing the hypothesis space, such that hypotheses are visited with a long-run frequency proportional to their probability. Our simulations demonstrated that this model reproduces many observed biases in human hypothesis generation. Finally, we confirmed in two experiments the model's prediction that subadditivity and superadditivity depend critically on the typicality of unpacked examples.

Our work extends a line of research on using rational process models to understand cognitive biases. Most prominently, \citet{Thomas2008} have attempted in their HyGene model to explain a wide range of hypothesis generation phenomena by assuming that Bayesian inference operates over a small subset of hypotheses drawn from memory. We follow a similar line of reasoning, but depart in the assumption that hypotheses may be generated \emph{de novo} through stochastic exploration of the hypothesis space. This assumption is important for understanding how humans can generate hypotheses in complex combinatorial spaces where it is impossible to store all relevant hypotheses in memory.

Prior studies suggest that---when averaged over long time periods or across individuals---probability estimates converge roughly to the Bayesian ideal \citep{Vul2014}. Like other models based on Monte Carlo methods \citep[e.g.,][]{multistability,Lieder2013,shi10}, our model predicts exact Bayesian inference in the limit of large sample sizes. However, cognitively bounded agents are expected to be \emph{computationally rational} \citep{Gershman2015}: sampling takes time and energy, and hence the optimal sampling strategy will tend to generate relatively few hypotheses \citep{Vul2014}. %Is energy the right word here?

Our model recreates several cognitive biases exhibited by humans: subadditivity, superadditivity, anchoring and adjustment, weaker confidence in self-generated hypotheses, the crowd within, and the dud alternative and weak evidence effects. While some of these biases have been accounted for by other models, ours is the first unified rational process account. Table \ref{tab:compare} provides a systematic comparison of which phenomena are accounted for by different models.

Our simulation results rest on two key features of the model. First, our model posits an interplay between generation and evaluation of hypotheses: when a low probability hypothesis has been generated, the sampler is more likely to accept new proposals compared to when a high probability hypothesis has been generated. This property of MCMC allows us to understand superadditivity and related effects (such as the dud alternative and weak evidence effects), where unpacking a query into low probability examples causes a reduction in the probability estimate for the focal space. This feature also explains why participants give lower probability estimates to hypotheses that are self-generated compared to those generated by others and presented to them. A shortcoming of previous models based on importance sampling \citep{shi10} or cued recall \citep{Thomas2008} is that the generation and the evaluation processes are largely decoupled; the probabilities of the hypotheses already in the cache of generated hypotheses do not affect whether or not new hypotheses are generated.

\begin{table}%[htbp]
\centering
\caption{Comparison of stochastic sampling algorithms}
\label{tab:compare}
\begin{tabular}{ p{.28\textwidth} p{.20\textwidth} p{.20\textwidth} p{.20\textwidth}}
\toprule
\textbf{Effect} & \multicolumn{3}{ p{0.60\textwidth}}{\textbf{Stochastic Sampling Variants}} 
\tabularnewline
& Importance Sampling & Global proposal MCMC & Local proposal MCMC
\tabularnewline
\midrule
Subadditivity & \checkmark & \checkmark & \checkmark
\tabularnewline
Superadditivity  & $\times$ & \checkmark & \checkmark
\tabularnewline
Weak Evidence effect & $\times$ & \checkmark & \checkmark
\tabularnewline
Dud Alternative effect & $\times$ & \checkmark & \checkmark
\tabularnewline
Self-generation effect & $\times\tablefootnote{While an importance sampler does reproduce this effect, we have elaborated in the section comparing our MCMC model to importance sampling how its explanation does not extend to follow-up studies on this effect in \cite{conf}}$ & \checkmark & \checkmark
\tabularnewline
Crowd within & $\times$ & $\checkmark$ & \checkmark
\tabularnewline
Anchoring \& adjustment&  $\times$ & $\times$ & \checkmark \tabularnewline
\bottomrule
\end{tabular}
\end{table}


The second key property of our model is the autocorrelation of hypotheses in the Markov chain. This autocorrelation arises from two sources: the non-zero rejection rate (which ensures that the chain sometimes stays at its current hypothesis for multiple time steps) and the locality of the proposal distribution (which ensures that proposed hypotheses are in the vicinity of the previously generated hypothesis). Previous models based on importance sampling or cued recall generate new candidate hypotheses independently of the hypotheses that have already been generated (i.e., the previously generated hypotheses have no impact on future hypotheses). \cite{Lieder2013} argued that autocorrelation and locality of proposals in MCMC models can account for the anchoring and adjustment phenomena. They analyzed a one-dimensional continuous hypothesis space for numerical estimation, and we generalized this idea to combinatorial spaces. More broadly, several findings in the literature suggest hypothesis autocorrelation \citep{multistability,vul08,Bonawitz2014}. For example, the ``crowd within'' phenomenon \citep{vul08}, which we also simulate, demonstrates that errors in numerical guesses are correlated in time, and this error is reduced if the guesses are spread out.

MCMC models with global proposal distributions will show much weaker autocorrelation compared to those with local proposal distributions, because any autocorrelation will depend entirely on rejection of proposals. Since efficient samplers have relatively low rejection rates \citep{robert13}, there is reason to believe that human probability estimation makes uses of local proposal distributions. Evidence for locality has been found in domains analogous to that of hypothesis generation \citep{abbott2015random,smith2013multiply}, further suggesting that humans use local proposal distributions.


Previous work demonstrating the effect of superadditivity \citep{super} did not find subadditivity except in situations where the search was over an ill-defined fuzzy category, such that unpacked typical examples lead participants to consider a larger hypothesis space than entailed by the packed query. However, this effect was driven by a single item: \textit{Guns that you buy at a hardware store} with \textit{staple gun} as the unpacked typical example. Excluding this item, typical unpackings were not subadditive. Our experiments demonstrated that subadditivity can be obtained in well-defined (non-fuzzy) domains like ``words starting with the letter A'', and where typical unpackings do not extend the hypothesis space. A possible explanation for this is that, as opposed to the studies in \cite{super}, we time constrain the responses from our participants. This time pressure could lead to fewer samples being taken and an amplification of the subadditivity effect.


%Our experiment 2 however shows results that agree with \cite{super} on the effects that are observable for paradigms with "well defined categories" in showing that subadditivity is not observed, whereas superadditivity is. 

%To explain these effects, we focus on two properties, both of which are replicated in our model (a) Subadditivity is a weaker effect that superadditivity (see Figs \ref{fig:subadd}, \ref{fig:superadd} and \ref{fig:results1}), (b) these effects are time sensitive and reduce with increase in processing time (see Figs \ref{fig:subadd} and \ref{fig:superadd}), up until a resource rational threshold. The effects for our first experiment were expected to be of higher magnitude and in the 20 second time constraint, did not die out. Whereas in the second experiment,  due to constraints from the design of the experiments, the overall effects were expected to be of a smaller magnitude. Subadditivity is a weaker effect that superadditivity, and was washed out in the given time while superadditivity survived.
%While the effects in the experiments conducted by \cite{super} were designed to be as strong as possible (like our experiment 1), the participants were given no time constraints. We conjecture that the similarity between our results from experiment 2 and those from \cite{super} is obtained by the cancellation of two opposing effects (a) reduced typicality in our experiment that decreases the size of the effects, and (b) reduced processing time in our experiment that increases the size of the effects. Comparing our experiment 1 with \cite{super}, we see that only point (b) from before applies---i.e. the typicality is unchanged, but we include a constrain on processing time unlike \cite{super}---therefore the effect size of implicit subadditivity is increased in our experiment 1 and is observable.

\section{Future work}

%Our model can be improved in several ways. particularly by better leveraging past experiences and run-time information. For example, improving the way the proposal distribution incorporates these. A proposal distribution is expected to be strongly affected by what has been learned from past experience i.e. from knowledge of the structure of the hypothesis space by which locality is determined, learning the prior and sometimes developing biases towards specific classes of hypotheses \citep{markantimpact}. A possible extension from this work is to think about other ways in which to organize the latent space rather than by brute force clustering (for example in \cite{kemp2009structured}), and how this organization is learned. Depending on the specific problem, the hypothesis space could be very differently organized, changing the locality metric, as well as by making various classes of hypotheses more or less accessible \citep{markantimpact}. Further, the proposal distribution does not incorporate any external information at run time. Which parts of the hypothesis space to focus on, as well as modulation of the step sizes of the proposal distribution in order to appropriately optimize exploration of the space, might depend on the specific case at hand and human behavior in these cases might be better modeled by incorporating the run-time data into the proposal distribution. Using data to drive the proposal distribution would result in weighting the likelihood higher than the prior, in resource-constrained runs\footnote{irrespective of initialization and proposal distribution, the Markov chain converges to the exact posterior asymptotically}. This might successfully model observed cognitive effects like base rate neglect.

Our model can be improved in several ways. First, we adopted relatively simple assumptions about initialization of the Markov chain. Recent work suggests that humans might use a fast, data-driven proposal distribution learned from previous experience \citep{Yildirim,Gershman2014b}. Second, our simplistic assumptions about the proposal distribution could likewise be made more sophisticated by using data-driven methods. Finally, we have assumed that the number of samples is constrained solely by the available time, but the computational rationality perspective argues that this number is chosen adaptively to balance the benefits of taking more samples against their costs in time and energy \citep{Gershman2015,Vul2014,griffiths2015}. Investigating cognitive algorithms for meta-control of sampling is an interesting avenue for future research.

%This idea could potentially explain several cognitive biases not captured by our model, such as indirect priming effects \citep{mcnamara92} and the representativeness heuristic \citep{kahneman72}. As this initialization method incorporates the run-time data, it too might lead to weighting the likelihood more than the prior in resource constrained runs, giving base rate neglect.

%Another degree of freedom of our model is how strict or lenient the acceptance function is. One can modulate how likely the acceptance function is to accept states of lower probability by altering the `temperature' of the simulation \citep{hwang1988simulated} without losing the asymptotic exactness properties of MCMC Metropolis-Hastings. At high temperatures, the chain is more likely to accept lower probability hypotheses and vice versa for lower temperatures. How likely people are to explore the hypothesis space, and how likely they are to instead instead stay in one high probability mode, could reflect different values for this temperature parameter. This might help characterize differences in hypothesis search behavior between subjects or within subject depending on changing situations.

Our experiments and simulations only studied two domains (medical diagnosis and scene understanding), but there exist many real-world domains that impose a severe computational burden on mental inference. For example, it has been shown that humans are capable of simulating physical trajectories that they have never directly observed before and make fairly accurate inferences when predicting the motion of a projectile \citep{motion}, judging mass in collisions \citep{collision}, and judging the balance of block towers \citep{blocks}. Furthermore, research also suggests that humans sample noisy simulations of future trajectories \citep{smith2013sources,hamrick2015think}, but the precise sampling mechanisms are currently unknown. The number of possible trajectories is exponentially large in this domain, and thus approximate inference schemes like MCMC may come into play.

%As active learning tasks normally involve generating hypotheses and then calculating the expected reduction of posterior entropy over different tests \citep[see for example,][]{nelson2005finding, tsividis2014information}, a MCMC-based process model of hypothesis generation might be able to explain apparently sub-optimal active learning behavior such as confirmatory \citep{markant2015self}, local testing \citep{parpartactive} and other biases \citep{markantimpact} by the fact that the expected information gain is only evaluated over locally generated hypotheses proposed by a biased proposal distribution, and more likely accepted/generated only when the current hypothesis is bad.

% Needs a better written conclusion
%The scope of this work has been an attempt to further close the gap between optimal Bayesian models of cognition at the computational level and the biases and shortcomings we find when humans actually carry out inferences because of the algorithmic implementations of these computational ideas.

\subsection*{Acknowledgments}
This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. We thank Kevin Smith for helpful comments and discussions.
%\section*{Appendix: Latent Dirichlet allocation model of natural scenes}

%Latent Dirichlet allocation \citep[LDA;][]{lda} is a generative statistical model that posits that (in our case) objects observed in natural scenes are sampled from a mixture of $k$ possible `topics' (for example, kitchen appliances, or outdoor objects), each of which determines a distribution over objects (out of say $v$ possible objects) determined by the parameter $\boldsymbol{\Phi}$. How much of each of the $k$ topics is in each of the scenes is determined a priori by how likely these topics are in the environment, as dictated by the parameter $\boldsymbol{\alpha}$. To put these ideas in more mathematical terms:

%\begin{enumerate}
%\item A mixture over topics $\theta$ is sampled from a \textit{Dirichlet distribution} $Dir(\alpha)$. This Dirichlet distribution has one vector parameter $\boldsymbol{\alpha}$ determined by how probable, a priori, each of the $k$ possible topics is in the environment. The vector $\theta$ is a probability vector over all possible topics, giving the mixing proportions of each topic in a scene.
%\begin{align}
%p(\boldsymbol{\theta} | \boldsymbol{\alpha}) = \frac{\Gamma(\sum_{i}^k \alpha_i)}{\prod_i^k \Gamma(\alpha_i)} \theta_1^{\alpha_1 -1} \cdots \theta_k^{\alpha_k -1}
%\end{align}
%\item A specific topic $z$ is sampled from a \textit{Multinomial distribution} $Mult(\theta)$. This picks one topic, and the probability of picking each topic is given by its corresponding probability in the probability vector $\theta$ drawn in the previous step.
%\begin{align}
%p(z_i | \boldsymbol{\theta}) = \theta_i
%\end{align}
%\item Given this topic $z$, a specific object $w_j$, of all $v$ possible objects is sampled from another \textit{Multinomial distribution} $Mult(\boldsymbol{\phi_z})$. This picks one object, and the probability of picking each object is given by its corresponding probability in a probability vector $\boldsymbol{\phi_z}$ determined by how probable each of the $v$ possible objects is, given a specific topic z. There is one such vector for every $k$ possible topic, and these together constitute a matrix $\Phi$.
%\begin{align}
%p(w^j | \boldsymbol{\phi_z}) = \phi_z^j
%\end{align}
%\end{enumerate}

%The parameters of this model are $\boldsymbol{\alpha}$ which is a vector of size $k$, and $\Phi$ which is a matrix of size $k \times v$. Given these parameters, the probability of a scene is given by the joint probability of the $M$ objects in the scene ($\boldsymbol{w}$ of size $M$), the topic each of these objects were sampled from ($\boldsymbol{z}$ of size $M$), and the probability vector over topics ($\boldsymbol{\theta}$) for this scene that these topics $\boldsymbol{z}$ were sampled from. This is:

%\begin{align}
%p(\boldsymbol{w, z, \theta} | \boldsymbol{\alpha, \Phi} ) = p(\boldsymbol{\theta} | \boldsymbol{\alpha}) \prod_m^M p(z_m| \boldsymbol{\theta}) p(w_m | z_m, \Phi)
%\end{align}

%The logic for choosing a Dirichlet prior for $\theta \sim Dir(\boldsymbol{\alpha})$ becomes clear on seeing this form of the joint probability -- the Dirichlet distribution is conjugate to the multinomial distribution and when they are multiplied together, they form another multinomial distribution. From this joint probability, we can marginalize out the variables $\boldsymbol{\theta}$ and $\boldsymbol{z}$ to get a probability of the observed objects $\boldsymbol{w}$ in the scene, conditioned on the topic model parameters.

%\begin{align}
%p(\boldsymbol{w} | \boldsymbol{\alpha, \Phi} ) = \int p(\boldsymbol{\theta} | \boldsymbol{\alpha}) \prod_m^M \sum_{z_m} p(z_m| \boldsymbol{\theta}) p(w_m | z_m, \Phi) d\theta
%\end{align}

%To get the probability of the objects seen in several observed scenes, we can multiply together this expression obtained for each of the individual scenes, as all the scenes are considered independent. The data available to us \citep{greene13} is in the form of lists of objects that were observed to have occurred together in a scene. We use a python library \citep{ldapython}, to maximize the likelihood of this data assuming there are $k = 5$ topics, by optimizing the parameter values $\boldsymbol{\alpha}$ and $\boldsymbol{\Phi}$.

%Given the parameters for this generative model, we can now calculate the joint probability of any object combinations being in a scene (query word and cue word) -- even if we have never seen that specific combination of objects before. This is proportional to the quantity we are interested in -- the probability of the query word conditioned on the presence of the cue object. But this has a normalizing constant that can has to be found by marginalizing over all possible query objects. So we find ourselves in the familiar predicament where it is easy to find an unnormalized probability for each hypothesis (query object) conditioned on data (cue objects), but this distribution is very difficult to normalize as it involves adding up the probabilities of all the possible hypotheses (query objects). We use Markov chain Monte Carlo to draw samples from this distribution and make a sample based approximation of this probability distribution over query objects. We have stressed in previous sections that several effects we model (see Table \ref{tab:compare}) are immune to the choice of proposal distribution. Since we are interested in only the sub- and super-additivity effect in this domain (which can be replicated without locality in the proposal distribution), we refrain from making claims about how this complex hypothesis space of objects in scenes is organized in order to create clusters in this space to engineer a local proposal distribution. We instead use a simpler global proposal that satisfies the two more basic requirements of ergodicity and symmetry; a uniform distribution over possible hypotheses.

%The form of question we ask participants is ``What is the probability of the presence of object (or class of objects) Y given the scene contains object X?'' The answer to this question is not a normalized probability distribution. For example, in the presence of a tree in a scene, it might be 60\% probable that grass is in the scene, as well as 60\% probable that there are clouds in the scene, which already add up to more than 100\%. This is because there could be more than one other query object in the scene along with the cue object. The probability of the presence of Y in a scene with $k$ possible objects is given by:
% \begin{align}
% P_k(Y|X) = 1 - P_k(\neg Y|X)
% \end{align}
% where $P_k(\neg Y|X)$ is the probability of the object Y \textit{not} being in the scene with k possible objects. This in turn is given by the probability of none of each of the k objects being object Y :

% \begin{align}
% P_k(Y|X) = 1 - P(\neg Y|X)^k
% \end{align}

% Estimates of the probability distribution $P(Y|X)$ over objects Y in the presence of only one other object, are the normalized probabilities returned by the MCMC model. So the final answer from the participants answering the question are linked to the probabilities from the model as:

%\begin{equation}
%1 - (1 - P(Y|X))^k
%\label{eq:conv}
%\end{equation}
%Where $P(Y|X)$ are the normalized probability estimates returned by the MCMC model.

%Therefore, our model has two free parameters: the number of hidden objects in the scene ($k$) and the number of samples ($N$). These parameters were fit to the behavioral data from both experiments using a coarse grid search to optimize the mean-squared error between the experimental probability estimates and the probability estimates from the model. The value of $k$ that best fit the data was $k=6$, and the number of samples $N= 230$. This value of $k$ is in the same ballpark as values found for average number of uniquely labeled objects in natural scenes from data collected in \cite{greene13}. This value for $N$ as the number of samples is higher than numbers found in some previous work like \cite{Vul2014} etc, but an important distinction between he meaning of the `number of samples' in different cases is that in our case, the number of samples are decomposed into a histogram over a much smaller number of generated hypotheses, each of which can appear several times. so even is the number of samples is larger than numbers found previously, the number of generated hypotheses is more comparable.

% To lay this out more intuitively in the domain of the scene data, LDA prescribes that natural scenes are generated by first sampling a `topic', for example the topic of a kitchen, or a driveway etc. Each of these topics places a probability distribution over all the objects that one might see - for example, the kitchen topic places a higher probability on `microwave' than on `car', whereas the reverse is true for the driveway topic. The goal of the fit to a Latent Dirichlet Allocation model is to check what these topics are, how likely they are to be sampled, and how likely each object is, in each topic. 

