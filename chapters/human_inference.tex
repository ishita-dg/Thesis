%!TEX root = ../dissertation.tex
%\begin{savequote}[75mm]
%Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
%\qauthor{Quoteauthor Lastname}
%\end{savequote}


\chapter{Probabilistic inference in humans}
\label{chap:psych}

Reasoning in the face of uncertainty -- i.e. making probabilistic judgments -- is a hallmark of intelligent behavior. Understanding how humans reason probabilistically is therefore an important question. In this chapter, I will first provide some background on Bayesian models of cognition; these posit that humans are entirely rational in how they perform probabilistic inference. I will discuss what they provide and where they fall short. I will then provide an overview of approaches that have attempted to address these shortcomings by positing boundedly rational behavior. In particular, I will summarize two major lines of work: the biases and heuristics literature, and the literature on computational rationality. Finally, I will introduce a special case of bounded rational behavior viz. ecological rationality. This proposes that human probabilistic inference leverages structure in its environment to shape its inference procedures, and make best use of bounded rationality.

\section{Bayesian models of cognition}
Bayesian models of cognition follow in the tradition of `rational analysis'\cite{shepard1987toward, anderson1990adaptive}: an approach to cognitive science that frames cognition as an approximately optimal response to the structure and uncertainty present in natural tasks and environments. This framework specifies the goals of a system and the information it has access to, and makes predictions about behavior by determining what would be optimal under these assumptions. A central contribution to such models is that they go beyond describing phenomena and mechanisms, and attempt to provide insight into why the processes might be as they are. Since the optimal response can be derived from the assumptions, these models make testable predictions about behavior under new assumptions that can be empirically manipulated, and thereby facilitate the scientific process of building, falsifying and improving theories.

Bayesian models posit that humans reason probabilistically, following the tenets of Bayes' rule. Probability theory specifies how rational agents should reason in situations of uncertainty\cite{hacking2006emergence, gigerenzer1990empire}, and is therefore an important part of rational models of cognition. The motivation for using an specifically a Bayesian approach to probabilistic inference is that they provide an answer -- at least in principle -- of how humans go beyond the data collected solely from their own experience of the world, integrate it with abstract prior information, and make intelligent \textit{inductive inferences}. Many problems in our everyday life are vastly under-specified by our sensory input. To take an example from \citet{griffiths2008bayesian}: deducing with certainty the color of an object simply by observing light reflected from it is impossible since the input we receive is a combination of the light illuminating the scene, and the spectrum reflected by the object. Bayesian models posit that the reason we are nonetheless able to make reasonable guesses about the colors of objects around us is that we have strong expectations about the spectrum of light that usually illuminates our surroundings. This takes the form of a priori knowledge gained from previous experience, that we can integrate with the visual signal received -- using Bayes' rule -- to make decent guesses about the problem at hand.

While such models have been used to successfully and parsimoniously model behavior in several domains, two key concerns remain. First, exactly applying Bayes' rule is often intractable. This is an established and extensively studied problem in statistics. We discuss the details of this problem and potential solutions to it in Chapter \ref{chap:approx}. Second, significant empirical evidence suggests that humans responses often deviate significantly from Bayes optimal in systematic and predictable ways\citep{tversky1974judgment, slovic1971comparison, grether1980bayes, fischhoff1983hypothesis}. Both of these raise concerns with whether or not humans are actually performing exact Bayesian inference, and has led to much controversy\citep{mellers2001frequency, gigerenzer1996narrow, samuels2012ending}. 

In the next section we discuss some other approaches to modeling human inference that skirt these concerns with exact Bayesian models of cognition. These approaches move away from the rational analysis perspective and move closer to the psychological mechanisms underlying behavior -- but do so in very different ways.

\section{Bounded rationality}

We establish a stronger connection between rational models of cognition and psychological mechanisms by recognizing that humans are resource limited, and computing optimal responses might be outside the scope of the psychological mechanisms available at their disposal. This idea was formalized in \citet{simon1955behavioral} as `bounded rationality': the idea that the rationality -- and therefore the optimality -- of individual actors is limited by the information they have, the limitations on their cognitive resources, as well the finite amount of time they have to make decisions (see \citet{russell2016rationality} for a review). Several different strategies for taking into account the effects of information-processing constraints have been considered. Here we consider two key ones; first, we consider rejecting the principle of rational analysis in favor of finding simple but effective heuristics; and second, we consider incorporating constraints into the optimization process.

\subsection{Heuristics and Biases: shortcuts around optimal inference}

If the goal is achieving bounded rationality, there is no reason to retain rational analysis. We can posit other heuristic mechanisms by which people arrive at responses, that might be far easier to compute. Several of these might provide reasonably good responses most of the time -- satisfying the claim of being `boundedly' rational -- yet have no real connection to the procedures of optimal inference. 

Therefore, the argument is that while certain behaviors might look `as if' people are engaging in optimal Bayesian inference, they might be doing something completely different -- viz a heuristic strategy that is much easier to implement. In cases where this heuristic contradicts optimal Bayesian inference, we get the systemic and predictable deviations from optimality as recorded in empirical studies. This approach has been hugely influential in behavioral economics, -- pioneered by Kahneman and Tversky\cite{tversky} -- as well as in psychology (see \citet{gigerenzer2002bounded} for a review). 

A crucial shortcoming of these approaches however is that while they answer the `what' of the processes underlying human inference, by relinquishing the optimization perspective of rational analysis, they often fail to account for the `why'\footnote{\citet{gigerenzer2008heuristics} does address a version of the `why' question by characterizing heuristic judgment as a rational response to structure in the environment. i.e. by claiming that heuristics are `ecologically rational'. We discuss this in greater detail in the next section, as well as point out the shortcomings of this explanation.} This can lead to lists of heuristics, conceived with inspiration from the specific modes of failure noted in human inference, without a unifying theory of how these heuristics are learned or where they come from. 

Another problem is that of\emph{strategy selection} \citep{gigerenzer2008heuristics, marewski2014strategy} -- how do we choose a heuristic for a specific context? Most models of strategy selection assume that people are able to assess the usefulness of a strategy, through cost-benefit analysis \citep{johnson85, beach1978contingency, lieder2017strategy}, reinforcement learning \citep{erev05, rieskamp06}, or based on the strategy's applicability in a particular domain \citep{marewski2011cognitive, schulz2016simple} -- which in an of itself might be a resource-intensive process outside the scope of the posited limitations on cognitive resources. Further, all of these approaches require, either explicitly or implicitly, a feedback signal. This requirement poses a problem in inferential settings where no feedback is available. People can readily answer questions like ``How likely is it that a newly invented machine could transform a rose into a blackbird?'' \citep{Griffiths15} which lack an objective answer even in principle. 

Finally, while these heuristics have been studied primarily in the domain of judgment and decision making, probabilistic inference in humans is important for a much wider set of domains in humans -- including concept learning, causal attributions, and language learning. Many of the proposed heuristics are often specific to the kinds of problems studied in explicit judgment and decision making, very often in the domain of linear regression. These might not transfer well to other use cases for probabilistic inference in humans. In addition, the procedures used to isolate heuristics so far -- by studying deviations from normative inference in these explicit judgment tasks -- might not transfer to these other more complex domains since it might be difficult to isolate interpretable deviations from normativity in these more complex domains. A broader, more general theory of how heuristic inference arises in humans, as driven by some underlying principles, would allow a a more general theory of probabilistic inference humans -- Bayesian or otherwise -- that spans domains.

\subsection{Computational rationality: optimization under constraints}

Another approach is to explicitly account for the costs of computation in the overall optimization. 
%Some approaches to this are to involved in rational analysis (e.g., Anderson, 1990), handicapping rational models to produce behavior closer to that of human participants (e.g., Steyvers, Tenenbaum, Wagenmakers, & Blum, 2003), 
These approaches entail specifying the costs of information-gathering, cognitive resources, and time, as well as specifying an algorithm for computing a response that makes specific demands on these resources. By including these resource limitations in the optimization problem, we arrive at a bounded rational solution.

This problem can be discussed at different levels ranging from a computational-level account that describes the problem being solved but does not propose a plausible mechanism, to more mechanism driven accounts. At one end is to simply describe behavior as resource-rational \citep{Vul2014,griffiths2015, schulz2016simple} and posit a new optimization problem that accounts explicitly for the costs of internal computations. This approach however, punts the original problem of intractability one step ahead -- the bounded optimal objective function, that accounts for resources in addition to the original optimization objective, might be even harder to optimize than the original optimization. While this approach has great explanatory power, it still leaves open the question of how humans might be achieving this bounded optimality.

A middle ground is to build rational process models. These make specific claims about how the original optimization problem might be approximated, and demonstrates how limitations on information-gathering, cognitive resources, and time -- during the process of approximation -- could lead to deviations from optimality. Many of these approaches still retain an interpretation as computational rationality, i.e. that ceasing additional investment of time or cognitive resources is a (conscious or sub-conscious) choice that is made by computing the \textit{marginal utility} of additional investment, and deciding whether this additional utility is worth the cost of the investment\cite{gershman15}. The assumption is that increased investment of computational resources will start providing smaller and smaller gains to the marginal utility, whereas the cost of resources remains constant. Therefore we will reach a point in time where additional investment is not longer worth it.

To naively compute marginal utility we need to know how much closer we would get to the optimal response with additional investment. However, if we don't know the optimal response, it is not possible to measure how close we are or will be to it. The question remains therefore of how to decide when to stop investing computational resources. One way around this is to build rational process models where the gradient of the cost function (including the cost of resources) can be locally estimated. This requires the cost function to be smooth and differentiable. Under these assumptions, while we may not be able to calculate the exact utility of additional investment -- since we don't know the optimal response -- we can compute if this utility is going up or down. The signal to stop additional investment, is therefore when this gradient becomes negative.

In Chapter \ref{chap:MCMC} we introduce a rational process model for probabilistic inference where the cost function is in fact smooth and amenable to local gradient estimation. However, several other models for resource-rational inference do not fall into this smooth optimization regime. While much progress has been made in characterizing several behaviors as resource-rational, they continue -- without further assumptions -- to fall into the trap of being an `as-if' model without a realistic proposal of how boundedly optimal behavior could be implemented. 

Further, most rational process models are based on domain-general algorithms, and thus struggle to explain the context-sensitivity of inferential errors (see \citet{mercier2017enigma} for a similar argument). Some models explain why certain kinds of queries induce certain kinds of errors \citep{dasgupta2017hypotheses}, but do not explain how errors can be modulated by other queries in the same context \citep{gershman2014amortized,dasgupta2018remembrance}. 


\section{Ecological rationality}

The two approaches in the previous sections -- of heuristic inference and rational process models -- have some common ground; certain heuristics might be considered accurate approximations \citep{gigerenzer2009homo, parpart2018heuristics, belousov2016catching}. This interpretation has been furthered significantly by the approach proposed by \citet{gigerenzer2011heuristic}'s research program. This program suggests that heuristics are not simply sub-optimal hacks that serve error-prone human inference, but rather that they leverage underlying structural information in environments to make smart inferences without excessive investment. This idea is termed ecological rationality.

This idea was also originally suggested by \citet{simon1955behavioral}. He used the analogy of a pair of scissors for human inference, where one blade represents the cognitive limitations of humans and the other the structures of the environment, illustrating how minds compensate for limited resources by exploiting known structural regularity in the environment. While the heuristics literature has extensively studied the ecological rationality of the heuristics studied in human judgment \citep{gigerenzer2008heuristics}, many studies remain tied to specific judgment and decision-making domains and cannot be generalized since they do not make explicit claims about underlying mechanism. In addition, and it remains to to be shown how -- even in the standard JDM domains -- such heuristics are learned or chosen optimally for the specific environment at hand. On the other hand, the rational process model literature provides more generalizable models since they are based on underlying mechanism. But they fail to account for structure in the environment, since by the very virtue of domain-general inference algorithms, there is no adaptation to specific environments. Certain models that do explicitly take feedback from the environment within a domain general procedure, fall into the strategy selection trap -- where optimally choosing the right domain-sensitive heuristic remains intractable and outside the scope of realistic psychological mechanisms.

In this thesis, I propose a way to combine the the advantages of these two approaches into one. I show how the principle of amortization (discussed extensively in Chapter \ref{chap:amort}) used within rational process models of human probability judgment based on approximate Bayesian inference (discussed in Chapter \ref{chap:approx}) can lead to ecologically rational behavior, within the scope of realistic psychological mechanisms. I also discuss how this approach can also inform the study and engineering of artificially intelligent systems, as well as its relevance across domains -- from more traditional JDM domains to much broader ones like causal inference and language.

