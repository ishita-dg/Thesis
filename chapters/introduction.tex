%!TEX root = ../dissertation.tex
\chapter{Introduction}
\label{chap:intro}

Understanding the flexibility and efficiency of human intelligence, as well as how to incorporate it in artificial systems, has been a long-standing open problem. A promising and successful approach has been to build structured probabilistic models of cognition\footnote{In Chapter \ref{chap:psych}, I discuss in greater detail specifically Bayesian probabilistic models, which are a subset of probabilistic models in general. Structured probabilistic models are also a type of \textit{generative model} since they make explicit claims about how the observed data is generated.}. These have been very successful as a theory of human-like intelligence for two key reasons. First, by being `structured' they allow for primitives of human symbolic thought -- like rules, grammar, and logic -- that allow humans to generalize far beyond our direct experience. \cite{fodor88, chomsky2002syntactic} A classic example is of mathematical knowledge, for example, once we know the rules of addition, we can add two numbers we may never have added or even seen before. For a more everyday example, we know that if ``Jane is taller than Gloria'', this implies that ``Gloria is shorter than Jane", since that is a logical deduction -- even if we may never have met Jane or Gloria, or know anything about their actual heights. These abilities require abstract, logical, structured representations. However, such representations cannot explain another crucial aspect of human intelligence, namely graded, uncertain inferences.\cite{rogers2004semantic} In many such cases, people do not have adequate information to make logical deductions, but can make reasonable `guesses' and are aware of their own uncertainty. Solely with access to discrete, logical rules, it would be impossible to account for human beliefs like `most birds can fly', or the human ability to make sense of and learn from ambiguous signals. This brings us to the second key aspect of structured probabilistic models -- that they operate over probabilities, not Boolean truths. Structured probabilistic models provide a broad, flexible framework with these key desirable properties, that abstracts away the core of the problems that intelligent systems need to solve, and provides a normative solution. This permits us to understand human cognition across a wide range of domains\citep{griffiths2008bayesian} from motor control\citep{kording2006bayesian} to social cognition\citep{baker2007goal}. They have also been adopted and used in artificial intelligence\citep{lake2015human, del2013understanding, zhao2011image, steyvers2007probabilistic}.

However, these models face some crucial shortcomings as models of intelligent behavior. 
% I first discuss the chief concerns with them as models of human cognition, followed by an outline of the approach this thesis takes -- of taking a renewed interest in the environment in which intelligent systems function -- to address these problems. I then discuss the role of structured probabilistic models in artificial intelligence, the key shortcomings, and resulting engineering solutions. I then discuss how the same approach -- of studying and understanding the role of the environment -- can better inform our search for human-like artificial intelligence.
While structured probabilistic models provide a normative answer and a good theory of the abstract problem intelligent systems are trying to solve, they provide no insight into how these systems might actually solve these problems (i.e. make probabilistic inferences in structured models) at the level of algorithms or psychological processes\citep{marr1976understanding}. %In other words, these models provide a good description at the `computational level' of abstraction in Marr's hierarchy\citep{marr1976understanding}, but do not provide an `algorithm / representation level' solution. 
This concern is exacerbated by two key issues. First, making exact probabilistic inferences in these structured models is largely intractable. The very flexibility of these models, in being able to generalize far outside direct experience and account for uncertainty, can hinder finding a solution to a specific given problem. The space of solutions these models can represent is so vast, that finding the right solution for the problem at hand is like finding a needle in a haystack. I discuss this issue in greater detail in Chapter \ref{chap:approx}.\footnote{The intractability of exact inference is exactly analogous to the intractability of computing the partition function in statistical physics. Several approaches developed to address this problem in physics can be generalized to the problem of approximate probabilistic inference. These are discussed in greater detail in Chapter \ref{chap:approx}.} Therefore, despite significant advances towards explaining behavior, they largely remain `as-if' models that describe what intelligent systems might be doing, but do not provide a complete picture of how cognition works. These concerns have also hindered the adoption of these structured probabilistic models, in mainstream machine intelligence. Second, much empirical evidence suggests that humans make several systematic inferential errors, with their probabilistic estimates deviating consistently from the predictions of exact probabilistic inference. Some of these include effects like base rate neglect\citep{koehler1996base}, where people tend to ignore certain sources of information like prior probabilities, or the anchoring effect\citep{tversky} where putatively unrelated sources of information influence future inferences, among many others. Wikipedia lists over a hundred such established cognitive biases. The fields of behavioral economics\citep{kahneman1973psychology} (which was arguably formed around the documentation and analysis of such biases), as well as cognitive psychology\citep{phillips1966conservatism, gigerenzer1996reasoning} have responded to these findings with the proposal that humans in fact do not perform probabilistic inference at all, and instead employ heuristic strategies. These heuristic strategies usually sacrifice the guarantee of being optimal but are often sufficient for achieving immediate goals, and are computationally much cheaper than computing optimal responses. In keeping with this, the hugely successful deep-learning approach to machine intelligence is largely based on heuristic pattern-matching\citep{marcus2018deep, lake18}, rather than normative probabilistic inference in structured models. 

Heuristic solutions however do not have the explanatory power of structured probabilistic models: they also do not generalize well, and tend to be very domain-specific. Can we address the shortcomings of structured probabilistic models, to allow them to be more complete models of human intelligence?

%These approaches have been discussed in greater detail in Chapter \ref{chap:psych}. 

%While such models can replicate many of these biased behaviors in certain domains, without any sense of normativity, it is difficult to answer question like when and why certain heuristics are employed, and even exactly what the heuristic even is, or how it came about. Without answers to these questions, it is difficult to falsify specific claims or make new predictions, thereby losing explanatory power. 

%Therefore, while these heuristic theories make stronger and possibly more believable claims about the psychological mechanisms underlying human inference, by relinquishing the normative framework of inference in structured probabilistic models, they lose explanatory power of `why' human cognition is the way it is.
%these heuristics are brittle and any single heuristic cannot explain the wide variety of behaviors observed in human inference -- ranging from near-normativity in some case to wide range of different kinds of cognitive biases. This has lead to the development of several different heuristics, and theories where people choose a specific heuristic for each domain. However, 
%this approach leaves many questions open: What is the set of heuristic strategies humans use? In the absence of any sense of normativity, why were those specific heuristic chosen? How do humans choose when to use which heuristic? With such a loosely defined theory, it is difficult to falsify specific claims or make new predictions. Therefore, while these heuristic theories make stronger and possibly more believable claims about the psychological mechanisms underlying human inference, by relinquishing the normative framework of inference in structured probabilistic models, they lose explanatory power of `why' human cognition is the way it is.

\section*{Considering the environment in human intelligence}

This thesis addresses these two key criticisms of structured probabilistic models by providing a) a more computationally tractable solution to inference and b) an explanation for deviations from exact inference. This is done by taking a renewed look at the environments in which humans operate. The importance of the environment in shaping intelligent behavior, and thereby the importance of considering it when trying to understand intelligence, has been around in psychology as far back as 1943 with the work of Brunswik\citep{brunswik1943organismic}, and has since been periodically reinstated by the works of Simon\citep{simon1956rational}, %Anderson\citep{anderson1990adaptive} 
and Gigerenzer \citep{gigerenzer1999simple} among others. These works posit that a better lens to look at human cognition is not of pure rationality or normativity, but rather through the lens of `ecological rationality' -- where the mind makes best use of limited cognitive resources, by leveraging underlying structure in the environment. However, most mainstream psychology focuses primarily on the internal frameworks, mechanisms, and representations within the human mind, largely neglecting the structure of the environment and how it might strongly impact these. In the words on Egon Brunswik, ``Psychology has forgotten that it is a science of organism-environment relationships, and has become a science of the organism''.  

This thesis posits new models of human probabilistic inference that leverage environmental structure flexibly to ease the computational burden of exact inference in structured probabilistic models. This adaptation to the environment is made possible by considering the role of memory as a computational resource, and re-using previous computations. If there is underlying structure in the distribution of queries posited by an environment, it can be picked up and used by a system that tries to efficiently re-use the computations done in response to these queries. This process is called \textit{amortization}, and is discussed in further detail in Chapter \ref{chap:amort}. %If there is underlying structure in the distribution of queries posited by an environment, it can be picked up and used by a system that tries to efficiently re-use the computations done in response to these queries (see Chapter \ref{chap:amort} for details). 
The resulting computational savings lead to more psychologically realistic demands of human cognition, thereby rendering approximate probabilistic inferences in structured models less intractable and more plausible. But there is no free lunch; these computational savings come at a cost. While they might ease some inferences, they do so by making smart approximations. This can make certain kinds of errors more likely. A system that utilizes implicit underlying structure in the distribution of queries will make mistakes when this structure is violated. I show that the errors made by such models exactly mirror the patterns of cognitive biases observed (historically as well as in a series of new experiments) in humans. This program of research therefore jointly addresses both of the key concerns with structured probabilistic models of human cognition outlined above.

%Further, it is not clear how these structured models might be learned from experience. Most approaches involve a search over many structures to find the one that fits data best. My virtue of being highly structured, this space of possible structures is large. While significant progress has been made in improving the quality of this search, it remains computationally challenging. In this thesis, we will primarily address the first issue, and discuss the second issue briefly in Chapter \ref{chap:LTI}.

\section*{Considering the environment in machine intelligence}

While much recent work has demonstrated the advantages of structured probabilistic models in building better artificial intelligent systems \citep{lake2015human, del2013understanding, zhao2011image, steyvers2007probabilistic}, the difficulty of inference remains a major impediment in their wide-spread adoption. Engineering solutions to easing this inference have included the re-use of past computations i.e. amortization, in the form of a recognition model\citep{dayan1995helmholtz, kingma2014auto} or, in the age of deep-learning, an inference network \citep{kulkarni2015picture, mnih2014neural, rezende2015variational, paige2016inference}. In fact, even in the absence of any explicit probabilistic generative model, many machine learning methods that simply do discriminative\footnote{Discriminative models directly model conditional distributions, i.e. posterior distributions, as opposed to generative models that model the full data generating process, i.e. the joint distribution. A more detailed discussion of this distinction is made in Chapter \ref{chap:amort} in the section on amortization in machine learning.} classification or `pattern matching', can be interpreted as amortized inference in an implicit model.% (see Chapter \ref{chap:amort} for details). 

As discussed briefly earlier (and in greater detail in Chapter \ref{chap:amort}), amortization leads to adaptation to the environmental distribution of queries. Most modern machine-learning methods implicitly invoke some form of amortized inference, and are thereby strongly influenced by statistical structure in their training environments. More explicitly addressing the role of the environment in shaping the inference procedures learned by these systems can have great value -- both for better understanding current systems, as well as building better ones. However, as in cognitive science and psychology, machine learning methods have historically largely neglected the role of the system's learning or training environment. Progress is usually gauged by how new models and mechanisms perform on unchanging, often arbitrarily selected, standardized data sets. Yet it is often unclear -- without a better understanding of these data sets -- what exactly good performance on them really means. Not only is the role of the training environment in shaping learned procedures poorly understood, it is as a result also underutilized. We have complete control over the training environments our artificial systems receive. Engineering these environments -- rather than only engineering the inner mechanisms -- is an important and promising approach (with complementary advantages to the engineering of inner mechanisms) towards building systems that exhibit intelligent behavior.

The second part of this thesis discusses and demonstrates how analysis and manipulation of learning environments can provide insight into how to artificially develop central tenets of human intelligence like causal inference and language. These insights also provide new ways to study how humans acquire and implement these complex behaviors.

%This is further highlighted by the fact that humans have limited computing resources, and therefore are in more dire need to adapt to their environments and selectively optimize performance where it matters. Learning about things that don't occur often is often not even possible due to limited experience, and is largely not worth it given the cost on computation.  define ecological rationality.

\section*{Outline}

The key insight that I build on in this thesis is that humans are not general purpose computers -- and neither should artificial systems that hope to emulate human intelligence. Humans and and human-like machines have to work with limited computational power, and have to interact only with specific kinds of environments. While normative approaches like structured probabilistic models give us deep insights into the domain-general `what and why' of intelligent behavior, it leaves open the crucial question of `how' intelligent behavior can realistically manifest in ecologically relevant domains, within limits on time, data and energy. I argue that algorithmic realizations of ecological rationality -- where intelligent systems make the best use of limited resources by leveraging underlying structure in the environment -- within the framework of structured probabilistic models, provides a promising way forward. Through a series of investigations into human and artificial intelligence, I demonstrate how this insight can lead us to better models of human cognition, as well as better approaches to artificial intelligence.

In Chapter \ref{chap:psych}, I review the literature on human probabilistic inference. I first review normative accounts, followed by a review of other approaches to human probabilistic inference. I highlight the contributions and shortcomings of these approaches, and outline how this thesis suggests a more complete picture by unifying their complementary advantages. In Chapters \ref{chap:approx} and \ref{chap:amort}, I review some technical and conceptual background. In Chapter \ref{chap:approx} I cover technical background on approximate probabilistic inference. Several of these methods have been developed in statistical physics, and I review how they can be generalized to address the intractability of Bayesian inference. In Chapter \ref{chap:amort} I introduce a formal notion of computational re-use i.e. amortization and discuss how it can be used within algorithms for approximate inference. I outline how the process of amortization can leverage underlying environmental structure to give ecologically rational behavior. Finally, I review how amortization has been an implicit part of several approaches to machine learning as well as models of human cognition, and discuss the value of addressing it more explicitly.

%In this thesis, I show how adaptation to environments (as necessary under resource constraints) can both a) render inference more tractable on average, and b) explain deviations from normative Bayesian inference.

In Chapters \ref{chap:MCMC} to \ref{chap:LTI}, I discuss how these concepts can be used to build new models for human probabilistic inference. Chapter \ref{chap:MCMC} studies human inferences in large hypothesis spaces. This is a challenging problem, where exact inference is intractable. I demonstrate that a sample-based approximation under ecologically rational constraints, can replicate the specific kinds of biases observed in human inference in such large hypothesis spaces. 
% This model relies on two crucial components to replicate these deviations from normativity: first, a small number of samples, and second, initialization based on question framing. I argue that these components are ecologically rational: humans have limited cognitive resources, hindering the possibility of a large number of samples in the approximation, and initialization based on question framing is reasonable since the context of queries in the environment are often related to and informative of the optimal response. 
Further, in small hypothesis spaces, this model returns optimal responses. This allows us to jointly explain both the rationality as well as various kinds of irrationality of human inference within the same framework. Chapter \ref{chap:MCMC_amort} expands on the role of the environment in sample-based approximations. I empirically demonstrate, as well as build models for, re-use of computation in consecutive related queries. This kind of re-use -- or amortization -- is ecologically rational since queries in the real world are often related, and the best use of limited resources is to re-use previously completed computations.

Chapter \ref{chap:LTI} studies the role of re-use and ecological rationality in greater detail. In particular, I discuss amortization in a variational framework, which is more amenable to flexible re-use than sampling. I demonstrate how this framework can replicate several cognitive biases that involve context-sensitive non-normative reactions to different sources of information, 
% -- including base-rate neglect, conservatism and belief bias -- 
as well as various various other effects.
% such as the response variability in probabilistic judgments, effects of experimental design, and certain memory effects. 
This work posits an algorithmic approach, via amortization of inference, to understanding ecologically rational heuristic behavior. I also discuss how this approach can be combined with previously discussed sampling approaches. This unification results in a single model that can jointly account (with few assumptions, and with mechanistic commitments that are within the realm of the psychologically plausible) for a wide range of biased inferences -- while still retaining the capacity for optimal inference in the limit of infinite experience in the environment, and/or infinite computational resources.

Chapters \ref{chap:sentences} and \ref{chap:causal} shift gears to focus on the role of the environment in studying and improving modern artificial intelligence (AI).  In Chapter \ref{chap:sentences}, I consider a system for natural language processing, where ecologically rational heuristic behavior can explain the errors this system makes -- analogous to how they explained human biases in Chapter \ref{chap:LTI}. I outline an approach that uses this insight to better asses as well as improve these systems. In Chapter \ref{chap:causal}, I demonstrate how we can leverage the strong influence of the environment on the inference procedures learned, to engineer the inference procedures we want. I show that manipulation of the environment of a very simple learning architecture, can give rise to complex behaviors including causal inference procedures and active information seeking. The strong control we have over the environments encountered by artificial systems also allows new investigations into ecological rationality, and how it shapes human cognitive abilities.