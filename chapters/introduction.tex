%!TEX root = ../dissertation.tex
\chapter{General Introduction}
\label{chap:intro}

Understanding the flexibility and efficiency of human intelligence, as well as how to incorporate it artificially, is a long-standing open problem. A promising and successful approach to shed light on human-like intelligence has been through the development of structured probabilistic models of cognition. These have been vastly successful as a theory of intelligence for two key reasons. First, by being "structured" they allow for primitives of human symbolic thought -- like rules, grammar, and logic -- that allow humans to generalize far beyond our direct experience\cite{fodor88, chomsky2002syntactic}. A classic example is of mathematical knowledge, for example, once we know the rules of addition, we can add two numbers we may never have added or even seen before. For a more everyday example, we know that if "Jane is taller than Gloria", this implies that "Gloria is shorter than Jane", since that is a logical deduction -- even if we may never have met Jane or Gloria, or know anything about their actual heights. These abilities require abstract, structured representations. However, such representations cannot explain another crucial aspect of human intelligence, i.e. graded inferences\cite{rogers2004semantic}, where despite not having adequate information to make deductions, people function reasonably well. Solely with access to discrete, logical rules, it would be impossible to account for human beliefs like `most birds can fly', or to understand any sort of `exceptions to a rule', or notions of things that are `more similar' than others. This brings us to the second key aspect of structured probabilistic models -- that they operate over probabilities, not discrete truths. With these key properties, structured probabilistic models provide a broad  framework that abstracts away the core of the problems that intelligent systems need to solve. This permits us to understand human cognition across a wide range of domains \footnote{In Chapter \ref{chap:psych}, I discuss in greater detail the advantages specifically of Bayesian models.}. ..They have also been adopted and used in artificial intelligence... 

However, two crucial shortcoming of these models are that they are intractable to learn and second that they are intractable to carry out inferences in. \footnote{In this thesis I will focus primarily on the second problem, though I briefly address the first issue in Chapter \ref{chap:causal}}. Despite significant advances in how they explain behavior in humans, they remain "as-if" models that do not provide a complete model of how humans actually work. This view has been fueled by much evidence that shows that people aren't quite doing this anyway, with biases and heuristics. 

A crucial shortcoming of these approaches is that they focus purely on the computational processes taking place within the human mind. Even approaches that make process-level claims, focus solely on cognitive mechanisms. What is missing is a serious consideration of the environments in which intelligent systems act. The analogous concern also exists with the study of AI, where progress is gauged largely by how new models and mechanisms perform on unchanging, often arbitrarily selected, standardized data sets. A crucial insight is that humans are not a general purpose computing machine, as several "fully normative" models would have you believe. Therefore, neither should AI that hopes to emulate human intelligence. Humans have developed -- both over evolutionary and lifetime time scales -- to solve specific kinds of problems. Our environment strongly affects our learning and inference procedures. This is further highlighted by the fact that humans have limited computing resources, and therefore are in more dire need to adapt to their environments and selectively optimize performance where it matters. Learning about things that don't occur often is often not even possible due to limited experience, and is largely not worth it given the cost on computation.  define ecological rationality.
Herb Simon / Brunswick = importance f the environment.

This leads us to a solution to the conundrum that humans are flexible, but models that reflect this flexibility are intractable. This thesis expands on this. I incorporate the role of the environment in how Bayesian models of cognition can be approximated. This provides a mechanistic accounts of how inference might be carried out within resources limitations. We also find that several errors observed in human inference can be modeled by ecologically rational limitations on general-purpose approximate inference, as well as test new predictions of these models in new experiments.

Further, the large computational costs of these models has also torpedoed the adoption of such structured models in modern AI. Many approaches therefore implicitly amortize computations. Amortized computations, as discussed before, have a tendency to reflect environmental structure. But this aspect has been largely neglected.  I also discuss how a deeper understanding of the role of the environment in shaping the inference procedures of intelligent systems, can inform the study and engineering of artificially intelligent systems. AI has similarly largely neglected the crucial role of the environment in shaping the inference strategies learned by intelligent systems, choosing instead to focus only on the internal mechanisms. I demonstrate how analysis and manipulation of learning environments can provide insight into how to artificially develop central tenets of intelligence like causal inference and language, as well as inform the underlying mechanisms of how humans acquire and implement these complex behaviors.

\section*{Outline}

First and foremost, in Chapter \ref{chap:psych}, I review the theoretical framework surrounding human probabilistic inference. I first review rational Bayesian accounts, followed by a review of boundedly rational approaches. Finally I discuss ecological rationality -- a specific subset of these boundedly rational approaches that explicitly posits adaptation to structure in the environment. I highlight the contributions and shortcomings of these approaches, and outline how this thesis suggests a more complete picture by unifying their complementary advantages. In Chapters \ref{chap:approx} and \ref{chap:amort} I review the technical and conceptual background for this thesis. In Chapter \ref{chap:approx} I cover technical background on approximate probabilistic inference. In Chapter \ref{chap:amort} I introduce a formal notion of computational re-use i.e. amortization and how it can be used within algorithms for approximate inference. I also discuss in each chapter the history of these concepts as applied -- explicitly or implicitly -- in models of human cognition. In subsequent chapters, I demonstrate how these concepts can be used to build new and better models of human inference.

%In this thesis, I show how adaptation to environments (as necessary under resource constraints) can both a) render inference more tractable on average, and b) explain deviations from normative Bayesian inference.

In Chapter \ref{chap:MCMC}, I study human inferences in large hypothesis spaces. This is a challenging problem, where exact inference is intractable. I demonstrate that a sample-based approximation under ecologically rational constraints, can replicate the specific kinds of biases observed in human inference in such large hypothesis spaces. 
% This model relies on two crucial components to replicate these deviations from normativity: first, a small number of samples, and second, initialization based on question framing. I argue that these components are ecologically rational: humans have limited cognitive resources, hindering the possibility of a large number of samples in the approximation, and initialization based on question framing is reasonable since the context of queries in the environment are often related to and informative of the optimal response. 
Further, in small hypothesis spaces, this model returns optimal responses. This allows us to parsimoniously explain both the rationality as well as various kinds of irrationality of human inference within the same framework. Chapter \ref{chap:MCMC_amort} expands on the role of the environment in such sampling frameworks. I empirically demonstrate, as well as models for, re-use of computation in consecutive related queries. This kind of re-use -- or amortization -- is ecologically rational since queries in the real world are often related, and the best use of limited resources is to re-use previously completed computations.

Chapter \ref{chap:LTI} studies the role of amortization in greater detail. In particular, I discuss amortization in a variational framework, which is more amenable to flexible re-use than sampling. I demonstrate how this framework can replicate several cognitive biases that involve context-sensitive sub-optimal reactions to different sources of information, 
% -- including base-rate neglect, conservatism and belief bias -- 
as well as various various other effects.
% such as the response variability in probabilistic judgments, effects of experimental design, and certain memory effects. 
This work posits an algorithmic approach, via amortization of inference, to understanding ecologically rational heuristic behavior. I also discuss how this approach can be combined with previously discussed sampling approaches, to jointly and parsimoniously account (with mechanistic commitments) for a wide range of biased inferences while still retaining the capacity for optimal inference in the limit of infinite experience in the environment, or infinite computational resources.

Chapters \ref{chap:sentences} and \ref{chap:causal} shift gears to focus on the role of the environment in studying and improving modern artificial intelligence (AI).  In Chapter \ref{chap:sentences}, I consider a system for natural language processing. I first discuss this system, like many modern deep-learning based AI systems, can be interpreted as performing amortized inference. I then demonstrate how ecological rationality in this inference procedure can explain the errors this system makes -- analogous to how they explained human biases in Chapter \ref{chap:LTI}. I outline an approach that uses this insight to better asses and potentially also improve these systems. In Chapter \ref{chap:causal}, I demonstrate how we can leverage the strong influence the environment has on the inference procedures learned, to engineer the inference procedures we want. I show that manipulation of the environment of a very simple learning architecture, can give rise to complex behaviors including causal inference procedures and active information seeking behavior. The strong control we have over the environments encountered by artificial systems also allows new investigations into ecological rationality, and how it shapes human cognitive abilities.