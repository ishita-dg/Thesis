%!TEX root = ../dissertation.tex
%\begin{savequote}[75mm]
%Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
%\qauthor{Quoteauthor Lastname}
%\end{savequote}


\chapter{Probabilistic inference in humans}
\label{chap:psych}

Reasoning in the face of uncertainty is a hallmark of intelligent behavior. In this chapter, we study how humans make such probabilistic inferences. I will first provide some background on Bayesian models of cognition; these posit that humans are entirely rational in how they perform probabilistic inference. I will discuss what they provide and where they fall short. I will then provide an overview of approaches that have attempted to address these shortcomings by positing boundedly rational behavior. In particular, I will summarize two major lines of work: the heuristics and biases literature, and the literature on computational rationality. Finally, I will introduce a special case of bounded rational behavior viz. ecological rationality. This proposes that human probabilistic inference leverages structure in its environment to shape its inference procedures, and makes best use of bounded rationality.

\section{Bayesian models of cognition}
Bayesian models of cognition follow in the tradition of `rational analysis'\cite{shepard1987toward, anderson1990adaptive}: an approach to cognitive science that frames cognition as an approximately optimal response to the structure and uncertainty present in natural tasks and environments. This framework specifies the goals of a system and the information it has access to, and makes predictions about behavior by determining what would be optimal or normative under these assumptions. A central contribution of such models is that they go beyond describing phenomena and mechanisms, and attempt to provide insight into \textit{why} the processes might be as they are. Since the normative response can be derived from the assumptions, these models make testable predictions about behavior in new situations (that can be empirically generated and manipulated), and thereby facilitate the scientific process of building, falsifying and improving theories.

Bayesian models posit that humans reason probabilistically, following the tenets of Bayes' rule. Probability theory specifies how rational agents should reason in situations of uncertainty\cite{hacking2006emergence, gigerenzer1990empire}, and is therefore an important part of rational models of cognition. The motivation for using specifically a Bayesian approach to probabilistic inference is that they provide an answer -- at least in principle -- of how humans go beyond the data collected solely from their own experience of the world, integrate it with abstract prior information, and make intelligent \textit{inductive inferences}. Many problems in our everyday life are vastly under-specified by our sensory input. To take an example from \citet{griffiths2008bayesian}: deducing with certainty the the color of an object is impossible simply by observing light reflected from it, since the input we receive is a combination of the light illuminating the scene, and the spectrum reflected by the object. Bayesian models posit that the reason we are nonetheless able to make intelligent guesses about the colors of objects around us is that we have strong expectations about the spectrum of light that usually illuminates our surroundings. This takes the form of a priori knowledge gained from previous experience, that we can integrate with the visual signal received -- using Bayes' rule -- to make useful guesses about the problem at hand.

While such models have been successfully used to model behavior in several domains, two key concerns remain. First, exactly applying Bayes' rule is often intractable -- particularly in flexible, structured models. This is an established and extensively studied problem in statistics.\cite{Jaakkola1999, andrieu2003introduction} I discuss the details of this problem and potential solutions to it in Chapter \ref{chap:approx}. Second, significant empirical evidence suggests that humans responses often deviate significantly from Bayes optimal in systematic and predictable ways. \citep{tversky1974judgment, slovic1971comparison, grether1980bayes, fischhoff1983hypothesis} Both of these raise concerns about whether or not humans are actually performing exact Bayesian inference, and has led to much controversy. \citep{mellers2001frequency, gigerenzer1996narrow, samuels2012ending} 

In the next section I discuss some other approaches to modeling human inference that skirt these concerns with exact Bayesian models of cognition. These approaches move away from the rational analysis perspective and move closer to the psychological mechanisms underlying behavior\citep{griffiths2012bridging} -- but do so in very different ways.

\section{Bounded rationality}
\label{sec:psych_BR}

An important path toward establishing a stronger connection between rational models of cognition and psychological mechanisms is to recognize that humans are resource limited, and computing exact normative responses might be outside the scope of the psychological mechanisms available at their disposal. This idea was formalized in \citet{simon1955behavioral} as `bounded rationality': the idea that the rationality -- and therefore the normativity -- of individual actors is limited by the information they have, the limitations on their cognitive resources, as well the finite amount of time they have to make decisions (see \citet{russell2016rationality} for a review). Several different strategies for taking into account the effects of information-processing constraints have been considered. Here I present two key ones; first, we consider rejecting the principle of rational analysis in favor of finding simple but effective heuristics; and second, we consider incorporating constraints into the optimization process.

\subsection{Heuristics and Biases: shortcuts around normative inference}

If the goal is bounded rationality, we need not retain the principle of optimality from rational analysis. Rather, we can posit heuristic mechanisms by which people arrive at responses, that might be far easier to compute. Several of these might provide reasonably good responses most of the time -- satisfying the claim of being `boundedly' rational -- however their means of arriving at these responses might be largely disconnected to the process of explicitly computing the optimal response (via normative probabilistic inference, as prescribed by rational analysis).

Therefore, the argument is that while certain behaviors might look `as if' people are engaging in normative Bayesian inference, they might be doing something completely different -- viz. a heuristic strategy that is much easier to implement. In cases where this heuristic contradicts normative Bayesian inference, we get the systemic and predictable deviations from normativity as recorded in empirical studies. This approach has been hugely influential in behavioral economics, -- pioneered by Kahneman and Tversky\cite{tversky} -- as well as in psychology (see \citet{gigerenzer2002bounded} for a review). 

A crucial shortcoming of these approaches however is that while they answer the `what' of the processes underlying human inference, by relinquishing the optimization perspective of rational analysis, they often fail to account for the `why'\footnote{\citet{gigerenzer2008heuristics} does address a version of the `why' question by characterizing heuristic judgment as an adaptive response to structure in the environment, by claiming that heuristics are `ecologically rational'. I discuss this in greater detail in the next section.}, and the `how'. This can lead to lists of heuristics, conceived with inspiration from the specific modes of failure noted in human inference, without a unifying theory of why and how these heuristics are learned or where they come from. 

Another problem is that of \emph{strategy selection} \citep{gigerenzer2008heuristics, marewski2014strategy} -- how do we choose a heuristic for a specific context? Most models of strategy selection assume that people are able to assess the usefulness of a strategy, through cost-benefit analysis \citep{johnson85, beach1978contingency, lieder2017strategy}, reinforcement learning \citep{erev05, rieskamp06}, or based on the strategy's applicability in a particular domain \citep{marewski2011cognitive, schulz2016simple} -- which in and of itself might be a resource-intensive process outside the scope of the posited limitations on cognitive resources. Further, all of these approaches require, either explicitly or implicitly, a feedback signal. This requirement poses a problem in inferential settings where no feedback is available. People can readily answer questions like ``How likely is it that a newly invented machine could transform a rose into a blackbird?'' \citep{Griffiths15} which lack an objective answer even in principle. 

Finally, while these heuristics have been studied primarily in the domain of judgment and decision making, probabilistic inference in humans is important for a much wider set of domains -- including concept learning, causal attributions, and language learning. Many of the proposed heuristics are often specific to the kinds of problems studied in explicit judgment and decision making, very often in the domain of linear regression across a series of attributes that influence a binary forced choice between two options\citep{czerlinski1999good, csimcsek2015learning}. These heuristics might not transfer well to other use cases for probabilistic inference in humans, where the structure of the problem can be significantly more complex. In addition, the procedures used to isolate and understand heuristics so far -- by studying deviations from normative inference in these explicit decision making tasks -- might not apply to these other more complex domains, since isolating and testing interpretable deviations from normativity in such complex domains is challenging. A broader, more general theory of how heuristic inference arises in humans, as driven by concrete underlying principles, would allow a more general theory of probabilistic inference in intelligent systems that spans domains.

\subsection{Computational rationality: optimization under constraints}

Another approach is to explicitly account for the costs of computation in the overall optimization, i.e., to extend the principle of rational analysis to bounded agents.
%Some approaches to this are to involved in rational analysis (e.g., Anderson, 1990), handicapping rational models to produce behavior closer to that of human participants (e.g., Steyvers, Tenenbaum, Wagenmakers, & Blum, 2003), 
These approaches entail specifying the costs of information-gathering, cognitive resources, and time, as well as specifying an algorithm for computing a response that makes specific demands on these resources. By including these resource limitations in the optimization problem, we arrive at a `boundedly rational' solution.

This problem can be discussed at different levels ranging from a computational-level account that describes the problem being solved but does not propose a plausible mechanism, to more mechanism driven accounts. At one end is to simply describe behavior as resource-rational \citep{Vul2014, schulz2016simple} and posit a new optimization problem that accounts explicitly for the costs of internal computations. This approach however, punts the original problem of intractability one step ahead -- the new `boundedly rational' objective function, which accounts for resources in addition to the original optimization objective, might be even harder to optimize than the original optimization. While this approach has great explanatory power, it still leaves open the question of how humans might be achieving this bounded rationality.

A middle ground is to build rational process models\citep{griffiths2015, sanborn2010rational, dasgupta2017hypotheses}. These make specific claims about how the original optimization problem might be approximated, and demonstrates how limitations on information-gathering, cognitive resources, and time -- during the process of approximation -- could lead to deviations from normativity. Many of these approaches still retain an interpretation as computational or resouce rationalit: ceasing additional investment of time or cognitive resources is a (conscious or sub-conscious) choice. This choice is made by computing the \textit{marginal utility} of additional investment (also often formalized at the value of computation\citep{horvitz1989reflection}), and deciding whether this additional utility is worth the cost of the investment\cite{gershman15, griffiths2015}. The assumption is that increased investment of computational resources will start providing smaller and smaller gains to the marginal utility, whereas the cost of resources remains constant. Therefore, we will reach a certain degree of resource / computational investment, where additional investment is no longer worth it.

To naively compute marginal utility however, we need to know how much closer we would get to the normative response with additional investment. If we do not know the normative response, it is not possible to measure how close we are or will be to it. The question remains therefore of how to decide when to stop investing computational resources. Certain properties of the cost function (including the cost of resources) can alleviate this issue. In particular, if the cost function is convex and smooth, then the optimization can be performed using local gradients. In this case it is possible to tractably compute whether we have arrived at the `optimal' trade-off between proximity to the normative response, and computational investment (see Section \ref{sec:MCMC_optimal_stop} for details).

% \footnote{I discuss this is greater detail in Chapter \ref{chap:MCMC}, where we present a rational process mdoel that has these desirable properties. I discuss optimal stopping in Section \ref{sec:MCMC_optimal_stop}.}

In Chapter \ref{chap:MCMC} I introduce a rational process model for probabilistic inference where the cost function has these properties. However, several other models for resource-rational inference might not fall into this smooth optimization regime. \citep{horvitz1989reflection, russell1994provably, hay2014selecting} While much progress has been made in characterizing several behaviors as resource-rational, they continue -- without further assumptions -- to fall into the trap of being an `as-if' model without a realistic proposal of how boundedly normative behavior could be implemented. 

One approach is to learn a predictive model that learns (using past experience) the optimal trade-off point, based on features of the problem. This constitutes a form of amortization, that leverages memory and structure in the environment to make this problem more tractable. Learning predictive models for the optimal stopping point can be extended to a rational solution to the strategy selection problem\citep{lieder2017strategy} where the accuracy and cost of each heuristic is learned (or amortized) over experience with reinforcement learning and chosen between. \citep{erev05, rieskamp06} This however, still leaves open questions like where these heuristics come from in the first place, as well as fails to account for how people form inference strategies in cases where external feedback is unavailable. In Chapter \ref{chap:LTI}, I introduce a formalism where heuristics can be learned as well as selected without explicit feedback.

Further, most rational process models are based on domain-general algorithms, and thus struggle to explain the context-sensitivity of inferential errors (see \citet{mercier2017enigma} for a similar argument). Some models explain why certain kinds of queries induce certain kinds of errors \citep{dasgupta2017hypotheses}, but do not explain how errors can be modulated by other queries in the same context \citep{gershman2014amortized,dasgupta2018remembrance}. As a broader implication of being domain-general, these approaches suggest potential explanations for biases in human inference by positing limitations on computations -- but do not provide explanations for how people (with the same limited computation) can sometimes perform so close to optimally in certain domains.

\section{Ecological rationality}

The two approaches in the previous sections -- of heuristic inference and rational process models -- have some common ground; certain heuristics might be considered accurate approximations \citep{gigerenzer2009homo, parpart2018heuristics, belousov2016catching}. The approach of understanding heuristic inference as fast and frugal hacks that result in adaptive behaviors has been furthered significantly by the research program in \citet{gigerenzer2011heuristic}. This program suggests that heuristics are not simply sub-optimal hacks that serve error-prone human inference, but rather that they leverage underlying structural information in environments to make smart inferences without excessive investment. This idea is termed ecological rationality. It traces back to \citet{simon1955behavioral}. He used the famous analogy of a pair of scissors for human inference, where one blade represents the cognitive limitations of humans and the other the structures of the environment. This analogy illustrates how minds compensate for limited resources by exploiting known structural regularity in the environment. It has however, largely dropped out of the limelight in psychological research, with the majority of approaches focusing on internal mechanisms, with relatively little focus on the environment.

A crucial exception to the broader oversight of ecological rationality in psychology is the work of \citep{gigerenzer2008heuristics}. However, while this literature has extensively studied the ecological rationality of `fast and frugal' heuristics in real-world decision environments, many of these studies remain tied to specific judgment and decision-making domains. These cannot be generalized since they do not make explicit claims about underlying mechanism. For example, it is not immediately obvious how patently decision-centric heuristics like `take-the-best (where the value of two alternatives is decided by the first cue that discriminates between the alternatives, if the cues are arranged by cue validity) apply to say a continuous space of options, or to estimating the value of a option directly (rather than comparatively), or when the options are not explicitly provided but have to be constructed. In addition, it remains to be shown how -- even in the standard judgment and decision-making domains -- such heuristics are learned and chosen for the specific environment at hand. 

On the other hand, the rational process model literature provides more generalizable models that can apply across domains and tasks, since they are based on underlying mechanism. But they fail to account for structure in the environment, since by the very virtue of these algorithms being domain-general, there is no adaptation to specific environments. They are therefore unable to explain differences in performance -- either in how normative people are, or in the specific deviations from normativity people exhibit -- across different domains. Certain rational process models that do explicitly take feedback from the environment within a domain general procedure, fall into the strategy selection trap -- where optimally choosing the `right' heuristic remains intractable and outside the scope of realistic psychological mechanisms. In cases where there is some adaption to environments by learning strategy selection from previous experience (via amortized planning in a reinforcement learning model, see Chapter \ref{chap:amort} for details), this adaptation relies on external feedback which is not always a reasonable assumption. Further, it continues to require the pre-specification of the heuristics under consideration.

\subsection{Algorithmic approaches to ecological rationality}

This thesis proposes an approach that combines the complementary advantages of the heuristics and biases approach with rational process models of inference. I show how the principle of amortization (discussed in Chapter \ref{chap:amort}) can be used to facilitate approximate Bayesian inference, and lead to ecologically rational heuristic behavior in human probability judgment. This can be applied across domains and tasks, since it specifies underlying mechanisms (similar to a rational process model), but also allows adaptation to the environment. These models can explain both how humans make good decisions with limited resources in certain domains (similar to heuristic approaches), as well as the context-sensitivity of inferential errors (overcoming the strategy selection problem). This paves the way forward to a more comprehensive view of bounded rationality in human inference. In the following two chapters, I expand on the conceptual framework for this approach: first by discussing approximate solutions to exact probabilistic inference, followed by a discussion of amortized computation.

I also discuss how this approach (of explicitly studying the role of the environment in acquiring domain-specific amortized inference strategies) can inform the study and engineering of artificially intelligent systems. I demonstrate how analysis and manipulation of learning environments can provide insight into how to artificially develop central tenets of intelligence like causal inference and natural language, as well as inform the underlying mechanisms of how humans acquire and implement these complex behaviors.
