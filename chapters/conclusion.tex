%!TEX root = ../dissertation.tex
\chapter{Conclusion}
\label{chap:conclusion}

In 1955, Herb Simon put forth the challenge facing more realistic theories of human intelligence: "Broadly stated, the task is to replace the global rationality of economic man with a kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environments in which such organisms exist." This thesis hopes to do exactly that. By taking into account the circumstances under which intelligent behavior manifests -- both the limitations on resources, structure in the environment, and how these two interact -- we provide new computational models of human probabilistic inference, that are psychologically plausible. Without plausible algorithmic solutions to rational or normative inference in structured Bayesian models, they remain unsatisfying as models of human cognition. We also cannot leverage their many desirable properties in building intelligent machines. The ideas furthered in this thesis, of leveraging environmental structure via flexible re-use of previous computations to simplify inference, bring Bayesian models of intelligent behavior back into business.

Further, these models parsimoniously explain a wide range of empirical findings about non-normative inference, and how humans can sometimes be so close to optimal, and at other times (with the same cognitive resources), so biased -- and biased in so many different context-sensitive ways. These insights also lead to entirely new ways to understand and engineer artificial systems, via manipulation of the environments in which they learn and function. This confluence suggests links between the analysis of ecological rationality in humans and in machines, leading to new lines of research into understanding both.

\section*{Open Questions and Future Work}

\paragraph{Model acquisition}

An important question not addressed directly in this thesis is of how structured probabilistic models are acquired in the first place. In our studies of human cognition, we have distinguished between `learning about the world' or `potential knowledge', and `learning to think' or `realized knowledge'. Most of the work on human cognition presented in this work operate solely on the second, i.e. in the realm of internal processes within the mind, \textit{after} all external knowledge has already been gained and represented as a structured probabilistic model. In some of these examples, we verbally provide the data generating process, i.e. the underlying structured probabilistic model (for example the urn experiments in Chapter \ref{chap:LTI}), but in several others, we assume this is known from pre-experimental experience (for example in the scene statistics domain used in Chapters \ref{chap:MCMC} - \ref{chap:LTI}). How might these structured Bayesian models be acquired via interaction with the environment?

One way to look at model acquisition is as a higher level probabilistic inference. That is, the representation we acquire of how a domain works is by searching over some space of possible models built from structured primitives, assigning probabilities for how well they explain the observed data (the likelihood of that model), and then choosing a model such that it has high posterior probability. This is of course, also a very challenging inference problem\citep{schulz2012finding, bramley2018grounding}. Further, it risks passing the buck further down to how we can know what the primitives to building a good hypothesis space of models even is. Several findings show that many primitives of structure might be innate and available at birth before any interaction with the environment\citep{spelke1998nativism, chomsky1967recent}, but that too leaves open the question of how (potentially via evolution) such primitives came to be innately encoded. The challenges of how this search over models might occur therefore forms another key criticism of structured Bayesian models for intelligent behavior. These criticisms have been developed further largely by the `connectionist' approach to cognition\cite{rogers2004semantic, mcclelland2010letting} that posits instead that structure emerges from interactions with the environment, via low-level learning mechanisms, rather than via a discrete search over the space of possible structured models. Most modern approaches to artificial intelligence, i.e. deep neural networks, follow in this tradition. Recent work has also suggested that more structured forms of intelligent behavior can in fact emerge from such `low-level' learning\citep{dasgupta2019causal, wang2018, botvinick2019reinforcement}.

Explicitly structured probabilistic models however have several desirable features -- like efficient learning\citep{kemp2007learning}, greater generalizability\citep{lake2017building}, and an accurate representation of uncertainty\cite{hacking2006emergence}. An important direction of future research therefore is to find ways to harness these advantages while avoiding the prohibitive costs and implausibility of learning these models via search. One possibility is that structure in the environment and amortization can alleviate these costs, the same way we suggest it alleviates the intractability of inference within a learned model. Models for amortized inference, including the inference network studied in Chapter \ref{chap:LTI}, have striking similarities to connectionist models. This suggests future directions of research that explore this connection, both theoretically as well as towards building new hybrid models that combine the complementary advantages of these approaches.

%In Chapter \ref{chap:causal}, we showed that the end-to-end process of inferring a causal structure from observations (inferring a model) in addition to performing causal inference within that model can be amortized over previous experience. Preliminary extensions show that the process of inferring a causal model

\paragraph{Two kinds of learning}

Another interesting direction is to consider the interaction between learning about the world, and learning to think. In most real world domains, learning about the model, and learning to make inferences in it are not separate tasks. In fact, we almost never learn models directly, we learn them as an intermediary towards performing some task that requires an inference within that model. This is analogous to the discussion in Chapter \ref{chap:amort} regarding amortization in discriminative models: these models learn both kinds of information entirely end-to-end and have no explicit representation of an intermediate structured representation. In contrast, a generative model represents the underlying generative process, i.e. it explicitly represents such a `model'. This dichotomy has also been studied extensively in the literature on reinforcement learning as model-based vs model-free methods. We also discuss this literature briefly in Chapter \ref{chap:amort}.

As an illustrative example, we consider a classic example from reinforcement learning, of latent learning in Tolman's rat mazes\citep{tolman1948cognitive}. Here, rats learned to navigate mazes of very specific shapes, to get to a reward. Simply memorizing the actions required to get to the reward in these mazes would have been sufficient to always receive the reward. \citet{tolman1948cognitive} found however that rats developed a more abstract model, or `cognitive map' of the spatial position of the reward with respect to their starting point. This was evidenced by the fact that the rats find the reward by navigating directly to it in close to a straight line, when the walls of the maze are removed\footnote{The actual experiment did not remove the walls of the maze but replaced it with a maze that contained several radial arms, and found that rats take close to the shortest path to the reward by choosing the right arm.}. In this case, learning the model is like learning the spatial position of the reward with respect to you. This captures something about the underlying structure of the environment (spatial in this case), and this knowledge can generalize to give reasonable performance in different situations -- like having different kinds of mazes or obstacles in the way. Inference in this model corresponds to planning ones actions (within constraints like walls of mazes, and wanting to minimize energy spent) in order to get to the reward. The task that the rats are trained on only really requires the ability to make some very specific inferences in an otherwise much more general / complex model, with no explicit requirement to represent an intermediate structured model. But we see that they acquire such a representation nonetheless. In other words, they could learn a purely \textit{discriminative} model (for the purposes of the task they are trained on), but instead learn an at least partially \textit{generative} model. This is characteristic of several domains -- even when abstract models are useful, they are rarely explicitly taught or tested. Rather, they are acquired as an implicit intermediary to a task that additionally also requires inference in such a model.

%This is very characteristic of almost all domains where an abstract model is useful -- we never learn this model directly, but as an implicit intermediary to making some inference in that model. \footnote{} 
% learn about the world, as well as learn how to think, simultaneously?

We have so far taken the structure of the model for granted, assuming that world knowledge has already been acquired and is encompassed in an abstract representation of that domain. However, if the model is never directly trained on (and is only acquired indirectly), the question arises of what degree and kinds of structure are necessary to represent. Structure and abstraction allows for efficient learning (via the `blessing of abstraction'\cite{harlow1949formation, kemp2007learning, goodman1983fact}) and relatedly, also allows generalization far outside the range of observed queries. It is for this very reason however that inference in these models is difficult: by being structured and abstract, they can express a vast range of hypotheses. This permits good generalization, but inferring the right hypothesis for a particular problem in this vast space, or representing a distribution over them, is difficult. Generalization and ease of inference therefore trade off. 

If the model itself is also being learned, in parallel with learning how to make inferences in it, a decision needs to be made on how to choose the `right' level of abstraction and structure such that it makes a good trade-off between generalization capacity and ease of inference. This will be influenced strongly by statistics in the environment. The environment will determine what extent and what kinds of generalization are useful: it is not useful to be able to generalize to situations that never occur. The amount of experience, and time pressures for decisions in the environment will also inform how efficient our inferences can and need to be. This thesis studies how to optimize the inference efficiency (by re-using past computation and leveraging structure in the environment) given a fixed model. Future work should consider the problem of jointly optimizing the generalization capacity of the learned model and inference efficiency within it, for a more complete picture of the underpinnings of frugal and flexible intelligent behavior. In Chapters \ref{chap:sentences} and \ref{chap:causal}, we start exploring this. We demonstrate the emergence of representations that are not universally generalizable but only generalize in \textit{certain ways}, by augmenting environments and thereby manipulating which representations are ecologically rational. In future work, these insights can also be leveraged to better understand what the `right' representation for a domain is, as well as inform how to engineer them into artificial systems.

\paragraph{Shaping our environments}

So far, we have assumed that the interaction between the environment and the intelligent systems that live and learn in it is one-directional -- we have only considered the impact of the environment on the procedures learned by agents that interact with it. However, intelligent agents frequently strongly shape their own environments. This two-way interaction is especially pertinent in domains like language where the production mechanisms themselves are shaped and limited by human cognitive abilities. Our ability to learn and understand compositionally structured languages is learned from data produced by other humans' ability to produce these compositionally structured languages. The role of shaping one's environment also relevant in other domains that are not as directly produced by humans. As far back 1956 in the study of category and concept learning, \citet{bruner2017study} presented a distinction between learning through passive reception of observations and through active selection of observations in support of hypothesis testing. Much subsequent work has expanded upon the significant impact that active information seeking behaviors have on learning\citep{nelson2005finding,markant2014better, gureckis2012self}, suggesting that even very young children can and do engage in behaviors that shape their own learning environments\citep{ruggeri2017toma, gopnik1996scientist, montessori1912montessori}. 

A `rational analysis' approach to active learning posits that humans maximize information gain, subject to the costs of information gathering. However, similar to our quandaries about exact probabilistic inference in humans, exactly computing information gain is nearly intractable. Further, several studies often find biases in people's information seeking tendencies\citep{jonas2001confirmation, beattie1988confirmation}. Future work should consider how processes like caching, re-use and amortization that can ease the computational burden of normative information seeking, and potentially lead to new rational process models of active learning.


\paragraph{Grounding the theory}

A key aspect of this thesis is to more explicitly consider the role of memory in human inference. Even within this framework of using memory as a computational resource, it is yet to be understood what the contributions of different memory mechanisms (episodic, semantic, procedural, etc.) might be. This thesis has been largely agnostic to the specific kinds of re-use and how they might be realized in human memory systems. Future work can more explicitly investigate these different kinds of re-use. In the same vein, ecological rationality and biased judgments have been studied extensively, and several models for these have been proposed. We have discussed some of these alternatives in this thesis, as well as how many of them fit into the broader framework of amortized inference. Future work can work towards better understanding how these many models and different memory mechanisms fit together and inform each other.

Another consideration is how such theories might be implemented in the brain. Future work should look for signatures of amortization in the brain, and better understand which parts of the brain are involved in the different kinds of learning discussed here. We have briefly discussed the neural plausibility of approximation algorithms like Markov chain Monte Carlo, and variational inference in Chapter \ref{chap:approx}. A main proposal of this thesis is a hybrid model that incorporates aspects of both algorithms. Future work should consider how such hybrids might be realized in networks of neurons.


%An advantage of structured models are that they abstract away many details that are `not relevant'. For example in our balls-and-urns domain, it is not necessary to represent the physical structure of the urns. For the purpose of that domain, the urns are simply abstract containers whose physical properties have no significance in the kinds of questions asked. Experience with a balls-and-urns task with wooden urns transfers entirely to one with plastic urns. This abstraction allows generalization. However, which features are `relevant' can change. If we saw the same urn in a task where the goal is to predict the probability that the urn will break when dropped from a certain height and angle, the physical properties of the urn are suddenly relevant in our model of that task, and can no longer be `abstracted away'. Additionally, certain details almost never change, so even though they are `relevant' they need not be represented. For example, we assume the presence of gravity in both tasks: we assume that the balls in the urns will not arbitrarily float out of the urn, and that an urn that is dropped will fall to the ground. Even though this variable -- whether or not gravity is present -- is in fact relevant for the domain, it can still be  The kinds of structure we represent in our models for a domain is heavily influenced by the statistics of the environment?

%A broader implication of actively adapting our inference procedures to environmental structure, is that inference can be `learned' as well, via interaction with the outside world. The transition from `potential' to `realized' knowledge no longer happens entirely inside the mind. This blurs the lines between learning about the world and learning to think -- since both involve learning about some external information, just different kinds of information. 

%`Model-free' approaches on the other hand (including the emergentist connectionist views), blur this distinction entirely. The goal is to directly learn how to react to the world, without an intermediate stop at a model representation.

%Amortization of this inference allows a reconciliation between the two. 
%Do humans meta-learn? Structured and compositional amortization

%With much of the expressive burden being shifted to amortization, we are left with a deeper question. What really is the difference between learning and model, and learning to do inference in it? It leads us to question the notion that the models we learn are abstract and general purpose, agnostic to the questions we will ask of them downstream. To take an extreme example, a complete model of any domain, such that we can make good inferences to almost any downstream query about it, could be to represent each atom in it, and know about its interactions. With adequate computing power, this is sufficient to simulate forward every possible scenario, and make arbitrary inferences. But, in our urns-and-balls domain, it is pointless to represent the structural properties of these balls and urns. We know that for the purpose of that domain, the balls are considered abstract single units, and the urns are simply abstract containers whose physical properties have no significance in the kinds of questions asked. We can abstract away this structure because representing it does not improve our inferences in that domain. If we instead conducted an intuitive physics experiment with the exact same articles, asking question like the probability that the urn will break when dropped from a certain height, or domain, these structural properties suddenly become relevant and need to be represented.
