%!TEX root = ../dissertation.tex
\chapter{General Introduction}
\label{chap:intro}

Understanding the flexibility and efficiency of human intelligence, as well as how to incorporate it in artificial systems, is a long-standing open problem. A promising and successful approach has been to build structured probabilistic models of cognition. These have been very successful as a theory of human-like intelligence for two key reasons. First, by being "structured" they allow for primitives of human symbolic thought -- like rules, grammar, and logic -- that allow humans to generalize far beyond our direct experience\cite{fodor88, chomsky2002syntactic}. A classic example is of mathematical knowledge, for example, once we know the rules of addition, we can add two numbers we may never have added or even seen before. For a more everyday example, we know that if "Jane is taller than Gloria", this implies that "Gloria is shorter than Jane", since that is a logical deduction -- even if we may never have met Jane or Gloria, or know anything about their actual heights. These abilities require abstract, logical, structured representations. However, such representations cannot explain another crucial aspect of human intelligence, namely graded, and uncertain inferences\cite{rogers2004semantic}. In many such cases, people do not have adequate information to make logical deductions, but can make reasonable `guesses' and are aware of their own uncertainty. Solely with access to discrete, logical rules, it would be impossible to account for human beliefs like `most birds can fly', or the human ability to make sense of and learn from ambiguous signals. This brings us to the second key aspect of structured probabilistic models -- that they operate over probabilities\footnote{In Chapter \ref{chap:psych}, I discuss in greater detail the advantages specifically of Bayesian probabilistic models.}, not discrete truths. Structured probabilistic models provide a broad, flexible framework with these key desirable properties, that abstracts away the core of the problems that intelligent systems need to solve, and provides a normative solution. This permits us to understand human cognition across a wide range of domains. ..They have also been adopted and used in artificial intelligence... 

\paragraph{human cognition}

However, this models face some crucial shortcomings as models of human cognition. While these structured models provide a normative answer, and good theory of the abstract problem humans are trying to solve, they provide no insight into how humans might actually solve these problems (i.e. make probabilistic inferences in structured models) at the level of algorithms and psychological processes. %In other words, these models provide a good description at the `computational level' of abstraction in Marr's hierarchy\cite{marr1976understanding}, but do not provide an `algorithm / representation level' solution. 
This concern is exacerbated by two key issues. First, making exact probabilistic inferences in these structured models is largely intractable. Intuitively, the very flexibility of these models, in being able to generalize so far outside direct experience and account for uncertainty, can become a hindrance when trying to find a solution to a specific given problem. The space of solutions these models can represent is so vast, that finding the right solution for the problem at hand is like finding a needle in a haystack. I discuss this issue in greater detail in Chapter \ref{chap:approx}. Therefore, despite significant advances towards explaining behavior in humans, they largely remain "as-if" models that describe what humans might be doing, but do not provide a complete picture of how human cognition works. 

Second, much empirical evidence suggests that people make several systematic inferential errors, with their probabilistic estimates deviating consistently from the predictions of a normative model. Some of these include effects like base rate neglect, where people tend to ignore certain sources of information like prior probabilities, or the anchoring effect where putatively unrelated sources of information influence future inferences, among many others. For a sense of scale, Wikipedia lists over a hundred such established cognitive biases. The fields of cognitive psychology\citep{phillips1966conservatism, gigerenzer1996reasoning}, and behavioral economics\citep{kahneman1973psychology} have responded to these findings with the proposal that perhaps humans in fact do not perform normative inference at all, and instead employ heuristic strategies. While such models can replicate many of these biased behaviors in certain domains, these heuristics are brittle and any single heuristic cannot explain the wide variety of behaviors observed in human inference -- ranging from near-normativity, to wide range of different kinds of cognitive biases. This has lead to the development of several different heuristics, and theories where people choose a specific heuristic for each domain. However, this approach is also unsatisfying and leaves many questions open: What is the set of heuristic strategies humans use? In the absence of any sense of normativity, why were those specific heuristic chosen? How do humans choose when to use which heuristic? With such a loosely defined theory, it is difficult to falsify specific claims or make new predictions. Therefore, while these heuristic theories make stronger and possibly more believable claims about the psychological mechanisms underlying human inference, by relinquishing the normative framework of inference in structured probabilistic models, they lose explanatory power of `why' human cognition is the way it is.

A main goal of this thesis is to address these key criticisms of structured probabilistic models by providing a) a more computationally tractable solution to inference and b) an explanation for deviations for normative inference. I do this by taking a renewed look at the environment in which humans operate.

%Further, it is not clear how these structured models might be learned from experience. Most approaches involve a search over many structures to find the one that fits data best. My virtue of being highly structured, this space of possible structures is large. While significant progress has been made in improving the quality of this search, it remains computationally challenging. In this thesis, we will primarily address the first issue, and discuss the second issue briefly in Chapter \ref{chap:LTI}.

A crucial shortcoming of these approaches is that they focus purely on the computational processes taking place within the human mind. Even approaches that make process-level claims, focus solely on cognitive mechanisms. What is missing is a serious consideration of the environments in which intelligent systems act. The analogous concern also exists with the study of AI, where progress is gauged largely by how new models and mechanisms perform on unchanging, often arbitrarily selected, standardized data sets. A crucial insight is that humans are not a general purpose computing machine, as several "fully normative" models would have you believe. Therefore, neither should AI that hopes to emulate human intelligence. Humans have developed -- both over evolutionary and lifetime time scales -- to solve specific kinds of problems. Our environment strongly affects our learning and inference procedures. This is further highlighted by the fact that humans have limited computing resources, and therefore are in more dire need to adapt to their environments and selectively optimize performance where it matters. Learning about things that don't occur often is often not even possible due to limited experience, and is largely not worth it given the cost on computation.  define ecological rationality.
Herb Simon / Brunswick = importance f the environment.

This leads us to a solution to the conundrum that humans are flexible, but models that reflect this flexibility are intractable. This thesis expands on this. I incorporate the role of the environment in how Bayesian models of cognition can be approximated. This provides a mechanistic accounts of how inference might be carried out within resources limitations. We also find that several errors observed in human inference can be modeled by ecologically rational limitations on general-purpose approximate inference, as well as test new predictions of these models in new experiments.

Building artificially intelligent systems also come with resource constraints. While the limitations might be different, the cost of exact learning and inference in structured probabilistic models has been prohibitive. Rather, the lead approach has been deep learning. Many of these approaches implicitly amortize computations. Amortized computations, as discussed before, have a tendency to reflect environmental structure. But this aspect has been largely neglected.  I also discuss how a deeper understanding of the role of the environment in shaping the inference procedures of intelligent systems, can inform the study and engineering of artificially intelligent systems. AI has similarly largely neglected the crucial role of the environment in shaping the inference strategies learned by intelligent systems, choosing instead to focus only on the internal mechanisms. I demonstrate how analysis and manipulation of learning environments can provide insight into how to artificially develop central tenets of intelligence like causal inference and language, as well as inform the underlying mechanisms of how humans acquire and implement these complex behaviors.

\section*{Outline}

First and foremost, in Chapter \ref{chap:psych}, I review the theoretical framework surrounding human probabilistic inference. I first review rational Bayesian accounts, followed by a review of boundedly rational approaches. Finally I discuss ecological rationality -- a specific subset of these boundedly rational approaches that explicitly posits adaptation to structure in the environment. I highlight the contributions and shortcomings of these approaches, and outline how this thesis suggests a more complete picture by unifying their complementary advantages. In Chapters \ref{chap:approx} and \ref{chap:amort} I review the technical and conceptual background for this thesis. In Chapter \ref{chap:approx} I cover technical background on approximate probabilistic inference. In Chapter \ref{chap:amort} I introduce a formal notion of computational re-use i.e. amortization and how it can be used within algorithms for approximate inference. I also discuss in each chapter the history of these concepts as applied -- explicitly or implicitly -- in models of human cognition. In subsequent chapters, I demonstrate how these concepts can be used to build new and better models of human inference.

%In this thesis, I show how adaptation to environments (as necessary under resource constraints) can both a) render inference more tractable on average, and b) explain deviations from normative Bayesian inference.

In Chapter \ref{chap:MCMC}, I study human inferences in large hypothesis spaces. This is a challenging problem, where exact inference is intractable. I demonstrate that a sample-based approximation under ecologically rational constraints, can replicate the specific kinds of biases observed in human inference in such large hypothesis spaces. 
% This model relies on two crucial components to replicate these deviations from normativity: first, a small number of samples, and second, initialization based on question framing. I argue that these components are ecologically rational: humans have limited cognitive resources, hindering the possibility of a large number of samples in the approximation, and initialization based on question framing is reasonable since the context of queries in the environment are often related to and informative of the optimal response. 
Further, in small hypothesis spaces, this model returns optimal responses. This allows us to parsimoniously explain both the rationality as well as various kinds of irrationality of human inference within the same framework. Chapter \ref{chap:MCMC_amort} expands on the role of the environment in such sampling frameworks. I empirically demonstrate, as well as models for, re-use of computation in consecutive related queries. This kind of re-use -- or amortization -- is ecologically rational since queries in the real world are often related, and the best use of limited resources is to re-use previously completed computations.

Chapter \ref{chap:LTI} studies the role of amortization in greater detail. In particular, I discuss amortization in a variational framework, which is more amenable to flexible re-use than sampling. I demonstrate how this framework can replicate several cognitive biases that involve context-sensitive sub-optimal reactions to different sources of information, 
% -- including base-rate neglect, conservatism and belief bias -- 
as well as various various other effects.
% such as the response variability in probabilistic judgments, effects of experimental design, and certain memory effects. 
This work posits an algorithmic approach, via amortization of inference, to understanding ecologically rational heuristic behavior. I also discuss how this approach can be combined with previously discussed sampling approaches, to jointly and parsimoniously account (with mechanistic commitments) for a wide range of biased inferences while still retaining the capacity for optimal inference in the limit of infinite experience in the environment, or infinite computational resources.

Chapters \ref{chap:sentences} and \ref{chap:causal} shift gears to focus on the role of the environment in studying and improving modern artificial intelligence (AI).  In Chapter \ref{chap:sentences}, I consider a system for natural language processing. I first discuss this system, like many modern deep-learning based AI systems, can be interpreted as performing amortized inference. I then demonstrate how ecological rationality in this inference procedure can explain the errors this system makes -- analogous to how they explained human biases in Chapter \ref{chap:LTI}. I outline an approach that uses this insight to better asses and potentially also improve these systems. In Chapter \ref{chap:causal}, I demonstrate how we can leverage the strong influence the environment has on the inference procedures learned, to engineer the inference procedures we want. I show that manipulation of the environment of a very simple learning architecture, can give rise to complex behaviors including causal inference procedures and active information seeking behavior. The strong control we have over the environments encountered by artificial systems also allows new investigations into ecological rationality, and how it shapes human cognitive abilities.